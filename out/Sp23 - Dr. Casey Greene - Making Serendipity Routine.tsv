start	end	text
0	15100	leading to his first paper, which is affiliated with the Department of Genetics here at UGA.
15100	21300	The Green Lab develops machine learning methods to integrate disparate large-scale datasets,
21300	26020	develops deep learning methods for extracting contacts from these datasets, and then brings
26020	32140	these capabilities to molecular biologists through open and transparent science.
32140	38580	Of particular note to folks interested in large language models, like CHAT-GPT, Dr.
38580	44260	Green and others recently posted a preprint on how AI might be able to help us write and
44260	47420	revise our academic manuscripts.
47420	54420	Please join me in welcoming Dr. Casey Green to UGA.
54420	64700	Okay, thank you, yeah, it's good to be back.
64700	69380	I actually spent a summer working in this building in the Department of Genetics.
69380	71860	Everything around this building looks different now, but somehow the building still looks
71860	74500	the same.
74500	79980	I'm excited to get a chance to share some of what we've been doing in our group and
79980	83140	then to share some of what's going on at the University of Colorado and sort of give you
83140	87980	an idea of what the ecosystem is like where we are.
87980	94180	I think it's always important to think about kind of what our role as informaticists in
94180	100140	science is, like what do we bring to the ecosystem, and I like to think of what we contribute
100140	102900	as essentially serendipity, right?
102900	106580	If we do our jobs well, people will see something in their data that they hadn't seen before,
106580	109020	and they will make a different decision based on that.
109020	115540	So I like to, you know, think about how we can make more kind of serendipitous moments.
115540	122940	I'll start with a brief vignette of a project that we started quite a few years ago now
122940	127860	trying to understand rare diseases.
127860	133500	And in particular, what factors might drive a rare, what systemic factors might drive
133500	136340	a rare disease.
136340	142260	The world when we started this project was one where looking across multiple data sets
142260	144460	remained somewhat challenging.
144460	148980	It's time consuming, you have to deal with batch and technical artifacts.
148980	153380	And so our question when we started, a postdoc joined the lab and the idea was, well, you
153380	155180	know, here's the gap that we're facing.
155180	160060	If you want to analyze multiple data sets at the same time, to identify systemic factors,
160060	164180	that means you're looking at different tissues, probably looking at different cohorts, there's
164180	168060	potentially different disease contexts, might be different controls.
168060	170900	All of this makes your life a lot harder, you can't just kind of make an assumption
170900	175100	that you're going to do three different t tests and be done with it.
175100	179980	And so what this postdoc wanted to do was say, okay, can I find these commonalities
179980	183180	without it taking an inordinately large amount of time.
183180	187780	And she was very interested in not taking an inordinately large amount of time because
187780	194300	the strategy that she used in her PhD, before she joined the group, was developed, she developed
194300	197740	this approach using a modular framework to analyze these data sets, where you essentially
197740	203220	take different data sets, decompose them into modules, and then try to map a module in one
203220	207100	data set to a module in another data set to a module in another data set.
207100	211220	This is possible, if you're an expert in the disease, and you're an expert in all the tissues
211340	216340	that sort of are affected, you can do this, you can say, okay, this is this pathway response
216340	219740	in this tissue, and this is this other pathway response in this other tissue, but it's really
219740	220740	time consuming.
220740	227460	And the complexity grows essentially, with at least the square of the number of modules
227460	228460	that you want to look at.
228460	231380	So you sort of restrict yourself to a modest number of modules if you want to do this in
231380	233580	any practical amount of time.
233580	237060	There are potentially ways to automate this, you know, using over representation analysis
237060	240940	or other strategies to try to, you know, make life easier on the mapping stage, so you don't
240940	244220	spend a lot of time looking at stuff that's unlikely to be fruitful.
244220	250100	But either way, it's challenging, it takes a bunch of time and it requires a lot of expertise.
250100	255980	So what Dr. Jacqueline Tironi did was say, well, what I really want is a module library,
255980	261220	like how could this process or cell type be represented in any data set that I look at,
261220	266100	I'd like to be able to pull that off the shelf, and then take that module library to different
266100	270180	data sets, and then look at each of those data sets in terms of those modules and just
270420	271580	look directly across those modules.
271580	277500	I don't have to now sort of do the module connection after the fact, I can do it upfront.
277500	281460	This would be great, wouldn't it be great if.
281460	286220	And so her hypothesis was that, you know, these modules don't just exist in one data
286220	288180	set, they exist across human biology.
288180	294540	So could she, so her hypothesis was that she would be able to learn these reusable modules
294540	298700	by taking many, many, many different data sets, decomposing those different data sets
298700	304260	into modules, and then sort of learning which modules were necessary to reconstruct the
304260	306180	original data.
306180	310180	And then once she did that, she could do that on a generic collection of data, and then
310180	312860	hopefully be able to use that on her data sets of interest.
312860	317580	And in this case, we're interested in studying a disease called Enka-associated vasculitis,
317580	322060	which is rare enough that if you look at large collections of public RNA-seq data, you don't
322060	323680	find it.
323680	328100	So it's quite a rare disease.
328500	331700	So the idea here was, if she took a whole bunch of generic samples, essentially random
331700	335900	human data that she downloaded from the internet, transcriptomic data, she decomposed that,
335900	339620	she could decompose that into patterns, and then she could take those patterns and we
339620	342980	could apply those to the rare disease data sets of interest, and potentially do sort
342980	345940	of standard statistics with that.
345940	352500	We, just to give you an idea of the data set that we started with, so this is a data set
352500	356100	ReCount 2, I think there's now a ReCount 3.
356100	359220	This is produced by Jeff Leak's group at Hopkins.
359220	364660	And if you wanted 70,000 RNA-seq samples, you can just get 70,000 uniformly processed
364660	365660	RNA-seq samples.
365660	371140	What I like to tell Jackie is that her project had to be successful, because if you think
371140	374260	about the resources we were giving her to start it, if you just benchmark that those
374260	378780	samples probably cost about $1,000 to generate, you know, I like to tell her, look, we gave
378780	382460	you $70 million to start your project, you have to do something with $70 million, right?
383460	389300	So, this was the data that we used, and then we tried quite a few different methods to
389300	394300	extract patterns, including some that we developed in our group, but we ended up coming to this
394300	399260	method called PLYR, which is from Maria Shakina's group at Pitt.
399260	404340	And PLYR is the Pathway-Level Information Extractor, and it does a couple things that
404340	405340	are really nice.
405340	410500	So, it's essentially doing this matrix decomposition, but as you do the matrix decomposition, there's
410540	415580	a couple sort of regularization factors and some penalties in it, and essentially, it
415580	419420	has some sparsity properties that we really like, so the idea is that you want to be able
419420	422740	to explain a dataset with relatively few latent variables.
422740	427300	Also, you want your latent variables to have only a modest number of genes in them.
427300	432740	And finally, if those latent variables can align with a pathway, you'd really prefer
432740	436220	that the latent variable align with a pathway, because anytime you're doing this decomposition,
436220	438940	you're essentially doing some arbitrary rotation, right?
439180	445380	PCA is essentially learning an arbitrary, it's learning a rotation of your data, a reduced
445380	448380	dimension space rotation of your data.
448380	453340	The rotation in PCA is essentially arbitrary, I mean, you've chosen that you want to maximize
453340	457740	the variance on the first axis, but you could have chosen anything else, like ICA, you're
457740	461020	just like, I don't care, just make it small.
461020	466100	So in this case, what it's doing is it's essentially saying, if a pathway can line up with an axis,
466100	467260	let's do that.
467260	472620	And so it's a little bit less, that's the intuition for what it's doing, it's actually
472620	476700	a little bit, the regularization is a little bit different than that.
476700	479340	And what that gives you is a really nice level of interpretability.
479340	483700	So instead of having to think about a module as, like, instead of saying, okay, this cell
483700	488980	type is, these three different axes of variability added in some fraction, usually those cell
488980	491060	types are going to come out as a single axis in your data.
491060	497180	So it makes it much more easy to think through and reason about the solutions.
498100	502260	And so what we essentially did is say, okay, well, we have this enormous collection of
502260	506940	generic human data from the internet, ReCount2, and we have a plier, and we'd like that to
506940	510380	be a machine learning model that we could use in many different biological contexts.
510380	514900	So we named it multi-dataset plier, but because in bioinformatics, everything has to have
514900	518780	a name, we just shortened that to multiplier.
518780	521380	And so this is kind of a multiplier idea.
521380	526860	I'm just going to give you a couple highlight results from the paper that I think help to
527540	529820	kind of give an idea of why you might want to do this.
529820	531500	The paper is pretty exhaustive.
531500	535340	It has a significant deep dive into sort of exactly what's happening in this stuff, but
535340	539580	I'll just give you kind of the high points.
539580	543660	Essentially what we wanted to understand is, does this model learn something we didn't
543660	545740	already know?
545740	550180	And is it better than if you just took data from the disease of interest?
550180	553700	We couldn't get enough data from the disease of interest, which is psychosociative vasculitis.
553700	559540	So we did that analysis in the paper, and we just didn't learn many axes of variability
559540	561020	because there's too little data.
561020	564460	So what we wanted to do is say, well, let's pick something where we could actually learn
564460	565460	something.
565460	567220	Let's sort of give this idea a chance.
567220	571220	And then we said, well, let's imagine we're studying a different autoimmune disease, lupus.
571220	575660	So that's what's here, where it says SLE, this box plot.
575660	579940	What we've done is we've collected all the whole blood data that we could get from individuals
579940	585260	with lupus that was publicly available to create one collection of data.
585260	588780	Then what we've done with RECOUNT2, which is the generic human data from the internet,
588780	593820	is we've taken RECOUNT2 and we've subsampled it to be the same size as the lupus set.
593820	594940	So that's the box plot.
594940	597420	The box plot is RECOUNT2 subsampled.
597420	600460	And then where you see the diamond, that's what happens if you just take the complete
600460	602460	collection of data from RECOUNT2.
602460	604300	Okay, so let's label our X.
604300	608220	Oh, so yeah, so these data sets, if you think about the science behind this, this experiment
608220	612300	has these two data sets of the same size, but different composition.
612300	616140	These two are the same composition of the data, but they're quite different sizes.
616140	620780	I think 70 times larger for the diamond than the other.
620780	623580	And so then we can ask, okay, so how many patterns are we learning from our data?
623580	627660	So this method uses the latent variable decomposition, so the number of latent variables, which are
627660	629340	essentially patterns.
629340	632860	You can see that if you want to learn more patterns, there's sort of more, and there's
632860	637140	a heuristic and plier for sort of selecting the optimal number of latent variables, and
637140	640260	we use that heuristic here, it seems like a pretty reasonable heuristic.
640260	645140	It uses cross-validation to essentially ask how frequently you're rediscovering the same
645140	647860	latent variables.
647860	652900	So if you do that, you find that you can learn more latent variables or recurrent patterns
652900	656900	in the data if you have less heterogeneity in your data given a fixed sample size.
656900	660420	I don't think this is going to shock anyone, right?
660420	663900	If I give you a limited amount of data, you would like that data to be as consistent as
663900	666620	possible except for the thing you'd like to vary, right?
666620	670060	That's usually a good place to live.
670060	671340	So that's what we see here, right?
671340	675740	You get more latent variables out of the lupus data than the subsampled recount2 data, but
675740	679460	if I told you, well, you can have really messy data, but you can have a lot of it, now you
679460	681080	can learn a lot more patterns.
681080	685500	So you end up with many fold more patterns that you can learn, the kind of recurrent
685500	687620	patterns across the data set if you have more data.
687620	692780	So you'd rather have less heterogeneity, but sometimes having more samples can overcome
692780	696460	the heterogeneity issue.
697300	698300	So that's the first thing we ask.
698300	700060	So you get kind of more total patterns.
700060	707440	The next thing we can ask is, okay, well, we don't know what complete collection of
707440	710060	processes are transcriptionally co-regulated, right?
710060	711460	This is not something we know a priori.
711460	715940	We can get a collection of processes if we go to Gene Ontology or KEGG or other databases,
715940	718060	but some of those may not be transcriptionally co-regulated.
718060	722620	However, if we're seeing a process that's coming out as transcriptionally co-regulated,
722620	724860	that's probably a positive hit.
724860	726700	And so that's kind of the assumption we're making here.
726700	730820	So this is looking at both the SLE and the ReCount2 data again.
730820	736580	This axis is what fraction of the pathways that we know about are coming back as sort
736580	740580	of aligned with one or more axes in the data.
740580	744660	And so you can see this actually wasn't driven by composition of the data, this was driven
744660	745660	by sample size.
745660	750580	So if you put in more data, you can learn kind of more transcriptional co-regulation.
750580	754060	This obscures a little bit of what's going on here because the processes are a little
754060	756460	bit different.
756460	761740	If you look at what happens in the ReCount2 data, you end up learning, as you get to the
761740	765020	large sample size, you end up learning more granular processes.
765020	768780	So it seems like it's probably that you're covering the same things that are transcriptionally
768780	772740	co-regulated, you just get a higher level of resolution.
772740	776780	And then, so you kind of learn more of the stuff we know we should know about.
776780	780500	So at this point, anything I'm showing you, you could also do with GSEA or any of these
780580	783820	other methods, gene set enrichment analysis, or those types of methods.
783820	787620	But we can also ask, can we learn anything we didn't already know going in?
787620	794620	So this is asking what fraction of the latent variables that are coming back are not associated
794620	797100	with the pathway.
797100	803140	And what you can see is, when you have the sort of more modest sample sizes, about half
803140	805700	of the latent variables that come back were associated with the pathway, and the other
805700	807180	half are potentially novel.
807580	811460	Could be novel biology, it could be a technical artifact.
811460	815420	What you see when you get the large collection of ReCount2 data is, you know, that number
815420	816420	drops to about 20%.
816420	820300	So 80% of the latent variables that are coming back didn't exist in the databases that we
820300	822100	had available to us going into it.
822100	826780	So that's a really nice thing to have in your back pocket if you want to say, well, look,
826780	830580	I want to explore the biology of what's going on in this disease, but I don't want to limit
830580	833020	myself to what people have curated into a database.
833020	838380	So this is kind of a data-driven way to figure out what those modules could be.
838380	841700	And you might also say, well, there's probably just an enormous amount of technical artifacts.
841700	845360	You just told me you gathered a whole bunch of random human data from the internet.
845360	849140	One of the positive things that we saw that was kind of suggestive that this is not driven
849140	853900	exclusively by technical artifacts is actually this proportion bumps up a little bit when
853900	857140	you look at the ReCount2 data, as opposed to the SLE data.
857140	861460	If it was entirely driven by technical artifacts, you'd actually expect this to have fewer latent
861460	865420	variables that were associated only with a known process.
865420	868300	So this was also encouraging.
868300	871980	So this gives us the idea that we kind of learn more unknown unknowns.
871980	876380	And then there's quite a bit more of a deep dive in the paper, like looking at sort of
876380	879540	one of the things that we see is instead of having a cell cycle latent variable, you end
879540	883660	up with different phases of the cell cycle partitioned into latent variables.
883660	889600	But the sort of takeaway was that if you do these machine learning analyses while reusing
889640	894040	data from other contexts, you can get this level of detail that you couldn't get just
894040	895840	analyzing your data alone.
895840	899520	So starting with a whole bunch of other data, learning the pathways and processes there,
899520	902800	and then applying it to your data gives you this higher level of resolution.
902800	907040	There is a bit of an implicit assumption here, which is if the process that you were looking
907040	912240	at is truly unique and only occurs in your setting and no other settings, you can't find
912240	916920	it because it's not going to be present in the variability from other people's data.
916920	919080	I think this is probably rare.
919560	923400	I don't think it's terribly common that there are processes that are so exclusive that they're
923400	927120	only used in one and only one biological context and nowhere else.
927120	929800	But if you believe that to be the case, you should know you will not find it with this
929800	932800	method.
932800	934480	And so then just kind of recapitulating.
934480	939680	So in the past, when Jackie joined the group, she had this modular framework approach, which
939680	946280	is actually really nice and is now used, it's been used in scleroderma and other contexts
946280	950240	to connect pathways across tissues and studies.
950240	953480	But the multiplier approach she developed has some nice advantages.
953480	957520	So she takes this generic human data, recount two, she can train a model, then transfer
957520	962320	it to the datasets of interest, and then just look across those datasets with standard statistics.
962320	966080	So this is an example of one of the things we can do with this.
966080	970040	So this was actually the thing we wanted to do when we started the study.
970040	975200	So these are three different datasets from individuals with ANCA-associated vasculitis.
975200	979840	One of the challenges here is that all of these datasets are microarray-based.
979840	982560	All of our training data is RNA-seq.
982560	987240	A different student in the lab developed a technique to do, if you're interested in taking
987240	993920	sort of machine learning methods and applying them to gene expression data across these
993920	1001040	contexts, for many methods, there's reasonable ways to do that transformation that is not
1001040	1007280	completely horrendous, which is the best advertisement I can get for a method.
1007280	1008280	But there's quite a few different methods.
1008280	1011760	And actually, quantile normalization is not bad in this context.
1011760	1015400	The zeros kind of give you a bit of trouble, but it's not horrendous.
1015400	1017680	And so this is what we're doing here.
1017680	1023200	So we're actually asking, can the multiplier model actually apply to array datasets, even
1023200	1025000	though it's trained exclusively in RNA-seq data?
1025000	1028920	We would have done this in RNA-seq data, but it turned out there wasn't RNA-seq data for
1028920	1030440	this that was available yet.
1030840	1033040	So now we've gotten to the point where the datasets for this disease are large enough
1033040	1036440	that they actually do exist in RNA-seq as well.
1036440	1038080	So what's going on here?
1038080	1043160	So we've got three different datasets, airway epithelial cells, renal glomeruli, and PPMCs.
1043160	1046800	They're collected in three different studies, so there's no matched people.
1046800	1048960	And also the conditions are brutally different.
1048960	1052640	So what's going to happen is from the left side to the right side of each of these plots,
1052640	1056360	we're going to go from the least, from the most severe form of the disease to sort of
1056360	1059360	the least severe form of the disease for healthy controls.
1059360	1065320	So this dataset for the airway epithelial cells has, these are the vasculitis data,
1065320	1069280	but then it's got things like pancreatic rhinitis, healthy.
1069280	1075640	And so we're basically saying, okay, what is associated with severity across these three
1075640	1078160	different cohorts?
1078160	1083040	And so one of the latent variables that comes up as a severity associated is this M0 macrophage
1083040	1084040	signature.
1084360	1089640	And you can see the same thing where in each group you look in, the least severe form of
1089640	1092280	the disease is on the right, the more severe form of the disease is on the left.
1092280	1097440	So you can see this latent variable severity associated due to the bizarre sort of path
1097440	1100040	of academic publishing.
1100040	1104280	So our guess was that M0 macrophages could be involved here.
1104280	1109000	Well, before this actually was published, but after the preprint came out, we had some
1109000	1111640	follow-up work.
1111640	1114280	And I have to break all the chronology of science.
1114280	1118400	Our follow-up work came out first demonstrating that it looked like there was a change in
1118400	1123240	macrophage metabolism in the disease that could be sort of influencing severity in a
1123240	1124880	systemic way.
1124880	1128400	You can also use this type of analysis to say what's particular to a tissue, right?
1128400	1132560	So you could say what latent variables are associated with severity in this tissue, but
1132560	1133720	not other tissues.
1133720	1136920	So it gives you the ability to start doing those analyses in a way that it's pretty darn
1136920	1143760	difficult to do with just the modular framework alone.
1143760	1148560	And then I have an almost five-year-old, she turns five in three weeks, and she's been
1148560	1150680	watching Zootopia.
1150680	1155040	And there's a line in a song in Zootopia where I was listening, I'm like, oh my gosh, this
1155040	1158760	is science.
1158760	1161680	So the line is, I'll keep on making those new mistakes.
1161680	1165120	I'll keep on making them every day, those new mistakes.
1165120	1166880	And so we're really big on this in the lab, right?
1167840	1173160	But what I tell people is, it's not going to work, just make it not work differently
1173160	1174160	each time.
1174160	1177240	If it's not working the same way each time, that's not good, but if it's not working for
1177240	1180080	different reasons, that's perfect.
1180080	1181700	And so we do this in our own work.
1181700	1187660	So this is the first part of the GitHub that's rate me that's associated with this paper.
1187660	1191480	So if you want to know sort of, these are all notebooks, if you want to follow along
1191480	1194360	with the work that we did for this multiplier paper.
1194360	1197720	The first part of this is kind of our proof of concept exploration to just understand
1197720	1198720	how the method worked.
1198720	1203280	Then you get to the stuff that's in the paper, then you get to the stuff that's in the supplement,
1203280	1206080	then you get to the stuff that's neither in the paper nor the supplement, because it turned
1206080	1208080	out the paper was too long.
1208080	1212480	And so this gives you a way to see like, okay, here's all the stuff we did.
1212480	1215640	So there was one experiment that we did where we wanted to say, can you predict outcome
1215640	1218220	in clinical trials from these latent variables?
1218220	1223000	And so we got this Rituximab data set from the NIH that was testing this.
1223000	1227800	It turned out that the data set structure was, let's say, suboptimal, in that some of
1227800	1231320	it was paired end and some of it was not paired end sequencing.
1231320	1233880	And this was confounded with the endpoint.
1233880	1237800	So it turned out to be extremely difficult to analyze, and we couldn't really learn anything
1237800	1238800	from it.
1238800	1243160	But if you're interested in using that for your own work, probably not that data set,
1243160	1244160	that idea.
1244160	1248040	You know, we've got a notebook here that's like, okay, here was our attempt to build
1248040	1249240	a model to predict response.
1249240	1250240	So you can start from that.
1250480	1255840	So if you're interested in this, we try to do this for each of our papers.
1255840	1256840	So this is available.
1256840	1258400	The GitHub is here.
1258400	1263080	If you search for Taroni and multiplier, you'll probably find it.
1263080	1267960	But I thought this was a nice example of kind of how we've taken a project from inception
1267960	1271000	through execution through kind of deliverables.
1271000	1273640	This method, we've seen some other uses now.
1273640	1277600	So someone used the same thing to study neurofibromatosis.
1277600	1279120	That came out relatively recently.
1279120	1283120	I can't remember, there's a few other sort of rare disease analyses that people have
1283120	1284120	started using this for.
1284120	1285120	But we really like seeing that, right?
1285120	1291120	Because it demonstrates uptake that is in a, I mean, rare disease transcriptomics is
1291120	1293800	a relatively small community.
1293800	1297880	So it's nice to see this stuff beginning to catch on.
1297880	1304240	I would also say, you know, we started with about $70 million worth of data.
1304240	1308280	If you are Lego Grace Hopper and you happen to have an internet connection, you can have
1308280	1311600	about $4 billion worth of data at your fingertips.
1311600	1315800	So if you're interested, there's a few more resources that have come online.
1315800	1320040	I think Arches 4 now has like 650,000 samples.
1320040	1324120	So that's about, you know, if you want to estimate $650 million of preprocessed data
1324120	1325960	at your fingertips.
1325960	1328920	In a previous position, we built something called Refined Bio that's about a million
1328920	1331240	samples.
1331240	1334120	So these types of resources are available, which is great, because then you don't have
1334120	1335680	to go back and rebuild this.
1335680	1338960	You don't have to do all the software engineering to reprocess the data in a uniform way.
1338960	1341560	You just kind of start from the processed data.
1341560	1346600	And I think this opens up a lot of avenues of exploration.
1346600	1351880	I like to, you know, one of the things that I say about what our lab works on is machine
1351880	1353680	learning, public data, and the transcriptome.
1353680	1357040	Pick two of three, and we're probably interested.
1357040	1362840	One of our Bush essentially wrote and designed the way that we fund science in this country.
1362840	1366280	So this idea that most science is going to happen outside of government research labs,
1366280	1369800	it's mostly going to happen at universities, it's mostly going to be grant funded.
1369800	1374080	He wrote this letter to FDR that says, the pioneer spirit is still vigorous within this
1374080	1378080	nation science offers a largely unexplored hinterland for the pioneer who has the tools
1378080	1379080	for his task.
1379080	1385840	Well, I would say, I think open data is like the opportunities here are remarkable, like
1385840	1391520	the ability to, you can take these data sets off the shelf and learn how something works
1391520	1395760	at a scale that's very difficult to do from the data generated in only one lab.
1395760	1397360	And once you do that, you can then test it.
1397360	1400920	And I think I really think using other people's data as sort of the starting point to generate
1400920	1404080	hypotheses that you then go test.
1404080	1408080	There's an enormous amount of unexplored opportunity here.
1408080	1411640	We also think sometimes about other data types instead of just gene expression.
1411640	1415840	So this is work from David Nicholson, who was a PhD student in the lab who just graduated
1415840	1418120	last year, who was like, well, let's do that.
1418120	1421960	I just want to understand what's on bioRxiv anyway.
1421960	1426760	So at this point, I probably don't have to introduce it, bioRxiv is a preprint server.
1426760	1430560	And so this gives us the ability to also study the peer review process in some ways.
1430560	1434040	So we can see what gets posted to bioRxiv, and then we can look at the sort of what the
1434040	1436160	final paper looks like.
1436160	1443240	We started this project just around the time that bioRxiv released an XML repository of
1443240	1444920	their complete collection of data.
1444920	1448120	So if you're interested in not just having a complete collection of transcriptomic data,
1448120	1451640	you can also go get a complete collection of XML preprints, which I think is really
1451640	1455240	exciting and a lot of fun.
1455240	1458800	You learn some things if you start looking at just the metadata associated with this.
1458800	1465160	So this is one of the simple questions that David asked was just, well, if there are preprints
1465160	1469160	with multiple versions, are people sort of adjusting their preprint in response to peer
1469160	1470280	review?
1470280	1474040	So if someone submits their paper, they get comments back, do they generally repost it?
1474040	1476920	We can't directly answer that question, we don't have access to the journal system.
1476920	1481520	But we could say as well, if that were happening, probably what would happen is that at each,
1481520	1485120	you know, as you saw in each version, you'd see an extension in the time to publish.
1485120	1486120	Sure enough, you see that.
1486120	1493080	And actually, the coefficient on the X here is about 50, which is in days.
1493080	1497000	So it suggests that adding a version means sort of 50 days longer in the publication
1497000	1500560	process, which is kind of aligned with what you'd expect to see if people are putting
1500560	1506520	up papers and revising them in response to peer review.
1506520	1513480	Another thing we can ask, so this is getting into the text itself, is if the text changes,
1513480	1518320	do more changes in text between the preprint and the published version result in, does
1518320	1520640	that come with a longer time to publish?
1520640	1523400	And the answer to that is kind of yes-ish.
1523400	1528560	If a preprint changes more from the, if a published version changes more from the preprint,
1528560	1531000	it does take a bit longer to publish.
1531000	1533840	But it's not an incredibly substantial change.
1533840	1536560	And actually, the other thing that we did, so as we were doing this project, there was
1536560	1542420	another group that did a completely different project, where they took a set of COVID papers
1542420	1545480	that were published first as preprints and followed them through.
1545480	1546800	Their scientific question was different.
1546800	1552080	They wanted to say, for COVID-related papers, does the message of the paper change as it
1552080	1553080	goes through the publication process?
1553080	1556480	And they found that only in one case did that happen out of the 300-odd papers that they
1556480	1557480	examined.
1557480	1560100	But what that gave us was an annotated list of COVID papers.
1560100	1563280	So we could then take that and ask if it had the same relationship, and it actually didn't
1563280	1564800	have the same relationship.
1564800	1570460	So for the subset of the literature in early 2020, COVID papers were being published quickly
1570460	1574400	regardless of how much text changed between the preprint and published version.
1574400	1578280	So this was kind of an interesting way to explore how publishing was happening.
1578280	1584080	So for those of you who have had the opportunity to have papers go through peer review, can
1584080	1588800	you guess what the most common linguistic change is, if we just look at word-level linguistic
1588800	1591200	change during the publishing process?
1599200	1602200	Has anyone ever had to add supplementary or additional data?
1603000	1604480	It's not the most common.
1604480	1608280	The most common is actually, oh, no, it is the most common, yeah.
1608280	1609280	So additional and file.
1609280	1613320	So on the right here is what's enriched in the published literature, and the left is
1613320	1615280	what's enriched in preprints.
1615280	1618800	So file and additional and supplementary are all pretty high at the top.
1618800	1624280	So when people are changing their papers, we can infer that probably they're often changing
1624280	1628040	the, you know, they're adding stuff to the supplement, but maybe they're not adding that
1628040	1629640	much to the main paper.
1629640	1632560	The other stuff that's in there is kind of interesting, like fig and figure.
1632560	1636640	So because journals have different styles, the plus-minus symbol and the em dash.
1636640	1642960	So you can see the artifacts of typesetting, but this gives a way to kind of understand
1642960	1643960	what's on each side.
1643960	1648360	And this, I should say, we've done this, so this analysis is using only preprint published
1648360	1649360	pairs.
1649360	1652760	If you do the same thing with all of BioRxiv and all of PubMed, you essentially find field
1652760	1653760	differences.
1653760	1655400	So some fields use BioRxiv more than others.
1655400	1658840	So this is the more carefully controlled than that.
1659040	1662960	The other thing that we've done, just if you happen to have a preprint yourself, we have
1662960	1671720	this web server that does a linguistic comparison between a selected preprint and all of PubMed
1671720	1677320	and says, okay, well, here's journals that publish linguistically similar papers.
1677320	1681640	Here's papers that are linguistically similar to yours, and this has a secret feature.
1681640	1684400	So what we encourage people to do, and we designed it to try to get people to upload
1684400	1686920	their preprint, but then people are like, well, I have a preprint, but it's on archive
1686920	1688360	and you don't support archive.
1688880	1691720	So the secret feature, which you can also drag a PDF over the search box if you want
1691720	1698880	to, but we don't generally advertise that because the goal was to get people to post
1698880	1701560	preprints so they could use the service, but we don't support archive.
1701560	1705760	So if you have an archive preprint, we'll allow you to put, to drag it over the search
1705760	1710520	box, but no other PDFs.
1710520	1714800	So this came out last year, and there's, again, a GitHub associated with it if you want to
1714800	1718480	see all the kind of exploration that we did on the way.
1718480	1724480	David had a follow-up paper that I'm really excited about where he looked at, he looked
1724480	1731200	at the, what words change their meaning over time, and in the last 20 years of scientific
1731200	1732200	publishing.
1732200	1735520	So there's an application associated with that as well that I should have put the link
1735520	1737800	on here, but didn't, that we call WordLabs.
1737800	1742640	So if you go to our lab and look for WordLabs, you'll find that, and it's really interesting.
1742640	1747520	So we see things like hallmarks of new technologies, like, you know, CRISPR has a linguistic shift.
1747520	1753080	We also see a lot of pandemic-associated words have linguistic shifts.
1753080	1756760	So if you're interested in understanding how our language changes, that's also something
1756760	1759040	that David did.
1759040	1768200	Okay, and then I know this is a less medically-related audience than most of the places that I speak,
1768360	1774000	but one of the things that I thought I wanted to share was sort of how some of this basic
1774000	1777120	science or sort of the techniques that we develop in this basic science can contribute
1777120	1780040	to changes in how healthcare gets delivered.
1780040	1783480	And so this is also something that we think about, right?
1783480	1784960	Remember our business is serendipity.
1784960	1787080	Yes, sometimes that's in research, right?
1787080	1790560	Whether that's sort of me telling you how papers change, so that you can think about
1790560	1795160	how you would change your paper in response to peer review, just add more additional files.
1795160	1799200	But sometimes that's, you know, in care, in clinical care, right?
1799200	1802720	So that someone, you know, you can imagine a patient comes in, there might be reasons
1802720	1804720	that that patient might need to receive a different treatment.
1804720	1807920	Could we provide that kind of information at the point of care?
1807920	1815000	So this is a big focus at our med school and our health system that's associated with our
1815000	1821440	med school, University of Colorado Health, has an entire program in clinical intelligence.
1821440	1827000	This is sort of the idea that I like to highlight as sort of serendipity is like the right moment
1827000	1830440	at the right time to make the right decision.
1830440	1835600	In most health systems, if someone is going to get something called pharmacogenomic genetic
1835600	1836600	testing.
1836600	1839580	So the idea here is people have different variants in their genome.
1839580	1842640	Some of those variants can affect how you metabolize drugs, how you respond to different
1842640	1844280	drugs.
1844280	1849520	It's not terribly common that people get tested for pharmacogenomic variants, because if you
1849600	1854800	go get, if you are going to a hospital and, you know, you need to have a stent inserted,
1854800	1859080	one of the common treatments is Plavix, well, there's a, there's an interaction between
1859080	1862760	Plavix and a certain genetic variant, a set of genetic variants where you metabolize the
1862760	1865680	drug differently, and it doesn't work for you, which means you're not getting the benefit
1865680	1869800	of reducing your heart attack risk, or your clot risk.
1869800	1874560	And so, but most people don't get this testing, because if a physician orders the testing,
1874560	1878840	they get a 70 page PDF back, then they have to take that 70 page PDF and go to a table
1878840	1882560	like this, read everything related to the drug they're about to prescribe, to prescribe
1882560	1885360	on the table, and then understand if it applies, right?
1885360	1888160	That is not a common thing for a physician to do.
1888160	1891480	Providers don't get reimbursed for that type of type of work.
1891480	1895320	What's happening at the University of Colorado and UC Health is we've got clinical decision
1895320	1898280	support built into the electronic health record around this stuff.
1898280	1902640	So this is the same thing, except instead of a 70 page PDF, plus having to look at this
1902640	1908240	table, if a provider were to go in and order Plavix for an individual who's not going to
1908640	1912160	benefit from it, it pops up an alert that says, look, we recommend you remove this because
1912160	1916000	it's not going to work, and read about why it's not going to work if you want to, but
1916000	1919200	we recommend you apply one of these alternatives that will work for this patient.
1919200	1924520	And so this is serendipity, but not just in research, in clinical care.
1924520	1928400	And this is, if you're interested in this kind of story, this is another one.
1928400	1932920	This was an individual, different condition, where there was a question about drug efficacy.
1932920	1936120	This is a story from UC Health.
1936120	1940480	And in this case, the provider keyed in an order, and an alert popped up that says, oh,
1940480	1943280	this person's going to need a different dose.
1943280	1946640	And that was helpful to the provider to make that decision.
1946640	1951400	One of the things I've had the privilege of doing over the last couple of years is focusing
1951400	1952400	on this program.
1952400	1957000	So a lot of faculty in our department work in this program, and about a year and a half
1957000	1962480	ago, I guess two years ago, the previous director left, and so I ended up as the interim director.
1962480	1964920	So I've gotten to know this program pretty well.
1964920	1968080	So this is our Colorado Center for Personalized Medicine.
1968080	1970800	We have a biobank study that this is all tied to.
1970800	1975080	So if someone comes in, they can consent to have their sample collected for the biobank.
1975080	1977920	We have a robust return of results pipeline built on that.
1977920	1980600	So our biobank is growing pretty rapidly.
1980600	1982720	Our sample increase is picking up a lot.
1982720	1985800	But then the other thing we ask is, how is this making a difference in care?
1985800	1988560	So essentially, how many of these alerts are actually firing?
1988560	1994120	So over the last year, we've had about 1,000 patients who've had an alert fire at some
1994320	1996360	point in clinical care.
1996360	2000200	That's a tenfold increase over our entire previous history.
2000200	2006280	And that's been powered because we've recently focused on getting these results back into
2006280	2007560	the EHR in a structured way.
2007560	2011800	So we've seen almost 100-fold growth, actually more than 100-fold growth year over year.
2011800	2015800	We'll probably have 210,000 results in the electronic health record at sort of the two-year
2015800	2017080	mark.
2017080	2022080	And what this means is that if you're interested in studying this type of process in terms
2022080	2025600	of care delivery, if you're interested in studying how physicians respond, if you're
2025600	2030560	interested in looking for new cases where there are these sort of drug-gene interactions,
2030560	2034240	we have the ingredients to do that at Colorado in a way that no one else that I'm aware of
2034240	2035720	does.
2035720	2038200	And so this program continues to grow.
2038200	2039240	I'll just give you one.
2039240	2043320	This is actually a real story that happened over the last few months.
2043320	2044320	So this is a stock photo.
2044320	2049280	I cannot show you a picture of the patient, but a patient came in to a community oncology
2049280	2050280	clinic.
2050480	2052160	And this works across the entire UC health system.
2052160	2053680	So it's not just an academic hospital.
2053680	2057360	This is a major health system that serves the Mountain West.
2057360	2060280	So this patient came into a community oncology clinic.
2060280	2067480	They were prescribed a drug that based on their genetic variants would have created
2067480	2073520	a significant risk of life-threatening complications.
2073520	2081400	Our team noticed this, sent a message to the provider, and then the patient alert actually
2081400	2085000	fired and recommended a reduced dosage of the drug.
2085000	2088000	The oncologist actually did proactively reduce the dose.
2088000	2093680	So the person started at a different dose than would traditionally be used.
2093680	2095600	Even at that dose, they didn't tolerate it very well.
2095600	2097840	So they had to further reduce the dose.
2097840	2101200	In these types of cases, you can imagine what happens if you start at the highest sort of
2101200	2105920	the traditional dose at which these drugs can be for individuals with this particular
2105920	2107720	variant can be lethal.
2107720	2111680	And so this is a case where, you know, yes, I told you there's 1,000 alerts, but each
2111680	2114360	of those 1,000 alerts is some story like this, right?
2114360	2119640	And so it's nice to see this actually being used to deliver care at scale.
2119640	2123040	And so we're doing this, this is all informatics, right?
2123040	2127160	You can get all of this serendipity with sort of none of this here has machine learning
2127160	2130480	built into it, but it's going to.
2130480	2133920	And as we think about that, I think it's really important not just to sort of think
2133920	2137360	from the machine learning point of view, but to really think about practical clinical care
2137360	2138360	pathways.
2138360	2144160	So this is a piece from Siddhartha Mukherjee that sort of, if you're interested in AI and
2144160	2149040	medicine, I realize this is dated now, but it's still worth reading.
2149040	2154320	And it's also weird that five years is old, but it's still worth reading.
2154320	2157800	It has a quote from Geoffrey Hinton, sort of says they should stop training radiologists
2157800	2160440	right now.
2160440	2163280	And why would someone say this, right?
2163280	2165480	Well, so Geoff Hinton's looking at the literature, right?
2165480	2168760	So they're just trying to collect some literature from around the same time.
2168760	2171680	So this is sort of saying, look, deep learning is going to completely transform healthcare.
2171680	2174840	It's going to change how we care as we know it.
2174840	2180000	Another sort of similar example, more examples, everything you read in the literature, deep
2180000	2181000	learning.
2181000	2183760	Like, I mean, now we're all into large language models, but at the time these image models
2183760	2185600	were going to completely transform healthcare.
2185600	2188760	Well, you might ask, is there anything they're not good at?
2189080	2191440	Like they're good at everything, right?
2191440	2197000	Chihuahuas and blueberry muffins, not terribly good here.
2197000	2202520	This one's kind of wild.
2202520	2204840	So I perceive the thing on the left as a panda.
2204840	2208160	It looks like a picture of a panda.
2208160	2211640	I don't really perceive this as much of anything.
2211640	2216520	We're going to add, you know, this plus seven thousandths of this.
2216520	2222120	What do you think the output is going to look like?
2222120	2223120	A sloth.
2223120	2224120	A sloth?
2224120	2225120	Any Gibbons?
2225120	2226120	Anyone for Gibbon?
2226120	2227120	A bear.
2227120	2228120	A bear?
2228120	2229120	Okay.
2229120	2230120	So this is what it looks like.
2230120	2231120	It's a Gibbon.
2231120	2232120	Clearly a Gibbon.
2232120	2233120	No question.
2233120	2234120	That's a Gibbon.
2234120	2235120	A monkey.
2235120	2236120	Okay.
2236120	2246080	So it doesn't look like a Gibbon to me, but our neural network is exceedingly convinced.
2246080	2249200	And the reason this works, right, is because the decision boundaries in these neural networks
2249200	2252360	are sort of nonlinear, and you can end up pretty close to a decision boundary without
2252360	2254200	really knowing it.
2254200	2258160	This is another example, which I think is just a lot of fun, so I have to throw it in.
2258160	2263040	So this is someone who was like, well, can I just make adversarial, like, sticker?
2263040	2266960	Like, can I have a sticker that I can put on something and have a deep neural network
2266960	2269360	perceive it as something, even if it's not?
2269360	2272600	So this is the toaster sticker.
2272600	2276960	And so you can give, you can put the toaster sticker on a table next to what I perceive
2276960	2281480	to be a, I perceive this to be a toaster sticker on a table next to a banana.
2281480	2285320	I perceive this to be a banana on a table.
2285320	2287240	Neural network classifier, banana on a table.
2287240	2288240	It's good.
2288240	2289960	Stick the toaster sticker next to it?
2289960	2290960	Absolutely a toaster.
2290960	2295520	You can imagine doing the same things with stop signs, you can imagine doing the same
2295520	2300960	things with other fun technologies in the world of self-driving cars or deep neural
2301120	2302120	networks or vision.
2302120	2306200	Okay, so let's go back to the automated radiologist finding pneumonia.
2306200	2309200	This was one of the examples I showed you before.
2309200	2314720	There's a, John Zek is a guy who writes blog posts that are really good and then converts
2314720	2316600	them into papers.
2316600	2321480	So this was a blog post from John Zek, which then became a paper that sort of said, okay,
2321480	2323120	why is the system actually working?
2323120	2325200	Like what's happening here?
2325200	2331080	So he went to the, to the images and tried to understand, you know, what part of the
2331080	2333920	image is contributing to the prediction of pneumonia?
2333920	2338480	Well in this case, a positive, and this should probably find pneumonia.
2338480	2341760	There's high density in the lung here.
2341760	2342760	You can say, well, okay.
2342760	2349240	Oh, and I should say a positive number is suggestive of pneumonia, a negative number
2349240	2350240	is not.
2350240	2351240	Okay.
2351240	2355360	So what are the positive numbers?
2355360	2362720	The most positive numbers, bottom, yeah, where there's this interesting stripe, the interesting
2362720	2366800	stripe that's kind of unique characteristic of the scanner, the one that you might see
2366800	2371440	if you were in a department, if the scanner was placed proximal to sort of where pneumonia
2371440	2373840	diagnoses were usually occurring.
2373840	2374840	Yeah.
2374840	2378240	So, so, so that's interesting.
2378240	2380520	Here's another one.
2380800	2384320	I should say these probabilities are low, but the previous one was in the 99th percentile
2384320	2385320	for pneumonia.
2385320	2387320	This one's in the 95th percentile.
2387320	2388320	Yeah.
2388320	2389320	Portable?
2389320	2390320	Portable.
2390320	2392600	Someone's not well enough to go to the scanner.
2392600	2395860	Not a good sign for their health, could indicate pneumonia.
2395860	2399320	Probably not the part of the image you want to be looking at.
2399320	2405200	And so, you know, I think this to me, I was also reading this every once in a while, I
2405200	2408960	read the comment section of blog posts on the internet, which I do not recommend, but
2408960	2409960	there was one on big data.
2409960	2414320	You know, there was this blog post on big data and I'm like, well, it was like why statistics
2414320	2416800	don't matter in the era of big data or something like that.
2416800	2421040	And I'm like, okay, I'll, you know, and then I'm like, I have some disagreements with this.
2421040	2425720	Then I go to the comment section, I find this one, which I really agree with.
2425720	2429680	On big data, data collection biases are always larger than statistical uncertainty.
2429680	2433560	And I think this is why I sort of, you know, you can have these models that perform robustly
2433560	2436080	around a lot of these comparisons that still struggle.
2436200	2439240	And then I read, who made the post, and it's this guy named Daniel Himmelstein, which probably
2439240	2442160	doesn't mean anything to you, but he happened to be a postdoc in my lab.
2442160	2446440	And I'm like, dude, put this in the comment section of the internet, people should actually
2446440	2447440	read it.
2447440	2451120	So now I put it on slides, so at least someone can see it.
2451120	2454600	Okay, so how do we design systems that work?
2454600	2459080	So you know, for AI and medicine, regardless of what we're using, I think we need to have
2459080	2461520	some principles that we think about.
2461520	2465240	We just did something that sort of tries to go on this path.
2465400	2466960	So this is a little bit different.
2466960	2471800	So this is a piece from Nature, but it talks about a preprint that we put up earlier this
2471800	2477600	year, where we were trying to use GPT-based models, so in this case, GPT-3, to revise
2477600	2479280	academic manuscripts.
2479280	2483440	One of the things that we see all the time is, well, now it's really common, right?
2483440	2490240	People are developing services that can use GPT-3 or ChatGPT, or GPT-4 through ChatGPT
2490240	2491240	to revise your manuscript.
2491240	2493400	Well, what are the issues with those?
2493560	2496360	So our experience putting this together and running a bunch of test manuscripts through
2496360	2500280	it is that, yes, it is good at clarifying language.
2500280	2502800	There's a lot of things it can help you with.
2502800	2507480	It actually caught an error in an equation, which I was pretty darn impressed with.
2507480	2510640	On the other hand, it also makes stuff up.
2510640	2514480	And so a bit of a challenge if your idea is that you're just going to use this.
2514480	2519240	And I think, you know, I use this as an example because it's really trivial.
2519240	2523360	You can try it out yourself and see how it works, and you can get these examples yourself.
2523360	2526840	The same things are going to be true when we use this in medical context, so we need
2526840	2527840	to think about them.
2527840	2531440	I think thinking about them and trying them and experimenting in a low-risk environment
2531440	2534440	before we move to a high-risk environment is usually a good idea.
2534440	2537440	So what are the principles that we've kind of come up with as we think about this?
2537440	2542440	So first, we really do aim for kind of an augmentation, not replacement.
2542440	2547760	And what that means is, you know, when we apply this to manuscripts and we try to get
2547760	2552520	it to improve it, you know, we actually applied it to the manuscript about the tool.
2552520	2556200	And when we did that, it made up this thing that we had done that we had fine-tuned the
2556200	2558240	model on manuscripts of a similar type.
2558240	2561920	Yeah, you should absolutely fine-tune the model on manuscripts of a similar type.
2561920	2563200	Makes a ton of sense.
2563200	2564200	We didn't do it.
2564200	2568360	You probably shouldn't report that you did it in the manuscript.
2568360	2572280	So we're really thinking like, you know, okay, this is not like you're going to plug it in,
2572280	2573280	you're going to be done.
2573280	2576760	It's really, you need to design it around kind of an augmentation capability.
2576760	2580000	You've got to carefully consider your use cases.
2580000	2583280	You know, if it's easy to take the output, it's hard to compare.
2583280	2584800	It's probably not good, right?
2584800	2589240	Because you're creating the opportunity for a mistake that you don't need to create.
2589240	2591960	We really like to start with these kind of simple solutions and approaches and layer
2591960	2593320	complexity only as needed.
2593320	2600080	So in this case, you know, we start with some pretty simple prompts and then have the ability
2600080	2601280	to add complexity.
2601280	2605200	But usually we just kind of try to keep it relatively basic and simple.
2605200	2606200	The workflow is simple.
2606640	2608360	You can proof of concept it out really quickly.
2608360	2610440	And the most important thing is preserving attribution.
2610440	2611680	Like where did the content come from?
2611680	2615760	If you're thinking about this in a clinical setting, you know, what was provided and when?
2615760	2618000	And you know, did it come from an AI or a human first?
2618000	2621520	Because that's really going to matter as you're thinking about evaluating these workflows.
2621520	2625200	In academic writing, I think it's going to, you're going to want to keep track of whether
2625200	2628960	something came from an AI based system or if you wrote it.
2628960	2631600	I think this is, more and more journals are starting to require this.
2631600	2633520	It's going to be important.
2633520	2639160	But I just think like these are key principles that I would recommend keeping in mind.
2639160	2641680	And then finally, I just want to give you an idea of what the environment is like about
2641680	2642680	at Colorado.
2642680	2645840	Because some of you may one day look for future jobs and I figure you should know something
2645840	2646840	about us.
2646840	2647840	We're not at Boulder.
2647840	2653040	We're at the Anschutz Medical Campus, which is kind of between the airport and the Denver
2653040	2656360	airport and Denver itself.
2656360	2659240	We are a major academic medical center.
2659240	2663500	And like I said, we're not at Boulder, which is the thing that people most frequently get
2664480	2665780	confused about.
2665780	2670980	On July 1st of last year, we actually launched a new department of biomedical informatics.
2670980	2674020	And you know, we're trying to hire and put together a faculty that are focused on this
2674020	2677940	idea of kind of making serendipity routine, like how do you surface the right information
2677940	2680220	at the right time.
2680220	2685260	Our 30, we're now at 31 faculty, we have a new person starting in May, that will get
2685260	2690340	us to 32 faculty, have about $65 million in extramural research, just for faculty who
2690340	2695340	are PIs, on which faculty in the department are PIs, there's a lot of additional collaborative
2695340	2697620	funding that's not included in this.
2697620	2703380	We have expertise kind of across the spectrum from precision medicine, through kind of physiological
2703380	2706060	modeling, we have folks who think about human computer interaction, because if you want
2706060	2708740	to deploy this stuff in the clinic, you should really think about how humans are going to
2708740	2713580	interact with it through kind of electrical engineering, medical imaging and AI.
2713580	2716940	So there's a lot of faculty working in this area.
2716940	2721100	This I just collected some stuff from early last year.
2721100	2724660	And that just sort of like, okay, when our faculty mentioned in the press, so there's
2724660	2732540	stuff in MIT technology review nature, LeMond, Deutschland, I can't remember whatever the
2732540	2735500	German public radio station is.
2735500	2738700	So you know, you have a bunch of internationally renowned experts who are at Anschutz, we don't
2738700	2739700	happen to be at Boulder.
2739700	2744740	You know, to me, it's like the difference between Georgia Tech and Georgia, I feel like
2744740	2749660	they're different institutions, we should probably occasionally recognize that.
2749660	2753460	The other thing that sort of we focused on when we were creating the department, if you
2753460	2757060	read the sociology literature from the 80s, which I suspect all of you do on a regular
2757060	2762020	basis, there's this article that I actually think you should read from the sociology literature
2762020	2764740	from the 80s called the mundanity of excellence.
2764740	2768260	So this is someone who essentially studied swimmers at many different levels and asked
2768260	2772580	what differentiates swimmers at one level from another level.
2772700	2776260	There's a few different principles that come out, but one of the key ones is that excellence
2776260	2778620	requires qualitative differentiation.
2778620	2782180	And what I mean by that is, you know, you're not going to move up a level in swimming competitions
2782180	2784020	by swimming to extra labs.
2784020	2785020	That's not how it works.
2785020	2788020	What you're going to do is you're going to focus on your form, you're going to, you know,
2788020	2790920	you're going to approach the sport differently, you're going to focus on getting rest the
2790920	2794340	night before the meet, like that's the stuff that people do at higher levels that they
2794340	2796660	don't do at lower levels.
2796660	2799380	And as we think about a department, we had to ask like, what's our, okay, what's our
2799380	2800380	qualitative differentiator?
2800380	2804220	How are we not just like another biomedical informatics department that just happens to
2804220	2805580	have more money or something, right?
2805580	2808460	Like that's not, that's not a real differentiator.
2808460	2813580	And so what we thought about was creating promotion and tenure guidelines that have,
2813580	2815720	that are focused on real world impact.
2815720	2820700	So one of our bullet points in our departmental sort of idea of impact, there is a bullet
2820700	2822220	point that includes publication.
2822220	2825460	It's possible, you can do it, we can care about it.
2825540	2830780	But there's also technology development that gets deployed locally, nationally or internationally.
2830780	2834100	Software shows up, changes in policy because of your work.
2834100	2837100	All of that is included in and counts for impact.
2837100	2841340	Now, probably not all of you will look for tenure track faculty positions in our department.
2841340	2846660	But if you were to come train or send folks to train with us, I think it's important to
2846660	2848340	know like that filters down, right?
2848340	2850940	If that's how faculty are evaluated, that filters all the way down.
2850940	2854340	So there's an emphasis on real world impact that we have that I think can be our kind
2854340	2855340	of qualitative differentiator.
2855340	2859620	And I'll just say, we have a really good training environment.
2859620	2861100	So we have strong connections with UC Health.
2861100	2864660	So I told you earlier about one of the UC Health programs that we work closely with
2864660	2865660	them on.
2865660	2871220	And Children's Colorado, which is a nationally renowned pediatric cancer hospital, not pediatric
2871220	2872220	hospital.
2872220	2875940	I know the pediatric cancer people work in that space.
2875940	2876940	So we have those tight connections.
2876940	2881140	If you're interested in seeing your work translate to care, we have, if you're interested in
2881140	2883620	using genetics to guide care, I think we have one of the best programs in the country
2883620	2885340	through CCPM.
2885340	2888740	We have a diverse and internationally recognized faculty.
2888740	2892680	One of the things that's sometimes a little bit odd for folks at, you know, our tenure
2892680	2899620	track faculty in DBMI are actually majority women, which I think is uncommon at, in, in
2899620	2901380	our field.
2901380	2904900	And then if you like the climate and you, it's a little bit humid here.
2904900	2908340	We don't have that level of humidity, but we do have more hours of sun per year than
2908340	2909340	Miami and San Diego.
2909340	2911860	So if you're interested in the environment around you, we've got that.
2911860	2914780	And then we've got abundant outdoor activities.
2914780	2916580	This is one example of the programs that we have.
2916580	2919020	So this is our computational science PhD program.
2919020	2924740	There's also a postdoc training grant associated with the same thing.
2924740	2927780	So if you're interested in this type of thing, feel free to look us up.
2927780	2931580	You can always drop me an email and I can try to connect you with folks too.
2931580	2935860	And then with that, I just want to thank the people who make this possible.
2935860	2940140	So the members of the lab, we really have a kind of robust culture in the lab of sort
2940140	2944700	of sharing the work that's happening and kind of thinking through each other's projects
2944700	2946420	in ways that are really helpful.
2946420	2947420	We also do code review.
2947420	2952060	So people really pitch out, pitch in together, the department of biomedical informatics and
2952060	2955660	my leadership team, and then the folks in CCPM, since I shared some of that work, and
2955660	2958900	then the folks who give us money, I'd be happy to take whatever questions you have.
2965860	2975340	For the radiology, XAI explainability, did they end up using my grad cam as like how
2975340	2981380	they liberated the convolutional layer for the confluence?
2981380	2982380	Yeah.
2982380	2985580	I don't remember what strategy they used.
2985580	2991220	And I also don't remember, you know, the saliency map.
2991220	2994580	I think it's a saliency map, but I'm not 100% sure.
2995580	3000300	Yeah, there's a, so John posted that first as a blog post, there's now like a PLOS medicine
3000300	3001300	paper, I think.
3001300	3002300	Yeah.
3002300	3003300	I think I probably, yeah.
3003300	3004300	Yeah.
3004300	3007020	And I wrote this when it was the blog post and not when it was PLOS medicine paper, which
3007020	3009020	is what I'd look at.
3009020	3010020	Yeah.
3010020	3011020	Yeah.
3011020	3019540	So with the strange like explainability maps, did that happen even with like augmentations
3019540	3022100	that would rotate or like zoom in, zoom out?
3022100	3026300	Like was that with that, it still happened or with a train without that?
3026300	3032540	Because like, I was thinking like assume in augmentation might solve that bottom band
3032540	3033540	problem.
3033540	3034540	Yeah.
3034540	3036780	So I was wondering if that still.
3036780	3039180	I'm guessing, so this is Andrew Ng's stuff.
3039180	3040980	This was the one that he was criticizing.
3040980	3048820	I think this was just chest x-rays and I don't think they, I don't remember them doing at
3048820	3052020	least like a patch-based augmentation or anything like that.
3052020	3053020	Yeah.
3053020	3057100	And I would say now some of the techniques that are more sophisticated are likely to
3057100	3058100	control for some of that.
3058100	3061540	I mean, the other thing you could do is you can also just like do some adversarial training
3061540	3064500	around the location of the scanner.
3064500	3067940	The challenge with that is you need to know to do it and to know to do it, you have to
3067940	3070220	like have someone who's an expert probe your data.
3070220	3076300	And I think sometimes when we come to things from a computer science perspective, I do
3076300	3082340	think sometimes we get really excited that something is working and especially if it's
3082340	3091780	working as well as a human and maybe we get a little bit ahead of ourselves and aren't
3091780	3096660	skeptical enough about our own results.
3096660	3101020	So with the pharmacogenomics, the alert system that you pointed out.
3101020	3104100	So how often do you get sort of false alerts, right?
3104100	3112580	So because sometimes you can get an alert that a physician may not be really interested
3112580	3114940	in or think is valid, right?
3114940	3115940	Yeah.
3115940	3119940	So we designed this pretty carefully.
3119940	3125580	So we could bump up our alert number by just whether or not someone's getting a relevant
3125580	3128380	prescription, fire the alert.
3128380	3129620	That'd be great for our metrics.
3129620	3133620	On the other hand, not terribly useful for care and people would learn to ignore it.
3133660	3135060	So we're pretty focused.
3135060	3137260	So most of the alerts are non-interruptive.
3137260	3139500	So the idea is an 80-20 rule.
3139500	3144140	So only 20% of the alerts should be interruptive, 80% should be non-interruptive.
3144140	3149660	And because this isn't based on a predictive model, it's pretty straightforward to make
3149660	3152220	sure it fires largely at times when it's relevant.
3152220	3156180	So restricting kind of the clinic in which you can fire and that sort of stuff.
3156180	3159780	However, one of the really nice things about UCHealth, so I'll go back on my advertising
3159780	3162500	pitch for why you should come to Colorado and work at Colorado if this is something
3162500	3166660	you're interested in, UCHealth thought about this in advance.
3166660	3169620	So years ago, they built a virtual health center.
3169620	3173500	And if you want to look it up, you can look up the UCHealth virtual health center.
3173500	3178060	The guy who, to my understanding, put this together and sort of was visionary behind
3178060	3183060	it is a guy named Rich Zane, who's our chief innovation officer through the hospital.
3183060	3187780	And what they did there is they have nurses and clinicians who work offsite, but who are
3187780	3192900	available to look at these types of systems before they flow to people who are onsite
3192900	3194260	providing care.
3194260	3199260	And where this became really useful, so is anyone aware of kind of the EPIC sepsis model
3199260	3202820	thing that blew up maybe a year or two ago?
3202820	3203820	No.
3203820	3206860	So EPIC is one of the major providers of electronic health record systems.
3206860	3211300	They have a sepsis model, and that sepsis model is pretty noisy.
3211300	3216380	It likes to, it alerts probably more frequently than it should, and it misses cases it shouldn't
3216380	3217420	miss.
3217420	3223020	So there was a team at CU before my time, led by Tal Bennett, that had evaluated this
3223020	3230180	model and found that it had some predictive quality, but maybe it wasn't ready.
3230180	3232460	I want to be careful what I assert here.
3232460	3237940	It had some predictive quality, but deployed in practice at scale could have created a
3237940	3240620	lot of unnecessary burden on providers.
3240620	3243620	Well, what they did, because they have the virtual health center, is they're able to
3243620	3251060	deploy that model plus others in the virtual health center, have it alert nurses and clinicians
3251060	3256260	there, and then have them look at it carefully in the virtual health center and only send
3256260	3260100	the notice over to the folks who are working at the bedside if it's actually going to be
3260100	3261100	useful.
3261100	3263620	So a reason that could be good to work at Colorado, if you're interested in kind of
3263620	3267460	predictive analytics and deploying this stuff in practice, is you can have a model that's
3267460	3268620	not perfect, right?
3268620	3273820	It doesn't have to be good enough to hand to a provider at the bedside.
3273820	3277460	Because of the virtual health center, you can really proof of concept it out there,
3277460	3280060	improve it, understand how you can improve its predictive quality, and then deploy it
3280060	3281060	when it's ready.
3281060	3282660	But you can still get the benefit in the meantime.
3282660	3289220	So I guess I'd say, yeah, I think our noise level is pretty low on these alerts, but we
3289220	3297900	do have a system in place for noisier stuff if people want to deploy it.
3297900	3298900	It's fun to be back in Athens.
3298900	3299900	Time to go dogs.
3299900	3304220	I don't know what else I should say, but I'm just excited to be back.
3304220	3326100	I was really tickled when I got the invite, it was wonderful.
