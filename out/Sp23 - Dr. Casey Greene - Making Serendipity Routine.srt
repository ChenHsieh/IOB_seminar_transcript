1
00:00:00,000 --> 00:00:15,100
leading to his first paper, which is affiliated with the Department of Genetics here at UGA.

2
00:00:15,100 --> 00:00:21,300
The Green Lab develops machine learning methods to integrate disparate large-scale datasets,

3
00:00:21,300 --> 00:00:26,020
develops deep learning methods for extracting contacts from these datasets, and then brings

4
00:00:26,020 --> 00:00:32,140
these capabilities to molecular biologists through open and transparent science.

5
00:00:32,140 --> 00:00:38,580
Of particular note to folks interested in large language models, like CHAT-GPT, Dr.

6
00:00:38,580 --> 00:00:44,260
Green and others recently posted a preprint on how AI might be able to help us write and

7
00:00:44,260 --> 00:00:47,420
revise our academic manuscripts.

8
00:00:47,420 --> 00:00:54,420
Please join me in welcoming Dr. Casey Green to UGA.

9
00:00:54,420 --> 00:01:04,700
Okay, thank you, yeah, it's good to be back.

10
00:01:04,700 --> 00:01:09,380
I actually spent a summer working in this building in the Department of Genetics.

11
00:01:09,380 --> 00:01:11,860
Everything around this building looks different now, but somehow the building still looks

12
00:01:11,860 --> 00:01:14,500
the same.

13
00:01:14,500 --> 00:01:19,980
I'm excited to get a chance to share some of what we've been doing in our group and

14
00:01:19,980 --> 00:01:23,140
then to share some of what's going on at the University of Colorado and sort of give you

15
00:01:23,140 --> 00:01:27,980
an idea of what the ecosystem is like where we are.

16
00:01:27,980 --> 00:01:34,180
I think it's always important to think about kind of what our role as informaticists in

17
00:01:34,180 --> 00:01:40,140
science is, like what do we bring to the ecosystem, and I like to think of what we contribute

18
00:01:40,140 --> 00:01:42,900
as essentially serendipity, right?

19
00:01:42,900 --> 00:01:46,580
If we do our jobs well, people will see something in their data that they hadn't seen before,

20
00:01:46,580 --> 00:01:49,020
and they will make a different decision based on that.

21
00:01:49,020 --> 00:01:55,540
So I like to, you know, think about how we can make more kind of serendipitous moments.

22
00:01:55,540 --> 00:02:02,940
I'll start with a brief vignette of a project that we started quite a few years ago now

23
00:02:02,940 --> 00:02:07,860
trying to understand rare diseases.

24
00:02:07,860 --> 00:02:13,500
And in particular, what factors might drive a rare, what systemic factors might drive

25
00:02:13,500 --> 00:02:16,340
a rare disease.

26
00:02:16,340 --> 00:02:22,260
The world when we started this project was one where looking across multiple data sets

27
00:02:22,260 --> 00:02:24,460
remained somewhat challenging.

28
00:02:24,460 --> 00:02:28,980
It's time consuming, you have to deal with batch and technical artifacts.

29
00:02:28,980 --> 00:02:33,380
And so our question when we started, a postdoc joined the lab and the idea was, well, you

30
00:02:33,380 --> 00:02:35,180
know, here's the gap that we're facing.

31
00:02:35,180 --> 00:02:40,060
If you want to analyze multiple data sets at the same time, to identify systemic factors,

32
00:02:40,060 --> 00:02:44,180
that means you're looking at different tissues, probably looking at different cohorts, there's

33
00:02:44,180 --> 00:02:48,060
potentially different disease contexts, might be different controls.

34
00:02:48,060 --> 00:02:50,900
All of this makes your life a lot harder, you can't just kind of make an assumption

35
00:02:50,900 --> 00:02:55,100
that you're going to do three different t tests and be done with it.

36
00:02:55,100 --> 00:02:59,980
And so what this postdoc wanted to do was say, okay, can I find these commonalities

37
00:02:59,980 --> 00:03:03,180
without it taking an inordinately large amount of time.

38
00:03:03,180 --> 00:03:07,780
And she was very interested in not taking an inordinately large amount of time because

39
00:03:07,780 --> 00:03:14,300
the strategy that she used in her PhD, before she joined the group, was developed, she developed

40
00:03:14,300 --> 00:03:17,740
this approach using a modular framework to analyze these data sets, where you essentially

41
00:03:17,740 --> 00:03:23,220
take different data sets, decompose them into modules, and then try to map a module in one

42
00:03:23,220 --> 00:03:27,100
data set to a module in another data set to a module in another data set.

43
00:03:27,100 --> 00:03:31,220
This is possible, if you're an expert in the disease, and you're an expert in all the tissues

44
00:03:31,340 --> 00:03:36,340
that sort of are affected, you can do this, you can say, okay, this is this pathway response

45
00:03:36,340 --> 00:03:39,740
in this tissue, and this is this other pathway response in this other tissue, but it's really

46
00:03:39,740 --> 00:03:40,740
time consuming.

47
00:03:40,740 --> 00:03:47,460
And the complexity grows essentially, with at least the square of the number of modules

48
00:03:47,460 --> 00:03:48,460
that you want to look at.

49
00:03:48,460 --> 00:03:51,380
So you sort of restrict yourself to a modest number of modules if you want to do this in

50
00:03:51,380 --> 00:03:53,580
any practical amount of time.

51
00:03:53,580 --> 00:03:57,060
There are potentially ways to automate this, you know, using over representation analysis

52
00:03:57,060 --> 00:04:00,940
or other strategies to try to, you know, make life easier on the mapping stage, so you don't

53
00:04:00,940 --> 00:04:04,220
spend a lot of time looking at stuff that's unlikely to be fruitful.

54
00:04:04,220 --> 00:04:10,100
But either way, it's challenging, it takes a bunch of time and it requires a lot of expertise.

55
00:04:10,100 --> 00:04:15,980
So what Dr. Jacqueline Tironi did was say, well, what I really want is a module library,

56
00:04:15,980 --> 00:04:21,220
like how could this process or cell type be represented in any data set that I look at,

57
00:04:21,220 --> 00:04:26,100
I'd like to be able to pull that off the shelf, and then take that module library to different

58
00:04:26,100 --> 00:04:30,180
data sets, and then look at each of those data sets in terms of those modules and just

59
00:04:30,420 --> 00:04:31,580
look directly across those modules.

60
00:04:31,580 --> 00:04:37,500
I don't have to now sort of do the module connection after the fact, I can do it upfront.

61
00:04:37,500 --> 00:04:41,460
This would be great, wouldn't it be great if.

62
00:04:41,460 --> 00:04:46,220
And so her hypothesis was that, you know, these modules don't just exist in one data

63
00:04:46,220 --> 00:04:48,180
set, they exist across human biology.

64
00:04:48,180 --> 00:04:54,540
So could she, so her hypothesis was that she would be able to learn these reusable modules

65
00:04:54,540 --> 00:04:58,700
by taking many, many, many different data sets, decomposing those different data sets

66
00:04:58,700 --> 00:05:04,260
into modules, and then sort of learning which modules were necessary to reconstruct the

67
00:05:04,260 --> 00:05:06,180
original data.

68
00:05:06,180 --> 00:05:10,180
And then once she did that, she could do that on a generic collection of data, and then

69
00:05:10,180 --> 00:05:12,860
hopefully be able to use that on her data sets of interest.

70
00:05:12,860 --> 00:05:17,580
And in this case, we're interested in studying a disease called Enka-associated vasculitis,

71
00:05:17,580 --> 00:05:22,060
which is rare enough that if you look at large collections of public RNA-seq data, you don't

72
00:05:22,060 --> 00:05:23,680
find it.

73
00:05:23,680 --> 00:05:28,100
So it's quite a rare disease.

74
00:05:28,500 --> 00:05:31,700
So the idea here was, if she took a whole bunch of generic samples, essentially random

75
00:05:31,700 --> 00:05:35,900
human data that she downloaded from the internet, transcriptomic data, she decomposed that,

76
00:05:35,900 --> 00:05:39,620
she could decompose that into patterns, and then she could take those patterns and we

77
00:05:39,620 --> 00:05:42,980
could apply those to the rare disease data sets of interest, and potentially do sort

78
00:05:42,980 --> 00:05:45,940
of standard statistics with that.

79
00:05:45,940 --> 00:05:52,500
We, just to give you an idea of the data set that we started with, so this is a data set

80
00:05:52,500 --> 00:05:56,100
ReCount 2, I think there's now a ReCount 3.

81
00:05:56,100 --> 00:05:59,220
This is produced by Jeff Leak's group at Hopkins.

82
00:05:59,220 --> 00:06:04,660
And if you wanted 70,000 RNA-seq samples, you can just get 70,000 uniformly processed

83
00:06:04,660 --> 00:06:05,660
RNA-seq samples.

84
00:06:05,660 --> 00:06:11,140
What I like to tell Jackie is that her project had to be successful, because if you think

85
00:06:11,140 --> 00:06:14,260
about the resources we were giving her to start it, if you just benchmark that those

86
00:06:14,260 --> 00:06:18,780
samples probably cost about $1,000 to generate, you know, I like to tell her, look, we gave

87
00:06:18,780 --> 00:06:22,460
you $70 million to start your project, you have to do something with $70 million, right?

88
00:06:23,460 --> 00:06:29,300
So, this was the data that we used, and then we tried quite a few different methods to

89
00:06:29,300 --> 00:06:34,300
extract patterns, including some that we developed in our group, but we ended up coming to this

90
00:06:34,300 --> 00:06:39,260
method called PLYR, which is from Maria Shakina's group at Pitt.

91
00:06:39,260 --> 00:06:44,340
And PLYR is the Pathway-Level Information Extractor, and it does a couple things that

92
00:06:44,340 --> 00:06:45,340
are really nice.

93
00:06:45,340 --> 00:06:50,500
So, it's essentially doing this matrix decomposition, but as you do the matrix decomposition, there's

94
00:06:50,540 --> 00:06:55,580
a couple sort of regularization factors and some penalties in it, and essentially, it

95
00:06:55,580 --> 00:06:59,420
has some sparsity properties that we really like, so the idea is that you want to be able

96
00:06:59,420 --> 00:07:02,740
to explain a dataset with relatively few latent variables.

97
00:07:02,740 --> 00:07:07,300
Also, you want your latent variables to have only a modest number of genes in them.

98
00:07:07,300 --> 00:07:12,740
And finally, if those latent variables can align with a pathway, you'd really prefer

99
00:07:12,740 --> 00:07:16,220
that the latent variable align with a pathway, because anytime you're doing this decomposition,

100
00:07:16,220 --> 00:07:18,940
you're essentially doing some arbitrary rotation, right?

101
00:07:19,180 --> 00:07:25,380
PCA is essentially learning an arbitrary, it's learning a rotation of your data, a reduced

102
00:07:25,380 --> 00:07:28,380
dimension space rotation of your data.

103
00:07:28,380 --> 00:07:33,340
The rotation in PCA is essentially arbitrary, I mean, you've chosen that you want to maximize

104
00:07:33,340 --> 00:07:37,740
the variance on the first axis, but you could have chosen anything else, like ICA, you're

105
00:07:37,740 --> 00:07:41,020
just like, I don't care, just make it small.

106
00:07:41,020 --> 00:07:46,100
So in this case, what it's doing is it's essentially saying, if a pathway can line up with an axis,

107
00:07:46,100 --> 00:07:47,260
let's do that.

108
00:07:47,260 --> 00:07:52,620
And so it's a little bit less, that's the intuition for what it's doing, it's actually

109
00:07:52,620 --> 00:07:56,700
a little bit, the regularization is a little bit different than that.

110
00:07:56,700 --> 00:07:59,340
And what that gives you is a really nice level of interpretability.

111
00:07:59,340 --> 00:08:03,700
So instead of having to think about a module as, like, instead of saying, okay, this cell

112
00:08:03,700 --> 00:08:08,980
type is, these three different axes of variability added in some fraction, usually those cell

113
00:08:08,980 --> 00:08:11,060
types are going to come out as a single axis in your data.

114
00:08:11,060 --> 00:08:17,180
So it makes it much more easy to think through and reason about the solutions.

115
00:08:18,100 --> 00:08:22,260
And so what we essentially did is say, okay, well, we have this enormous collection of

116
00:08:22,260 --> 00:08:26,940
generic human data from the internet, ReCount2, and we have a plier, and we'd like that to

117
00:08:26,940 --> 00:08:30,380
be a machine learning model that we could use in many different biological contexts.

118
00:08:30,380 --> 00:08:34,900
So we named it multi-dataset plier, but because in bioinformatics, everything has to have

119
00:08:34,900 --> 00:08:38,780
a name, we just shortened that to multiplier.

120
00:08:38,780 --> 00:08:41,380
And so this is kind of a multiplier idea.

121
00:08:41,380 --> 00:08:46,860
I'm just going to give you a couple highlight results from the paper that I think help to

122
00:08:47,540 --> 00:08:49,820
kind of give an idea of why you might want to do this.

123
00:08:49,820 --> 00:08:51,500
The paper is pretty exhaustive.

124
00:08:51,500 --> 00:08:55,340
It has a significant deep dive into sort of exactly what's happening in this stuff, but

125
00:08:55,340 --> 00:08:59,580
I'll just give you kind of the high points.

126
00:08:59,580 --> 00:09:03,660
Essentially what we wanted to understand is, does this model learn something we didn't

127
00:09:03,660 --> 00:09:05,740
already know?

128
00:09:05,740 --> 00:09:10,180
And is it better than if you just took data from the disease of interest?

129
00:09:10,180 --> 00:09:13,700
We couldn't get enough data from the disease of interest, which is psychosociative vasculitis.

130
00:09:13,700 --> 00:09:19,540
So we did that analysis in the paper, and we just didn't learn many axes of variability

131
00:09:19,540 --> 00:09:21,020
because there's too little data.

132
00:09:21,020 --> 00:09:24,460
So what we wanted to do is say, well, let's pick something where we could actually learn

133
00:09:24,460 --> 00:09:25,460
something.

134
00:09:25,460 --> 00:09:27,220
Let's sort of give this idea a chance.

135
00:09:27,220 --> 00:09:31,220
And then we said, well, let's imagine we're studying a different autoimmune disease, lupus.

136
00:09:31,220 --> 00:09:35,660
So that's what's here, where it says SLE, this box plot.

137
00:09:35,660 --> 00:09:39,940
What we've done is we've collected all the whole blood data that we could get from individuals

138
00:09:39,940 --> 00:09:45,260
with lupus that was publicly available to create one collection of data.

139
00:09:45,260 --> 00:09:48,780
Then what we've done with RECOUNT2, which is the generic human data from the internet,

140
00:09:48,780 --> 00:09:53,820
is we've taken RECOUNT2 and we've subsampled it to be the same size as the lupus set.

141
00:09:53,820 --> 00:09:54,940
So that's the box plot.

142
00:09:54,940 --> 00:09:57,420
The box plot is RECOUNT2 subsampled.

143
00:09:57,420 --> 00:10:00,460
And then where you see the diamond, that's what happens if you just take the complete

144
00:10:00,460 --> 00:10:02,460
collection of data from RECOUNT2.

145
00:10:02,460 --> 00:10:04,300
Okay, so let's label our X.

146
00:10:04,300 --> 00:10:08,220
Oh, so yeah, so these data sets, if you think about the science behind this, this experiment

147
00:10:08,220 --> 00:10:12,300
has these two data sets of the same size, but different composition.

148
00:10:12,300 --> 00:10:16,140
These two are the same composition of the data, but they're quite different sizes.

149
00:10:16,140 --> 00:10:20,780
I think 70 times larger for the diamond than the other.

150
00:10:20,780 --> 00:10:23,580
And so then we can ask, okay, so how many patterns are we learning from our data?

151
00:10:23,580 --> 00:10:27,660
So this method uses the latent variable decomposition, so the number of latent variables, which are

152
00:10:27,660 --> 00:10:29,340
essentially patterns.

153
00:10:29,340 --> 00:10:32,860
You can see that if you want to learn more patterns, there's sort of more, and there's

154
00:10:32,860 --> 00:10:37,140
a heuristic and plier for sort of selecting the optimal number of latent variables, and

155
00:10:37,140 --> 00:10:40,260
we use that heuristic here, it seems like a pretty reasonable heuristic.

156
00:10:40,260 --> 00:10:45,140
It uses cross-validation to essentially ask how frequently you're rediscovering the same

157
00:10:45,140 --> 00:10:47,860
latent variables.

158
00:10:47,860 --> 00:10:52,900
So if you do that, you find that you can learn more latent variables or recurrent patterns

159
00:10:52,900 --> 00:10:56,900
in the data if you have less heterogeneity in your data given a fixed sample size.

160
00:10:56,900 --> 00:11:00,420
I don't think this is going to shock anyone, right?

161
00:11:00,420 --> 00:11:03,900
If I give you a limited amount of data, you would like that data to be as consistent as

162
00:11:03,900 --> 00:11:06,620
possible except for the thing you'd like to vary, right?

163
00:11:06,620 --> 00:11:10,060
That's usually a good place to live.

164
00:11:10,060 --> 00:11:11,340
So that's what we see here, right?

165
00:11:11,340 --> 00:11:15,740
You get more latent variables out of the lupus data than the subsampled recount2 data, but

166
00:11:15,740 --> 00:11:19,460
if I told you, well, you can have really messy data, but you can have a lot of it, now you

167
00:11:19,460 --> 00:11:21,080
can learn a lot more patterns.

168
00:11:21,080 --> 00:11:25,500
So you end up with many fold more patterns that you can learn, the kind of recurrent

169
00:11:25,500 --> 00:11:27,620
patterns across the data set if you have more data.

170
00:11:27,620 --> 00:11:32,780
So you'd rather have less heterogeneity, but sometimes having more samples can overcome

171
00:11:32,780 --> 00:11:36,460
the heterogeneity issue.

172
00:11:37,300 --> 00:11:38,300
So that's the first thing we ask.

173
00:11:38,300 --> 00:11:40,060
So you get kind of more total patterns.

174
00:11:40,060 --> 00:11:47,440
The next thing we can ask is, okay, well, we don't know what complete collection of

175
00:11:47,440 --> 00:11:50,060
processes are transcriptionally co-regulated, right?

176
00:11:50,060 --> 00:11:51,460
This is not something we know a priori.

177
00:11:51,460 --> 00:11:55,940
We can get a collection of processes if we go to Gene Ontology or KEGG or other databases,

178
00:11:55,940 --> 00:11:58,060
but some of those may not be transcriptionally co-regulated.

179
00:11:58,060 --> 00:12:02,620
However, if we're seeing a process that's coming out as transcriptionally co-regulated,

180
00:12:02,620 --> 00:12:04,860
that's probably a positive hit.

181
00:12:04,860 --> 00:12:06,700
And so that's kind of the assumption we're making here.

182
00:12:06,700 --> 00:12:10,820
So this is looking at both the SLE and the ReCount2 data again.

183
00:12:10,820 --> 00:12:16,580
This axis is what fraction of the pathways that we know about are coming back as sort

184
00:12:16,580 --> 00:12:20,580
of aligned with one or more axes in the data.

185
00:12:20,580 --> 00:12:24,660
And so you can see this actually wasn't driven by composition of the data, this was driven

186
00:12:24,660 --> 00:12:25,660
by sample size.

187
00:12:25,660 --> 00:12:30,580
So if you put in more data, you can learn kind of more transcriptional co-regulation.

188
00:12:30,580 --> 00:12:34,060
This obscures a little bit of what's going on here because the processes are a little

189
00:12:34,060 --> 00:12:36,460
bit different.

190
00:12:36,460 --> 00:12:41,740
If you look at what happens in the ReCount2 data, you end up learning, as you get to the

191
00:12:41,740 --> 00:12:45,020
large sample size, you end up learning more granular processes.

192
00:12:45,020 --> 00:12:48,780
So it seems like it's probably that you're covering the same things that are transcriptionally

193
00:12:48,780 --> 00:12:52,740
co-regulated, you just get a higher level of resolution.

194
00:12:52,740 --> 00:12:56,780
And then, so you kind of learn more of the stuff we know we should know about.

195
00:12:56,780 --> 00:13:00,500
So at this point, anything I'm showing you, you could also do with GSEA or any of these

196
00:13:00,580 --> 00:13:03,820
other methods, gene set enrichment analysis, or those types of methods.

197
00:13:03,820 --> 00:13:07,620
But we can also ask, can we learn anything we didn't already know going in?

198
00:13:07,620 --> 00:13:14,620
So this is asking what fraction of the latent variables that are coming back are not associated

199
00:13:14,620 --> 00:13:17,100
with the pathway.

200
00:13:17,100 --> 00:13:23,140
And what you can see is, when you have the sort of more modest sample sizes, about half

201
00:13:23,140 --> 00:13:25,700
of the latent variables that come back were associated with the pathway, and the other

202
00:13:25,700 --> 00:13:27,180
half are potentially novel.

203
00:13:27,580 --> 00:13:31,460
Could be novel biology, it could be a technical artifact.

204
00:13:31,460 --> 00:13:35,420
What you see when you get the large collection of ReCount2 data is, you know, that number

205
00:13:35,420 --> 00:13:36,420
drops to about 20%.

206
00:13:36,420 --> 00:13:40,300
So 80% of the latent variables that are coming back didn't exist in the databases that we

207
00:13:40,300 --> 00:13:42,100
had available to us going into it.

208
00:13:42,100 --> 00:13:46,780
So that's a really nice thing to have in your back pocket if you want to say, well, look,

209
00:13:46,780 --> 00:13:50,580
I want to explore the biology of what's going on in this disease, but I don't want to limit

210
00:13:50,580 --> 00:13:53,020
myself to what people have curated into a database.

211
00:13:53,020 --> 00:13:58,380
So this is kind of a data-driven way to figure out what those modules could be.

212
00:13:58,380 --> 00:14:01,700
And you might also say, well, there's probably just an enormous amount of technical artifacts.

213
00:14:01,700 --> 00:14:05,360
You just told me you gathered a whole bunch of random human data from the internet.

214
00:14:05,360 --> 00:14:09,140
One of the positive things that we saw that was kind of suggestive that this is not driven

215
00:14:09,140 --> 00:14:13,900
exclusively by technical artifacts is actually this proportion bumps up a little bit when

216
00:14:13,900 --> 00:14:17,140
you look at the ReCount2 data, as opposed to the SLE data.

217
00:14:17,140 --> 00:14:21,460
If it was entirely driven by technical artifacts, you'd actually expect this to have fewer latent

218
00:14:21,460 --> 00:14:25,420
variables that were associated only with a known process.

219
00:14:25,420 --> 00:14:28,300
So this was also encouraging.

220
00:14:28,300 --> 00:14:31,980
So this gives us the idea that we kind of learn more unknown unknowns.

221
00:14:31,980 --> 00:14:36,380
And then there's quite a bit more of a deep dive in the paper, like looking at sort of

222
00:14:36,380 --> 00:14:39,540
one of the things that we see is instead of having a cell cycle latent variable, you end

223
00:14:39,540 --> 00:14:43,660
up with different phases of the cell cycle partitioned into latent variables.

224
00:14:43,660 --> 00:14:49,600
But the sort of takeaway was that if you do these machine learning analyses while reusing

225
00:14:49,640 --> 00:14:54,040
data from other contexts, you can get this level of detail that you couldn't get just

226
00:14:54,040 --> 00:14:55,840
analyzing your data alone.

227
00:14:55,840 --> 00:14:59,520
So starting with a whole bunch of other data, learning the pathways and processes there,

228
00:14:59,520 --> 00:15:02,800
and then applying it to your data gives you this higher level of resolution.

229
00:15:02,800 --> 00:15:07,040
There is a bit of an implicit assumption here, which is if the process that you were looking

230
00:15:07,040 --> 00:15:12,240
at is truly unique and only occurs in your setting and no other settings, you can't find

231
00:15:12,240 --> 00:15:16,920
it because it's not going to be present in the variability from other people's data.

232
00:15:16,920 --> 00:15:19,080
I think this is probably rare.

233
00:15:19,560 --> 00:15:23,400
I don't think it's terribly common that there are processes that are so exclusive that they're

234
00:15:23,400 --> 00:15:27,120
only used in one and only one biological context and nowhere else.

235
00:15:27,120 --> 00:15:29,800
But if you believe that to be the case, you should know you will not find it with this

236
00:15:29,800 --> 00:15:32,800
method.

237
00:15:32,800 --> 00:15:34,480
And so then just kind of recapitulating.

238
00:15:34,480 --> 00:15:39,680
So in the past, when Jackie joined the group, she had this modular framework approach, which

239
00:15:39,680 --> 00:15:46,280
is actually really nice and is now used, it's been used in scleroderma and other contexts

240
00:15:46,280 --> 00:15:50,240
to connect pathways across tissues and studies.

241
00:15:50,240 --> 00:15:53,480
But the multiplier approach she developed has some nice advantages.

242
00:15:53,480 --> 00:15:57,520
So she takes this generic human data, recount two, she can train a model, then transfer

243
00:15:57,520 --> 00:16:02,320
it to the datasets of interest, and then just look across those datasets with standard statistics.

244
00:16:02,320 --> 00:16:06,080
So this is an example of one of the things we can do with this.

245
00:16:06,080 --> 00:16:10,040
So this was actually the thing we wanted to do when we started the study.

246
00:16:10,040 --> 00:16:15,200
So these are three different datasets from individuals with ANCA-associated vasculitis.

247
00:16:15,200 --> 00:16:19,840
One of the challenges here is that all of these datasets are microarray-based.

248
00:16:19,840 --> 00:16:22,560
All of our training data is RNA-seq.

249
00:16:22,560 --> 00:16:27,240
A different student in the lab developed a technique to do, if you're interested in taking

250
00:16:27,240 --> 00:16:33,920
sort of machine learning methods and applying them to gene expression data across these

251
00:16:33,920 --> 00:16:41,040
contexts, for many methods, there's reasonable ways to do that transformation that is not

252
00:16:41,040 --> 00:16:47,280
completely horrendous, which is the best advertisement I can get for a method.

253
00:16:47,280 --> 00:16:48,280
But there's quite a few different methods.

254
00:16:48,280 --> 00:16:51,760
And actually, quantile normalization is not bad in this context.

255
00:16:51,760 --> 00:16:55,400
The zeros kind of give you a bit of trouble, but it's not horrendous.

256
00:16:55,400 --> 00:16:57,680
And so this is what we're doing here.

257
00:16:57,680 --> 00:17:03,200
So we're actually asking, can the multiplier model actually apply to array datasets, even

258
00:17:03,200 --> 00:17:05,000
though it's trained exclusively in RNA-seq data?

259
00:17:05,000 --> 00:17:08,920
We would have done this in RNA-seq data, but it turned out there wasn't RNA-seq data for

260
00:17:08,920 --> 00:17:10,440
this that was available yet.

261
00:17:10,840 --> 00:17:13,040
So now we've gotten to the point where the datasets for this disease are large enough

262
00:17:13,040 --> 00:17:16,440
that they actually do exist in RNA-seq as well.

263
00:17:16,440 --> 00:17:18,080
So what's going on here?

264
00:17:18,080 --> 00:17:23,160
So we've got three different datasets, airway epithelial cells, renal glomeruli, and PPMCs.

265
00:17:23,160 --> 00:17:26,800
They're collected in three different studies, so there's no matched people.

266
00:17:26,800 --> 00:17:28,960
And also the conditions are brutally different.

267
00:17:28,960 --> 00:17:32,640
So what's going to happen is from the left side to the right side of each of these plots,

268
00:17:32,640 --> 00:17:36,360
we're going to go from the least, from the most severe form of the disease to sort of

269
00:17:36,360 --> 00:17:39,360
the least severe form of the disease for healthy controls.

270
00:17:39,360 --> 00:17:45,320
So this dataset for the airway epithelial cells has, these are the vasculitis data,

271
00:17:45,320 --> 00:17:49,280
but then it's got things like pancreatic rhinitis, healthy.

272
00:17:49,280 --> 00:17:55,640
And so we're basically saying, okay, what is associated with severity across these three

273
00:17:55,640 --> 00:17:58,160
different cohorts?

274
00:17:58,160 --> 00:18:03,040
And so one of the latent variables that comes up as a severity associated is this M0 macrophage

275
00:18:03,040 --> 00:18:04,040
signature.

276
00:18:04,360 --> 00:18:09,640
And you can see the same thing where in each group you look in, the least severe form of

277
00:18:09,640 --> 00:18:12,280
the disease is on the right, the more severe form of the disease is on the left.

278
00:18:12,280 --> 00:18:17,440
So you can see this latent variable severity associated due to the bizarre sort of path

279
00:18:17,440 --> 00:18:20,040
of academic publishing.

280
00:18:20,040 --> 00:18:24,280
So our guess was that M0 macrophages could be involved here.

281
00:18:24,280 --> 00:18:29,000
Well, before this actually was published, but after the preprint came out, we had some

282
00:18:29,000 --> 00:18:31,640
follow-up work.

283
00:18:31,640 --> 00:18:34,280
And I have to break all the chronology of science.

284
00:18:34,280 --> 00:18:38,400
Our follow-up work came out first demonstrating that it looked like there was a change in

285
00:18:38,400 --> 00:18:43,240
macrophage metabolism in the disease that could be sort of influencing severity in a

286
00:18:43,240 --> 00:18:44,880
systemic way.

287
00:18:44,880 --> 00:18:48,400
You can also use this type of analysis to say what's particular to a tissue, right?

288
00:18:48,400 --> 00:18:52,560
So you could say what latent variables are associated with severity in this tissue, but

289
00:18:52,560 --> 00:18:53,720
not other tissues.

290
00:18:53,720 --> 00:18:56,920
So it gives you the ability to start doing those analyses in a way that it's pretty darn

291
00:18:56,920 --> 00:19:03,760
difficult to do with just the modular framework alone.

292
00:19:03,760 --> 00:19:08,560
And then I have an almost five-year-old, she turns five in three weeks, and she's been

293
00:19:08,560 --> 00:19:10,680
watching Zootopia.

294
00:19:10,680 --> 00:19:15,040
And there's a line in a song in Zootopia where I was listening, I'm like, oh my gosh, this

295
00:19:15,040 --> 00:19:18,760
is science.

296
00:19:18,760 --> 00:19:21,680
So the line is, I'll keep on making those new mistakes.

297
00:19:21,680 --> 00:19:25,120
I'll keep on making them every day, those new mistakes.

298
00:19:25,120 --> 00:19:26,880
And so we're really big on this in the lab, right?

299
00:19:27,840 --> 00:19:33,160
But what I tell people is, it's not going to work, just make it not work differently

300
00:19:33,160 --> 00:19:34,160
each time.

301
00:19:34,160 --> 00:19:37,240
If it's not working the same way each time, that's not good, but if it's not working for

302
00:19:37,240 --> 00:19:40,080
different reasons, that's perfect.

303
00:19:40,080 --> 00:19:41,700
And so we do this in our own work.

304
00:19:41,700 --> 00:19:47,660
So this is the first part of the GitHub that's rate me that's associated with this paper.

305
00:19:47,660 --> 00:19:51,480
So if you want to know sort of, these are all notebooks, if you want to follow along

306
00:19:51,480 --> 00:19:54,360
with the work that we did for this multiplier paper.

307
00:19:54,360 --> 00:19:57,720
The first part of this is kind of our proof of concept exploration to just understand

308
00:19:57,720 --> 00:19:58,720
how the method worked.

309
00:19:58,720 --> 00:20:03,280
Then you get to the stuff that's in the paper, then you get to the stuff that's in the supplement,

310
00:20:03,280 --> 00:20:06,080
then you get to the stuff that's neither in the paper nor the supplement, because it turned

311
00:20:06,080 --> 00:20:08,080
out the paper was too long.

312
00:20:08,080 --> 00:20:12,480
And so this gives you a way to see like, okay, here's all the stuff we did.

313
00:20:12,480 --> 00:20:15,640
So there was one experiment that we did where we wanted to say, can you predict outcome

314
00:20:15,640 --> 00:20:18,220
in clinical trials from these latent variables?

315
00:20:18,220 --> 00:20:23,000
And so we got this Rituximab data set from the NIH that was testing this.

316
00:20:23,000 --> 00:20:27,800
It turned out that the data set structure was, let's say, suboptimal, in that some of

317
00:20:27,800 --> 00:20:31,320
it was paired end and some of it was not paired end sequencing.

318
00:20:31,320 --> 00:20:33,880
And this was confounded with the endpoint.

319
00:20:33,880 --> 00:20:37,800
So it turned out to be extremely difficult to analyze, and we couldn't really learn anything

320
00:20:37,800 --> 00:20:38,800
from it.

321
00:20:38,800 --> 00:20:43,160
But if you're interested in using that for your own work, probably not that data set,

322
00:20:43,160 --> 00:20:44,160
that idea.

323
00:20:44,160 --> 00:20:48,040
You know, we've got a notebook here that's like, okay, here was our attempt to build

324
00:20:48,040 --> 00:20:49,240
a model to predict response.

325
00:20:49,240 --> 00:20:50,240
So you can start from that.

326
00:20:50,480 --> 00:20:55,840
So if you're interested in this, we try to do this for each of our papers.

327
00:20:55,840 --> 00:20:56,840
So this is available.

328
00:20:56,840 --> 00:20:58,400
The GitHub is here.

329
00:20:58,400 --> 00:21:03,080
If you search for Taroni and multiplier, you'll probably find it.

330
00:21:03,080 --> 00:21:07,960
But I thought this was a nice example of kind of how we've taken a project from inception

331
00:21:07,960 --> 00:21:11,000
through execution through kind of deliverables.

332
00:21:11,000 --> 00:21:13,640
This method, we've seen some other uses now.

333
00:21:13,640 --> 00:21:17,600
So someone used the same thing to study neurofibromatosis.

334
00:21:17,600 --> 00:21:19,120
That came out relatively recently.

335
00:21:19,120 --> 00:21:23,120
I can't remember, there's a few other sort of rare disease analyses that people have

336
00:21:23,120 --> 00:21:24,120
started using this for.

337
00:21:24,120 --> 00:21:25,120
But we really like seeing that, right?

338
00:21:25,120 --> 00:21:31,120
Because it demonstrates uptake that is in a, I mean, rare disease transcriptomics is

339
00:21:31,120 --> 00:21:33,800
a relatively small community.

340
00:21:33,800 --> 00:21:37,880
So it's nice to see this stuff beginning to catch on.

341
00:21:37,880 --> 00:21:44,240
I would also say, you know, we started with about $70 million worth of data.

342
00:21:44,240 --> 00:21:48,280
If you are Lego Grace Hopper and you happen to have an internet connection, you can have

343
00:21:48,280 --> 00:21:51,600
about $4 billion worth of data at your fingertips.

344
00:21:51,600 --> 00:21:55,800
So if you're interested, there's a few more resources that have come online.

345
00:21:55,800 --> 00:22:00,040
I think Arches 4 now has like 650,000 samples.

346
00:22:00,040 --> 00:22:04,120
So that's about, you know, if you want to estimate $650 million of preprocessed data

347
00:22:04,120 --> 00:22:05,960
at your fingertips.

348
00:22:05,960 --> 00:22:08,920
In a previous position, we built something called Refined Bio that's about a million

349
00:22:08,920 --> 00:22:11,240
samples.

350
00:22:11,240 --> 00:22:14,120
So these types of resources are available, which is great, because then you don't have

351
00:22:14,120 --> 00:22:15,680
to go back and rebuild this.

352
00:22:15,680 --> 00:22:18,960
You don't have to do all the software engineering to reprocess the data in a uniform way.

353
00:22:18,960 --> 00:22:21,560
You just kind of start from the processed data.

354
00:22:21,560 --> 00:22:26,600
And I think this opens up a lot of avenues of exploration.

355
00:22:26,600 --> 00:22:31,880
I like to, you know, one of the things that I say about what our lab works on is machine

356
00:22:31,880 --> 00:22:33,680
learning, public data, and the transcriptome.

357
00:22:33,680 --> 00:22:37,040
Pick two of three, and we're probably interested.

358
00:22:37,040 --> 00:22:42,840
One of our Bush essentially wrote and designed the way that we fund science in this country.

359
00:22:42,840 --> 00:22:46,280
So this idea that most science is going to happen outside of government research labs,

360
00:22:46,280 --> 00:22:49,800
it's mostly going to happen at universities, it's mostly going to be grant funded.

361
00:22:49,800 --> 00:22:54,080
He wrote this letter to FDR that says, the pioneer spirit is still vigorous within this

362
00:22:54,080 --> 00:22:58,080
nation science offers a largely unexplored hinterland for the pioneer who has the tools

363
00:22:58,080 --> 00:22:59,080
for his task.

364
00:22:59,080 --> 00:23:05,840
Well, I would say, I think open data is like the opportunities here are remarkable, like

365
00:23:05,840 --> 00:23:11,520
the ability to, you can take these data sets off the shelf and learn how something works

366
00:23:11,520 --> 00:23:15,760
at a scale that's very difficult to do from the data generated in only one lab.

367
00:23:15,760 --> 00:23:17,360
And once you do that, you can then test it.

368
00:23:17,360 --> 00:23:20,920
And I think I really think using other people's data as sort of the starting point to generate

369
00:23:20,920 --> 00:23:24,080
hypotheses that you then go test.

370
00:23:24,080 --> 00:23:28,080
There's an enormous amount of unexplored opportunity here.

371
00:23:28,080 --> 00:23:31,640
We also think sometimes about other data types instead of just gene expression.

372
00:23:31,640 --> 00:23:35,840
So this is work from David Nicholson, who was a PhD student in the lab who just graduated

373
00:23:35,840 --> 00:23:38,120
last year, who was like, well, let's do that.

374
00:23:38,120 --> 00:23:41,960
I just want to understand what's on bioRxiv anyway.

375
00:23:41,960 --> 00:23:46,760
So at this point, I probably don't have to introduce it, bioRxiv is a preprint server.

376
00:23:46,760 --> 00:23:50,560
And so this gives us the ability to also study the peer review process in some ways.

377
00:23:50,560 --> 00:23:54,040
So we can see what gets posted to bioRxiv, and then we can look at the sort of what the

378
00:23:54,040 --> 00:23:56,160
final paper looks like.

379
00:23:56,160 --> 00:24:03,240
We started this project just around the time that bioRxiv released an XML repository of

380
00:24:03,240 --> 00:24:04,920
their complete collection of data.

381
00:24:04,920 --> 00:24:08,120
So if you're interested in not just having a complete collection of transcriptomic data,

382
00:24:08,120 --> 00:24:11,640
you can also go get a complete collection of XML preprints, which I think is really

383
00:24:11,640 --> 00:24:15,240
exciting and a lot of fun.

384
00:24:15,240 --> 00:24:18,800
You learn some things if you start looking at just the metadata associated with this.

385
00:24:18,800 --> 00:24:25,160
So this is one of the simple questions that David asked was just, well, if there are preprints

386
00:24:25,160 --> 00:24:29,160
with multiple versions, are people sort of adjusting their preprint in response to peer

387
00:24:29,160 --> 00:24:30,280
review?

388
00:24:30,280 --> 00:24:34,040
So if someone submits their paper, they get comments back, do they generally repost it?

389
00:24:34,040 --> 00:24:36,920
We can't directly answer that question, we don't have access to the journal system.

390
00:24:36,920 --> 00:24:41,520
But we could say as well, if that were happening, probably what would happen is that at each,

391
00:24:41,520 --> 00:24:45,120
you know, as you saw in each version, you'd see an extension in the time to publish.

392
00:24:45,120 --> 00:24:46,120
Sure enough, you see that.

393
00:24:46,120 --> 00:24:53,080
And actually, the coefficient on the X here is about 50, which is in days.

394
00:24:53,080 --> 00:24:57,000
So it suggests that adding a version means sort of 50 days longer in the publication

395
00:24:57,000 --> 00:25:00,560
process, which is kind of aligned with what you'd expect to see if people are putting

396
00:25:00,560 --> 00:25:06,520
up papers and revising them in response to peer review.

397
00:25:06,520 --> 00:25:13,480
Another thing we can ask, so this is getting into the text itself, is if the text changes,

398
00:25:13,480 --> 00:25:18,320
do more changes in text between the preprint and the published version result in, does

399
00:25:18,320 --> 00:25:20,640
that come with a longer time to publish?

400
00:25:20,640 --> 00:25:23,400
And the answer to that is kind of yes-ish.

401
00:25:23,400 --> 00:25:28,560
If a preprint changes more from the, if a published version changes more from the preprint,

402
00:25:28,560 --> 00:25:31,000
it does take a bit longer to publish.

403
00:25:31,000 --> 00:25:33,840
But it's not an incredibly substantial change.

404
00:25:33,840 --> 00:25:36,560
And actually, the other thing that we did, so as we were doing this project, there was

405
00:25:36,560 --> 00:25:42,420
another group that did a completely different project, where they took a set of COVID papers

406
00:25:42,420 --> 00:25:45,480
that were published first as preprints and followed them through.

407
00:25:45,480 --> 00:25:46,800
Their scientific question was different.

408
00:25:46,800 --> 00:25:52,080
They wanted to say, for COVID-related papers, does the message of the paper change as it

409
00:25:52,080 --> 00:25:53,080
goes through the publication process?

410
00:25:53,080 --> 00:25:56,480
And they found that only in one case did that happen out of the 300-odd papers that they

411
00:25:56,480 --> 00:25:57,480
examined.

412
00:25:57,480 --> 00:26:00,100
But what that gave us was an annotated list of COVID papers.

413
00:26:00,100 --> 00:26:03,280
So we could then take that and ask if it had the same relationship, and it actually didn't

414
00:26:03,280 --> 00:26:04,800
have the same relationship.

415
00:26:04,800 --> 00:26:10,460
So for the subset of the literature in early 2020, COVID papers were being published quickly

416
00:26:10,460 --> 00:26:14,400
regardless of how much text changed between the preprint and published version.

417
00:26:14,400 --> 00:26:18,280
So this was kind of an interesting way to explore how publishing was happening.

418
00:26:18,280 --> 00:26:24,080
So for those of you who have had the opportunity to have papers go through peer review, can

419
00:26:24,080 --> 00:26:28,800
you guess what the most common linguistic change is, if we just look at word-level linguistic

420
00:26:28,800 --> 00:26:31,200
change during the publishing process?

421
00:26:39,200 --> 00:26:42,200
Has anyone ever had to add supplementary or additional data?

422
00:26:43,000 --> 00:26:44,480
It's not the most common.

423
00:26:44,480 --> 00:26:48,280
The most common is actually, oh, no, it is the most common, yeah.

424
00:26:48,280 --> 00:26:49,280
So additional and file.

425
00:26:49,280 --> 00:26:53,320
So on the right here is what's enriched in the published literature, and the left is

426
00:26:53,320 --> 00:26:55,280
what's enriched in preprints.

427
00:26:55,280 --> 00:26:58,800
So file and additional and supplementary are all pretty high at the top.

428
00:26:58,800 --> 00:27:04,280
So when people are changing their papers, we can infer that probably they're often changing

429
00:27:04,280 --> 00:27:08,040
the, you know, they're adding stuff to the supplement, but maybe they're not adding that

430
00:27:08,040 --> 00:27:09,640
much to the main paper.

431
00:27:09,640 --> 00:27:12,560
The other stuff that's in there is kind of interesting, like fig and figure.

432
00:27:12,560 --> 00:27:16,640
So because journals have different styles, the plus-minus symbol and the em dash.

433
00:27:16,640 --> 00:27:22,960
So you can see the artifacts of typesetting, but this gives a way to kind of understand

434
00:27:22,960 --> 00:27:23,960
what's on each side.

435
00:27:23,960 --> 00:27:28,360
And this, I should say, we've done this, so this analysis is using only preprint published

436
00:27:28,360 --> 00:27:29,360
pairs.

437
00:27:29,360 --> 00:27:32,760
If you do the same thing with all of BioRxiv and all of PubMed, you essentially find field

438
00:27:32,760 --> 00:27:33,760
differences.

439
00:27:33,760 --> 00:27:35,400
So some fields use BioRxiv more than others.

440
00:27:35,400 --> 00:27:38,840
So this is the more carefully controlled than that.

441
00:27:39,040 --> 00:27:42,960
The other thing that we've done, just if you happen to have a preprint yourself, we have

442
00:27:42,960 --> 00:27:51,720
this web server that does a linguistic comparison between a selected preprint and all of PubMed

443
00:27:51,720 --> 00:27:57,320
and says, okay, well, here's journals that publish linguistically similar papers.

444
00:27:57,320 --> 00:28:01,640
Here's papers that are linguistically similar to yours, and this has a secret feature.

445
00:28:01,640 --> 00:28:04,400
So what we encourage people to do, and we designed it to try to get people to upload

446
00:28:04,400 --> 00:28:06,920
their preprint, but then people are like, well, I have a preprint, but it's on archive

447
00:28:06,920 --> 00:28:08,360
and you don't support archive.

448
00:28:08,880 --> 00:28:11,720
So the secret feature, which you can also drag a PDF over the search box if you want

449
00:28:11,720 --> 00:28:18,880
to, but we don't generally advertise that because the goal was to get people to post

450
00:28:18,880 --> 00:28:21,560
preprints so they could use the service, but we don't support archive.

451
00:28:21,560 --> 00:28:25,760
So if you have an archive preprint, we'll allow you to put, to drag it over the search

452
00:28:25,760 --> 00:28:30,520
box, but no other PDFs.

453
00:28:30,520 --> 00:28:34,800
So this came out last year, and there's, again, a GitHub associated with it if you want to

454
00:28:34,800 --> 00:28:38,480
see all the kind of exploration that we did on the way.

455
00:28:38,480 --> 00:28:44,480
David had a follow-up paper that I'm really excited about where he looked at, he looked

456
00:28:44,480 --> 00:28:51,200
at the, what words change their meaning over time, and in the last 20 years of scientific

457
00:28:51,200 --> 00:28:52,200
publishing.

458
00:28:52,200 --> 00:28:55,520
So there's an application associated with that as well that I should have put the link

459
00:28:55,520 --> 00:28:57,800
on here, but didn't, that we call WordLabs.

460
00:28:57,800 --> 00:29:02,640
So if you go to our lab and look for WordLabs, you'll find that, and it's really interesting.

461
00:29:02,640 --> 00:29:07,520
So we see things like hallmarks of new technologies, like, you know, CRISPR has a linguistic shift.

462
00:29:07,520 --> 00:29:13,080
We also see a lot of pandemic-associated words have linguistic shifts.

463
00:29:13,080 --> 00:29:16,760
So if you're interested in understanding how our language changes, that's also something

464
00:29:16,760 --> 00:29:19,040
that David did.

465
00:29:19,040 --> 00:29:28,200
Okay, and then I know this is a less medically-related audience than most of the places that I speak,

466
00:29:28,360 --> 00:29:34,000
but one of the things that I thought I wanted to share was sort of how some of this basic

467
00:29:34,000 --> 00:29:37,120
science or sort of the techniques that we develop in this basic science can contribute

468
00:29:37,120 --> 00:29:40,040
to changes in how healthcare gets delivered.

469
00:29:40,040 --> 00:29:43,480
And so this is also something that we think about, right?

470
00:29:43,480 --> 00:29:44,960
Remember our business is serendipity.

471
00:29:44,960 --> 00:29:47,080
Yes, sometimes that's in research, right?

472
00:29:47,080 --> 00:29:50,560
Whether that's sort of me telling you how papers change, so that you can think about

473
00:29:50,560 --> 00:29:55,160
how you would change your paper in response to peer review, just add more additional files.

474
00:29:55,160 --> 00:29:59,200
But sometimes that's, you know, in care, in clinical care, right?

475
00:29:59,200 --> 00:30:02,720
So that someone, you know, you can imagine a patient comes in, there might be reasons

476
00:30:02,720 --> 00:30:04,720
that that patient might need to receive a different treatment.

477
00:30:04,720 --> 00:30:07,920
Could we provide that kind of information at the point of care?

478
00:30:07,920 --> 00:30:15,000
So this is a big focus at our med school and our health system that's associated with our

479
00:30:15,000 --> 00:30:21,440
med school, University of Colorado Health, has an entire program in clinical intelligence.

480
00:30:21,440 --> 00:30:27,000
This is sort of the idea that I like to highlight as sort of serendipity is like the right moment

481
00:30:27,000 --> 00:30:30,440
at the right time to make the right decision.

482
00:30:30,440 --> 00:30:35,600
In most health systems, if someone is going to get something called pharmacogenomic genetic

483
00:30:35,600 --> 00:30:36,600
testing.

484
00:30:36,600 --> 00:30:39,580
So the idea here is people have different variants in their genome.

485
00:30:39,580 --> 00:30:42,640
Some of those variants can affect how you metabolize drugs, how you respond to different

486
00:30:42,640 --> 00:30:44,280
drugs.

487
00:30:44,280 --> 00:30:49,520
It's not terribly common that people get tested for pharmacogenomic variants, because if you

488
00:30:49,600 --> 00:30:54,800
go get, if you are going to a hospital and, you know, you need to have a stent inserted,

489
00:30:54,800 --> 00:30:59,080
one of the common treatments is Plavix, well, there's a, there's an interaction between

490
00:30:59,080 --> 00:31:02,760
Plavix and a certain genetic variant, a set of genetic variants where you metabolize the

491
00:31:02,760 --> 00:31:05,680
drug differently, and it doesn't work for you, which means you're not getting the benefit

492
00:31:05,680 --> 00:31:09,800
of reducing your heart attack risk, or your clot risk.

493
00:31:09,800 --> 00:31:14,560
And so, but most people don't get this testing, because if a physician orders the testing,

494
00:31:14,560 --> 00:31:18,840
they get a 70 page PDF back, then they have to take that 70 page PDF and go to a table

495
00:31:18,840 --> 00:31:22,560
like this, read everything related to the drug they're about to prescribe, to prescribe

496
00:31:22,560 --> 00:31:25,360
on the table, and then understand if it applies, right?

497
00:31:25,360 --> 00:31:28,160
That is not a common thing for a physician to do.

498
00:31:28,160 --> 00:31:31,480
Providers don't get reimbursed for that type of type of work.

499
00:31:31,480 --> 00:31:35,320
What's happening at the University of Colorado and UC Health is we've got clinical decision

500
00:31:35,320 --> 00:31:38,280
support built into the electronic health record around this stuff.

501
00:31:38,280 --> 00:31:42,640
So this is the same thing, except instead of a 70 page PDF, plus having to look at this

502
00:31:42,640 --> 00:31:48,240
table, if a provider were to go in and order Plavix for an individual who's not going to

503
00:31:48,640 --> 00:31:52,160
benefit from it, it pops up an alert that says, look, we recommend you remove this because

504
00:31:52,160 --> 00:31:56,000
it's not going to work, and read about why it's not going to work if you want to, but

505
00:31:56,000 --> 00:31:59,200
we recommend you apply one of these alternatives that will work for this patient.

506
00:31:59,200 --> 00:32:04,520
And so this is serendipity, but not just in research, in clinical care.

507
00:32:04,520 --> 00:32:08,400
And this is, if you're interested in this kind of story, this is another one.

508
00:32:08,400 --> 00:32:12,920
This was an individual, different condition, where there was a question about drug efficacy.

509
00:32:12,920 --> 00:32:16,120
This is a story from UC Health.

510
00:32:16,120 --> 00:32:20,480
And in this case, the provider keyed in an order, and an alert popped up that says, oh,

511
00:32:20,480 --> 00:32:23,280
this person's going to need a different dose.

512
00:32:23,280 --> 00:32:26,640
And that was helpful to the provider to make that decision.

513
00:32:26,640 --> 00:32:31,400
One of the things I've had the privilege of doing over the last couple of years is focusing

514
00:32:31,400 --> 00:32:32,400
on this program.

515
00:32:32,400 --> 00:32:37,000
So a lot of faculty in our department work in this program, and about a year and a half

516
00:32:37,000 --> 00:32:42,480
ago, I guess two years ago, the previous director left, and so I ended up as the interim director.

517
00:32:42,480 --> 00:32:44,920
So I've gotten to know this program pretty well.

518
00:32:44,920 --> 00:32:48,080
So this is our Colorado Center for Personalized Medicine.

519
00:32:48,080 --> 00:32:50,800
We have a biobank study that this is all tied to.

520
00:32:50,800 --> 00:32:55,080
So if someone comes in, they can consent to have their sample collected for the biobank.

521
00:32:55,080 --> 00:32:57,920
We have a robust return of results pipeline built on that.

522
00:32:57,920 --> 00:33:00,600
So our biobank is growing pretty rapidly.

523
00:33:00,600 --> 00:33:02,720
Our sample increase is picking up a lot.

524
00:33:02,720 --> 00:33:05,800
But then the other thing we ask is, how is this making a difference in care?

525
00:33:05,800 --> 00:33:08,560
So essentially, how many of these alerts are actually firing?

526
00:33:08,560 --> 00:33:14,120
So over the last year, we've had about 1,000 patients who've had an alert fire at some

527
00:33:14,320 --> 00:33:16,360
point in clinical care.

528
00:33:16,360 --> 00:33:20,200
That's a tenfold increase over our entire previous history.

529
00:33:20,200 --> 00:33:26,280
And that's been powered because we've recently focused on getting these results back into

530
00:33:26,280 --> 00:33:27,560
the EHR in a structured way.

531
00:33:27,560 --> 00:33:31,800
So we've seen almost 100-fold growth, actually more than 100-fold growth year over year.

532
00:33:31,800 --> 00:33:35,800
We'll probably have 210,000 results in the electronic health record at sort of the two-year

533
00:33:35,800 --> 00:33:37,080
mark.

534
00:33:37,080 --> 00:33:42,080
And what this means is that if you're interested in studying this type of process in terms

535
00:33:42,080 --> 00:33:45,600
of care delivery, if you're interested in studying how physicians respond, if you're

536
00:33:45,600 --> 00:33:50,560
interested in looking for new cases where there are these sort of drug-gene interactions,

537
00:33:50,560 --> 00:33:54,240
we have the ingredients to do that at Colorado in a way that no one else that I'm aware of

538
00:33:54,240 --> 00:33:55,720
does.

539
00:33:55,720 --> 00:33:58,200
And so this program continues to grow.

540
00:33:58,200 --> 00:33:59,240
I'll just give you one.

541
00:33:59,240 --> 00:34:03,320
This is actually a real story that happened over the last few months.

542
00:34:03,320 --> 00:34:04,320
So this is a stock photo.

543
00:34:04,320 --> 00:34:09,280
I cannot show you a picture of the patient, but a patient came in to a community oncology

544
00:34:09,280 --> 00:34:10,280
clinic.

545
00:34:10,480 --> 00:34:12,160
And this works across the entire UC health system.

546
00:34:12,160 --> 00:34:13,680
So it's not just an academic hospital.

547
00:34:13,680 --> 00:34:17,360
This is a major health system that serves the Mountain West.

548
00:34:17,360 --> 00:34:20,280
So this patient came into a community oncology clinic.

549
00:34:20,280 --> 00:34:27,480
They were prescribed a drug that based on their genetic variants would have created

550
00:34:27,480 --> 00:34:33,520
a significant risk of life-threatening complications.

551
00:34:33,520 --> 00:34:41,400
Our team noticed this, sent a message to the provider, and then the patient alert actually

552
00:34:41,400 --> 00:34:45,000
fired and recommended a reduced dosage of the drug.

553
00:34:45,000 --> 00:34:48,000
The oncologist actually did proactively reduce the dose.

554
00:34:48,000 --> 00:34:53,680
So the person started at a different dose than would traditionally be used.

555
00:34:53,680 --> 00:34:55,600
Even at that dose, they didn't tolerate it very well.

556
00:34:55,600 --> 00:34:57,840
So they had to further reduce the dose.

557
00:34:57,840 --> 00:35:01,200
In these types of cases, you can imagine what happens if you start at the highest sort of

558
00:35:01,200 --> 00:35:05,920
the traditional dose at which these drugs can be for individuals with this particular

559
00:35:05,920 --> 00:35:07,720
variant can be lethal.

560
00:35:07,720 --> 00:35:11,680
And so this is a case where, you know, yes, I told you there's 1,000 alerts, but each

561
00:35:11,680 --> 00:35:14,360
of those 1,000 alerts is some story like this, right?

562
00:35:14,360 --> 00:35:19,640
And so it's nice to see this actually being used to deliver care at scale.

563
00:35:19,640 --> 00:35:23,040
And so we're doing this, this is all informatics, right?

564
00:35:23,040 --> 00:35:27,160
You can get all of this serendipity with sort of none of this here has machine learning

565
00:35:27,160 --> 00:35:30,480
built into it, but it's going to.

566
00:35:30,480 --> 00:35:33,920
And as we think about that, I think it's really important not just to sort of think

567
00:35:33,920 --> 00:35:37,360
from the machine learning point of view, but to really think about practical clinical care

568
00:35:37,360 --> 00:35:38,360
pathways.

569
00:35:38,360 --> 00:35:44,160
So this is a piece from Siddhartha Mukherjee that sort of, if you're interested in AI and

570
00:35:44,160 --> 00:35:49,040
medicine, I realize this is dated now, but it's still worth reading.

571
00:35:49,040 --> 00:35:54,320
And it's also weird that five years is old, but it's still worth reading.

572
00:35:54,320 --> 00:35:57,800
It has a quote from Geoffrey Hinton, sort of says they should stop training radiologists

573
00:35:57,800 --> 00:36:00,440
right now.

574
00:36:00,440 --> 00:36:03,280
And why would someone say this, right?

575
00:36:03,280 --> 00:36:05,480
Well, so Geoff Hinton's looking at the literature, right?

576
00:36:05,480 --> 00:36:08,760
So they're just trying to collect some literature from around the same time.

577
00:36:08,760 --> 00:36:11,680
So this is sort of saying, look, deep learning is going to completely transform healthcare.

578
00:36:11,680 --> 00:36:14,840
It's going to change how we care as we know it.

579
00:36:14,840 --> 00:36:20,000
Another sort of similar example, more examples, everything you read in the literature, deep

580
00:36:20,000 --> 00:36:21,000
learning.

581
00:36:21,000 --> 00:36:23,760
Like, I mean, now we're all into large language models, but at the time these image models

582
00:36:23,760 --> 00:36:25,600
were going to completely transform healthcare.

583
00:36:25,600 --> 00:36:28,760
Well, you might ask, is there anything they're not good at?

584
00:36:29,080 --> 00:36:31,440
Like they're good at everything, right?

585
00:36:31,440 --> 00:36:37,000
Chihuahuas and blueberry muffins, not terribly good here.

586
00:36:37,000 --> 00:36:42,520
This one's kind of wild.

587
00:36:42,520 --> 00:36:44,840
So I perceive the thing on the left as a panda.

588
00:36:44,840 --> 00:36:48,160
It looks like a picture of a panda.

589
00:36:48,160 --> 00:36:51,640
I don't really perceive this as much of anything.

590
00:36:51,640 --> 00:36:56,520
We're going to add, you know, this plus seven thousandths of this.

591
00:36:56,520 --> 00:37:02,120
What do you think the output is going to look like?

592
00:37:02,120 --> 00:37:03,120
A sloth.

593
00:37:03,120 --> 00:37:04,120
A sloth?

594
00:37:04,120 --> 00:37:05,120
Any Gibbons?

595
00:37:05,120 --> 00:37:06,120
Anyone for Gibbon?

596
00:37:06,120 --> 00:37:07,120
A bear.

597
00:37:07,120 --> 00:37:08,120
A bear?

598
00:37:08,120 --> 00:37:09,120
Okay.

599
00:37:09,120 --> 00:37:10,120
So this is what it looks like.

600
00:37:10,120 --> 00:37:11,120
It's a Gibbon.

601
00:37:11,120 --> 00:37:12,120
Clearly a Gibbon.

602
00:37:12,120 --> 00:37:13,120
No question.

603
00:37:13,120 --> 00:37:14,120
That's a Gibbon.

604
00:37:14,120 --> 00:37:15,120
A monkey.

605
00:37:15,120 --> 00:37:16,120
Okay.

606
00:37:16,120 --> 00:37:26,080
So it doesn't look like a Gibbon to me, but our neural network is exceedingly convinced.

607
00:37:26,080 --> 00:37:29,200
And the reason this works, right, is because the decision boundaries in these neural networks

608
00:37:29,200 --> 00:37:32,360
are sort of nonlinear, and you can end up pretty close to a decision boundary without

609
00:37:32,360 --> 00:37:34,200
really knowing it.

610
00:37:34,200 --> 00:37:38,160
This is another example, which I think is just a lot of fun, so I have to throw it in.

611
00:37:38,160 --> 00:37:43,040
So this is someone who was like, well, can I just make adversarial, like, sticker?

612
00:37:43,040 --> 00:37:46,960
Like, can I have a sticker that I can put on something and have a deep neural network

613
00:37:46,960 --> 00:37:49,360
perceive it as something, even if it's not?

614
00:37:49,360 --> 00:37:52,600
So this is the toaster sticker.

615
00:37:52,600 --> 00:37:56,960
And so you can give, you can put the toaster sticker on a table next to what I perceive

616
00:37:56,960 --> 00:38:01,480
to be a, I perceive this to be a toaster sticker on a table next to a banana.

617
00:38:01,480 --> 00:38:05,320
I perceive this to be a banana on a table.

618
00:38:05,320 --> 00:38:07,240
Neural network classifier, banana on a table.

619
00:38:07,240 --> 00:38:08,240
It's good.

620
00:38:08,240 --> 00:38:09,960
Stick the toaster sticker next to it?

621
00:38:09,960 --> 00:38:10,960
Absolutely a toaster.

622
00:38:10,960 --> 00:38:15,520
You can imagine doing the same things with stop signs, you can imagine doing the same

623
00:38:15,520 --> 00:38:20,960
things with other fun technologies in the world of self-driving cars or deep neural

624
00:38:21,120 --> 00:38:22,120
networks or vision.

625
00:38:22,120 --> 00:38:26,200
Okay, so let's go back to the automated radiologist finding pneumonia.

626
00:38:26,200 --> 00:38:29,200
This was one of the examples I showed you before.

627
00:38:29,200 --> 00:38:34,720
There's a, John Zek is a guy who writes blog posts that are really good and then converts

628
00:38:34,720 --> 00:38:36,600
them into papers.

629
00:38:36,600 --> 00:38:41,480
So this was a blog post from John Zek, which then became a paper that sort of said, okay,

630
00:38:41,480 --> 00:38:43,120
why is the system actually working?

631
00:38:43,120 --> 00:38:45,200
Like what's happening here?

632
00:38:45,200 --> 00:38:51,080
So he went to the, to the images and tried to understand, you know, what part of the

633
00:38:51,080 --> 00:38:53,920
image is contributing to the prediction of pneumonia?

634
00:38:53,920 --> 00:38:58,480
Well in this case, a positive, and this should probably find pneumonia.

635
00:38:58,480 --> 00:39:01,760
There's high density in the lung here.

636
00:39:01,760 --> 00:39:02,760
You can say, well, okay.

637
00:39:02,760 --> 00:39:09,240
Oh, and I should say a positive number is suggestive of pneumonia, a negative number

638
00:39:09,240 --> 00:39:10,240
is not.

639
00:39:10,240 --> 00:39:11,240
Okay.

640
00:39:11,240 --> 00:39:15,360
So what are the positive numbers?

641
00:39:15,360 --> 00:39:22,720
The most positive numbers, bottom, yeah, where there's this interesting stripe, the interesting

642
00:39:22,720 --> 00:39:26,800
stripe that's kind of unique characteristic of the scanner, the one that you might see

643
00:39:26,800 --> 00:39:31,440
if you were in a department, if the scanner was placed proximal to sort of where pneumonia

644
00:39:31,440 --> 00:39:33,840
diagnoses were usually occurring.

645
00:39:33,840 --> 00:39:34,840
Yeah.

646
00:39:34,840 --> 00:39:38,240
So, so, so that's interesting.

647
00:39:38,240 --> 00:39:40,520
Here's another one.

648
00:39:40,800 --> 00:39:44,320
I should say these probabilities are low, but the previous one was in the 99th percentile

649
00:39:44,320 --> 00:39:45,320
for pneumonia.

650
00:39:45,320 --> 00:39:47,320
This one's in the 95th percentile.

651
00:39:47,320 --> 00:39:48,320
Yeah.

652
00:39:48,320 --> 00:39:49,320
Portable?

653
00:39:49,320 --> 00:39:50,320
Portable.

654
00:39:50,320 --> 00:39:52,600
Someone's not well enough to go to the scanner.

655
00:39:52,600 --> 00:39:55,860
Not a good sign for their health, could indicate pneumonia.

656
00:39:55,860 --> 00:39:59,320
Probably not the part of the image you want to be looking at.

657
00:39:59,320 --> 00:40:05,200
And so, you know, I think this to me, I was also reading this every once in a while, I

658
00:40:05,200 --> 00:40:08,960
read the comment section of blog posts on the internet, which I do not recommend, but

659
00:40:08,960 --> 00:40:09,960
there was one on big data.

660
00:40:09,960 --> 00:40:14,320
You know, there was this blog post on big data and I'm like, well, it was like why statistics

661
00:40:14,320 --> 00:40:16,800
don't matter in the era of big data or something like that.

662
00:40:16,800 --> 00:40:21,040
And I'm like, okay, I'll, you know, and then I'm like, I have some disagreements with this.

663
00:40:21,040 --> 00:40:25,720
Then I go to the comment section, I find this one, which I really agree with.

664
00:40:25,720 --> 00:40:29,680
On big data, data collection biases are always larger than statistical uncertainty.

665
00:40:29,680 --> 00:40:33,560
And I think this is why I sort of, you know, you can have these models that perform robustly

666
00:40:33,560 --> 00:40:36,080
around a lot of these comparisons that still struggle.

667
00:40:36,200 --> 00:40:39,240
And then I read, who made the post, and it's this guy named Daniel Himmelstein, which probably

668
00:40:39,240 --> 00:40:42,160
doesn't mean anything to you, but he happened to be a postdoc in my lab.

669
00:40:42,160 --> 00:40:46,440
And I'm like, dude, put this in the comment section of the internet, people should actually

670
00:40:46,440 --> 00:40:47,440
read it.

671
00:40:47,440 --> 00:40:51,120
So now I put it on slides, so at least someone can see it.

672
00:40:51,120 --> 00:40:54,600
Okay, so how do we design systems that work?

673
00:40:54,600 --> 00:40:59,080
So you know, for AI and medicine, regardless of what we're using, I think we need to have

674
00:40:59,080 --> 00:41:01,520
some principles that we think about.

675
00:41:01,520 --> 00:41:05,240
We just did something that sort of tries to go on this path.

676
00:41:05,400 --> 00:41:06,960
So this is a little bit different.

677
00:41:06,960 --> 00:41:11,800
So this is a piece from Nature, but it talks about a preprint that we put up earlier this

678
00:41:11,800 --> 00:41:17,600
year, where we were trying to use GPT-based models, so in this case, GPT-3, to revise

679
00:41:17,600 --> 00:41:19,280
academic manuscripts.

680
00:41:19,280 --> 00:41:23,440
One of the things that we see all the time is, well, now it's really common, right?

681
00:41:23,440 --> 00:41:30,240
People are developing services that can use GPT-3 or ChatGPT, or GPT-4 through ChatGPT

682
00:41:30,240 --> 00:41:31,240
to revise your manuscript.

683
00:41:31,240 --> 00:41:33,400
Well, what are the issues with those?

684
00:41:33,560 --> 00:41:36,360
So our experience putting this together and running a bunch of test manuscripts through

685
00:41:36,360 --> 00:41:40,280
it is that, yes, it is good at clarifying language.

686
00:41:40,280 --> 00:41:42,800
There's a lot of things it can help you with.

687
00:41:42,800 --> 00:41:47,480
It actually caught an error in an equation, which I was pretty darn impressed with.

688
00:41:47,480 --> 00:41:50,640
On the other hand, it also makes stuff up.

689
00:41:50,640 --> 00:41:54,480
And so a bit of a challenge if your idea is that you're just going to use this.

690
00:41:54,480 --> 00:41:59,240
And I think, you know, I use this as an example because it's really trivial.

691
00:41:59,240 --> 00:42:03,360
You can try it out yourself and see how it works, and you can get these examples yourself.

692
00:42:03,360 --> 00:42:06,840
The same things are going to be true when we use this in medical context, so we need

693
00:42:06,840 --> 00:42:07,840
to think about them.

694
00:42:07,840 --> 00:42:11,440
I think thinking about them and trying them and experimenting in a low-risk environment

695
00:42:11,440 --> 00:42:14,440
before we move to a high-risk environment is usually a good idea.

696
00:42:14,440 --> 00:42:17,440
So what are the principles that we've kind of come up with as we think about this?

697
00:42:17,440 --> 00:42:22,440
So first, we really do aim for kind of an augmentation, not replacement.

698
00:42:22,440 --> 00:42:27,760
And what that means is, you know, when we apply this to manuscripts and we try to get

699
00:42:27,760 --> 00:42:32,520
it to improve it, you know, we actually applied it to the manuscript about the tool.

700
00:42:32,520 --> 00:42:36,200
And when we did that, it made up this thing that we had done that we had fine-tuned the

701
00:42:36,200 --> 00:42:38,240
model on manuscripts of a similar type.

702
00:42:38,240 --> 00:42:41,920
Yeah, you should absolutely fine-tune the model on manuscripts of a similar type.

703
00:42:41,920 --> 00:42:43,200
Makes a ton of sense.

704
00:42:43,200 --> 00:42:44,200
We didn't do it.

705
00:42:44,200 --> 00:42:48,360
You probably shouldn't report that you did it in the manuscript.

706
00:42:48,360 --> 00:42:52,280
So we're really thinking like, you know, okay, this is not like you're going to plug it in,

707
00:42:52,280 --> 00:42:53,280
you're going to be done.

708
00:42:53,280 --> 00:42:56,760
It's really, you need to design it around kind of an augmentation capability.

709
00:42:56,760 --> 00:43:00,000
You've got to carefully consider your use cases.

710
00:43:00,000 --> 00:43:03,280
You know, if it's easy to take the output, it's hard to compare.

711
00:43:03,280 --> 00:43:04,800
It's probably not good, right?

712
00:43:04,800 --> 00:43:09,240
Because you're creating the opportunity for a mistake that you don't need to create.

713
00:43:09,240 --> 00:43:11,960
We really like to start with these kind of simple solutions and approaches and layer

714
00:43:11,960 --> 00:43:13,320
complexity only as needed.

715
00:43:13,320 --> 00:43:20,080
So in this case, you know, we start with some pretty simple prompts and then have the ability

716
00:43:20,080 --> 00:43:21,280
to add complexity.

717
00:43:21,280 --> 00:43:25,200
But usually we just kind of try to keep it relatively basic and simple.

718
00:43:25,200 --> 00:43:26,200
The workflow is simple.

719
00:43:26,640 --> 00:43:28,360
You can proof of concept it out really quickly.

720
00:43:28,360 --> 00:43:30,440
And the most important thing is preserving attribution.

721
00:43:30,440 --> 00:43:31,680
Like where did the content come from?

722
00:43:31,680 --> 00:43:35,760
If you're thinking about this in a clinical setting, you know, what was provided and when?

723
00:43:35,760 --> 00:43:38,000
And you know, did it come from an AI or a human first?

724
00:43:38,000 --> 00:43:41,520
Because that's really going to matter as you're thinking about evaluating these workflows.

725
00:43:41,520 --> 00:43:45,200
In academic writing, I think it's going to, you're going to want to keep track of whether

726
00:43:45,200 --> 00:43:48,960
something came from an AI based system or if you wrote it.

727
00:43:48,960 --> 00:43:51,600
I think this is, more and more journals are starting to require this.

728
00:43:51,600 --> 00:43:53,520
It's going to be important.

729
00:43:53,520 --> 00:43:59,160
But I just think like these are key principles that I would recommend keeping in mind.

730
00:43:59,160 --> 00:44:01,680
And then finally, I just want to give you an idea of what the environment is like about

731
00:44:01,680 --> 00:44:02,680
at Colorado.

732
00:44:02,680 --> 00:44:05,840
Because some of you may one day look for future jobs and I figure you should know something

733
00:44:05,840 --> 00:44:06,840
about us.

734
00:44:06,840 --> 00:44:07,840
We're not at Boulder.

735
00:44:07,840 --> 00:44:13,040
We're at the Anschutz Medical Campus, which is kind of between the airport and the Denver

736
00:44:13,040 --> 00:44:16,360
airport and Denver itself.

737
00:44:16,360 --> 00:44:19,240
We are a major academic medical center.

738
00:44:19,240 --> 00:44:23,500
And like I said, we're not at Boulder, which is the thing that people most frequently get

739
00:44:24,480 --> 00:44:25,780
confused about.

740
00:44:25,780 --> 00:44:30,980
On July 1st of last year, we actually launched a new department of biomedical informatics.

741
00:44:30,980 --> 00:44:34,020
And you know, we're trying to hire and put together a faculty that are focused on this

742
00:44:34,020 --> 00:44:37,940
idea of kind of making serendipity routine, like how do you surface the right information

743
00:44:37,940 --> 00:44:40,220
at the right time.

744
00:44:40,220 --> 00:44:45,260
Our 30, we're now at 31 faculty, we have a new person starting in May, that will get

745
00:44:45,260 --> 00:44:50,340
us to 32 faculty, have about $65 million in extramural research, just for faculty who

746
00:44:50,340 --> 00:44:55,340
are PIs, on which faculty in the department are PIs, there's a lot of additional collaborative

747
00:44:55,340 --> 00:44:57,620
funding that's not included in this.

748
00:44:57,620 --> 00:45:03,380
We have expertise kind of across the spectrum from precision medicine, through kind of physiological

749
00:45:03,380 --> 00:45:06,060
modeling, we have folks who think about human computer interaction, because if you want

750
00:45:06,060 --> 00:45:08,740
to deploy this stuff in the clinic, you should really think about how humans are going to

751
00:45:08,740 --> 00:45:13,580
interact with it through kind of electrical engineering, medical imaging and AI.

752
00:45:13,580 --> 00:45:16,940
So there's a lot of faculty working in this area.

753
00:45:16,940 --> 00:45:21,100
This I just collected some stuff from early last year.

754
00:45:21,100 --> 00:45:24,660
And that just sort of like, okay, when our faculty mentioned in the press, so there's

755
00:45:24,660 --> 00:45:32,540
stuff in MIT technology review nature, LeMond, Deutschland, I can't remember whatever the

756
00:45:32,540 --> 00:45:35,500
German public radio station is.

757
00:45:35,500 --> 00:45:38,700
So you know, you have a bunch of internationally renowned experts who are at Anschutz, we don't

758
00:45:38,700 --> 00:45:39,700
happen to be at Boulder.

759
00:45:39,700 --> 00:45:44,740
You know, to me, it's like the difference between Georgia Tech and Georgia, I feel like

760
00:45:44,740 --> 00:45:49,660
they're different institutions, we should probably occasionally recognize that.

761
00:45:49,660 --> 00:45:53,460
The other thing that sort of we focused on when we were creating the department, if you

762
00:45:53,460 --> 00:45:57,060
read the sociology literature from the 80s, which I suspect all of you do on a regular

763
00:45:57,060 --> 00:46:02,020
basis, there's this article that I actually think you should read from the sociology literature

764
00:46:02,020 --> 00:46:04,740
from the 80s called the mundanity of excellence.

765
00:46:04,740 --> 00:46:08,260
So this is someone who essentially studied swimmers at many different levels and asked

766
00:46:08,260 --> 00:46:12,580
what differentiates swimmers at one level from another level.

767
00:46:12,700 --> 00:46:16,260
There's a few different principles that come out, but one of the key ones is that excellence

768
00:46:16,260 --> 00:46:18,620
requires qualitative differentiation.

769
00:46:18,620 --> 00:46:22,180
And what I mean by that is, you know, you're not going to move up a level in swimming competitions

770
00:46:22,180 --> 00:46:24,020
by swimming to extra labs.

771
00:46:24,020 --> 00:46:25,020
That's not how it works.

772
00:46:25,020 --> 00:46:28,020
What you're going to do is you're going to focus on your form, you're going to, you know,

773
00:46:28,020 --> 00:46:30,920
you're going to approach the sport differently, you're going to focus on getting rest the

774
00:46:30,920 --> 00:46:34,340
night before the meet, like that's the stuff that people do at higher levels that they

775
00:46:34,340 --> 00:46:36,660
don't do at lower levels.

776
00:46:36,660 --> 00:46:39,380
And as we think about a department, we had to ask like, what's our, okay, what's our

777
00:46:39,380 --> 00:46:40,380
qualitative differentiator?

778
00:46:40,380 --> 00:46:44,220
How are we not just like another biomedical informatics department that just happens to

779
00:46:44,220 --> 00:46:45,580
have more money or something, right?

780
00:46:45,580 --> 00:46:48,460
Like that's not, that's not a real differentiator.

781
00:46:48,460 --> 00:46:53,580
And so what we thought about was creating promotion and tenure guidelines that have,

782
00:46:53,580 --> 00:46:55,720
that are focused on real world impact.

783
00:46:55,720 --> 00:47:00,700
So one of our bullet points in our departmental sort of idea of impact, there is a bullet

784
00:47:00,700 --> 00:47:02,220
point that includes publication.

785
00:47:02,220 --> 00:47:05,460
It's possible, you can do it, we can care about it.

786
00:47:05,540 --> 00:47:10,780
But there's also technology development that gets deployed locally, nationally or internationally.

787
00:47:10,780 --> 00:47:14,100
Software shows up, changes in policy because of your work.

788
00:47:14,100 --> 00:47:17,100
All of that is included in and counts for impact.

789
00:47:17,100 --> 00:47:21,340
Now, probably not all of you will look for tenure track faculty positions in our department.

790
00:47:21,340 --> 00:47:26,660
But if you were to come train or send folks to train with us, I think it's important to

791
00:47:26,660 --> 00:47:28,340
know like that filters down, right?

792
00:47:28,340 --> 00:47:30,940
If that's how faculty are evaluated, that filters all the way down.

793
00:47:30,940 --> 00:47:34,340
So there's an emphasis on real world impact that we have that I think can be our kind

794
00:47:34,340 --> 00:47:35,340
of qualitative differentiator.

795
00:47:35,340 --> 00:47:39,620
And I'll just say, we have a really good training environment.

796
00:47:39,620 --> 00:47:41,100
So we have strong connections with UC Health.

797
00:47:41,100 --> 00:47:44,660
So I told you earlier about one of the UC Health programs that we work closely with

798
00:47:44,660 --> 00:47:45,660
them on.

799
00:47:45,660 --> 00:47:51,220
And Children's Colorado, which is a nationally renowned pediatric cancer hospital, not pediatric

800
00:47:51,220 --> 00:47:52,220
hospital.

801
00:47:52,220 --> 00:47:55,940
I know the pediatric cancer people work in that space.

802
00:47:55,940 --> 00:47:56,940
So we have those tight connections.

803
00:47:56,940 --> 00:48:01,140
If you're interested in seeing your work translate to care, we have, if you're interested in

804
00:48:01,140 --> 00:48:03,620
using genetics to guide care, I think we have one of the best programs in the country

805
00:48:03,620 --> 00:48:05,340
through CCPM.

806
00:48:05,340 --> 00:48:08,740
We have a diverse and internationally recognized faculty.

807
00:48:08,740 --> 00:48:12,680
One of the things that's sometimes a little bit odd for folks at, you know, our tenure

808
00:48:12,680 --> 00:48:19,620
track faculty in DBMI are actually majority women, which I think is uncommon at, in, in

809
00:48:19,620 --> 00:48:21,380
our field.

810
00:48:21,380 --> 00:48:24,900
And then if you like the climate and you, it's a little bit humid here.

811
00:48:24,900 --> 00:48:28,340
We don't have that level of humidity, but we do have more hours of sun per year than

812
00:48:28,340 --> 00:48:29,340
Miami and San Diego.

813
00:48:29,340 --> 00:48:31,860
So if you're interested in the environment around you, we've got that.

814
00:48:31,860 --> 00:48:34,780
And then we've got abundant outdoor activities.

815
00:48:34,780 --> 00:48:36,580
This is one example of the programs that we have.

816
00:48:36,580 --> 00:48:39,020
So this is our computational science PhD program.

817
00:48:39,020 --> 00:48:44,740
There's also a postdoc training grant associated with the same thing.

818
00:48:44,740 --> 00:48:47,780
So if you're interested in this type of thing, feel free to look us up.

819
00:48:47,780 --> 00:48:51,580
You can always drop me an email and I can try to connect you with folks too.

820
00:48:51,580 --> 00:48:55,860
And then with that, I just want to thank the people who make this possible.

821
00:48:55,860 --> 00:49:00,140
So the members of the lab, we really have a kind of robust culture in the lab of sort

822
00:49:00,140 --> 00:49:04,700
of sharing the work that's happening and kind of thinking through each other's projects

823
00:49:04,700 --> 00:49:06,420
in ways that are really helpful.

824
00:49:06,420 --> 00:49:07,420
We also do code review.

825
00:49:07,420 --> 00:49:12,060
So people really pitch out, pitch in together, the department of biomedical informatics and

826
00:49:12,060 --> 00:49:15,660
my leadership team, and then the folks in CCPM, since I shared some of that work, and

827
00:49:15,660 --> 00:49:18,900
then the folks who give us money, I'd be happy to take whatever questions you have.

828
00:49:25,860 --> 00:49:35,340
For the radiology, XAI explainability, did they end up using my grad cam as like how

829
00:49:35,340 --> 00:49:41,380
they liberated the convolutional layer for the confluence?

830
00:49:41,380 --> 00:49:42,380
Yeah.

831
00:49:42,380 --> 00:49:45,580
I don't remember what strategy they used.

832
00:49:45,580 --> 00:49:51,220
And I also don't remember, you know, the saliency map.

833
00:49:51,220 --> 00:49:54,580
I think it's a saliency map, but I'm not 100% sure.

834
00:49:55,580 --> 00:50:00,300
Yeah, there's a, so John posted that first as a blog post, there's now like a PLOS medicine

835
00:50:00,300 --> 00:50:01,300
paper, I think.

836
00:50:01,300 --> 00:50:02,300
Yeah.

837
00:50:02,300 --> 00:50:03,300
I think I probably, yeah.

838
00:50:03,300 --> 00:50:04,300
Yeah.

839
00:50:04,300 --> 00:50:07,020
And I wrote this when it was the blog post and not when it was PLOS medicine paper, which

840
00:50:07,020 --> 00:50:09,020
is what I'd look at.

841
00:50:09,020 --> 00:50:10,020
Yeah.

842
00:50:10,020 --> 00:50:11,020
Yeah.

843
00:50:11,020 --> 00:50:19,540
So with the strange like explainability maps, did that happen even with like augmentations

844
00:50:19,540 --> 00:50:22,100
that would rotate or like zoom in, zoom out?

845
00:50:22,100 --> 00:50:26,300
Like was that with that, it still happened or with a train without that?

846
00:50:26,300 --> 00:50:32,540
Because like, I was thinking like assume in augmentation might solve that bottom band

847
00:50:32,540 --> 00:50:33,540
problem.

848
00:50:33,540 --> 00:50:34,540
Yeah.

849
00:50:34,540 --> 00:50:36,780
So I was wondering if that still.

850
00:50:36,780 --> 00:50:39,180
I'm guessing, so this is Andrew Ng's stuff.

851
00:50:39,180 --> 00:50:40,980
This was the one that he was criticizing.

852
00:50:40,980 --> 00:50:48,820
I think this was just chest x-rays and I don't think they, I don't remember them doing at

853
00:50:48,820 --> 00:50:52,020
least like a patch-based augmentation or anything like that.

854
00:50:52,020 --> 00:50:53,020
Yeah.

855
00:50:53,020 --> 00:50:57,100
And I would say now some of the techniques that are more sophisticated are likely to

856
00:50:57,100 --> 00:50:58,100
control for some of that.

857
00:50:58,100 --> 00:51:01,540
I mean, the other thing you could do is you can also just like do some adversarial training

858
00:51:01,540 --> 00:51:04,500
around the location of the scanner.

859
00:51:04,500 --> 00:51:07,940
The challenge with that is you need to know to do it and to know to do it, you have to

860
00:51:07,940 --> 00:51:10,220
like have someone who's an expert probe your data.

861
00:51:10,220 --> 00:51:16,300
And I think sometimes when we come to things from a computer science perspective, I do

862
00:51:16,300 --> 00:51:22,340
think sometimes we get really excited that something is working and especially if it's

863
00:51:22,340 --> 00:51:31,780
working as well as a human and maybe we get a little bit ahead of ourselves and aren't

864
00:51:31,780 --> 00:51:36,660
skeptical enough about our own results.

865
00:51:36,660 --> 00:51:41,020
So with the pharmacogenomics, the alert system that you pointed out.

866
00:51:41,020 --> 00:51:44,100
So how often do you get sort of false alerts, right?

867
00:51:44,100 --> 00:51:52,580
So because sometimes you can get an alert that a physician may not be really interested

868
00:51:52,580 --> 00:51:54,940
in or think is valid, right?

869
00:51:54,940 --> 00:51:55,940
Yeah.

870
00:51:55,940 --> 00:51:59,940
So we designed this pretty carefully.

871
00:51:59,940 --> 00:52:05,580
So we could bump up our alert number by just whether or not someone's getting a relevant

872
00:52:05,580 --> 00:52:08,380
prescription, fire the alert.

873
00:52:08,380 --> 00:52:09,620
That'd be great for our metrics.

874
00:52:09,620 --> 00:52:13,620
On the other hand, not terribly useful for care and people would learn to ignore it.

875
00:52:13,660 --> 00:52:15,060
So we're pretty focused.

876
00:52:15,060 --> 00:52:17,260
So most of the alerts are non-interruptive.

877
00:52:17,260 --> 00:52:19,500
So the idea is an 80-20 rule.

878
00:52:19,500 --> 00:52:24,140
So only 20% of the alerts should be interruptive, 80% should be non-interruptive.

879
00:52:24,140 --> 00:52:29,660
And because this isn't based on a predictive model, it's pretty straightforward to make

880
00:52:29,660 --> 00:52:32,220
sure it fires largely at times when it's relevant.

881
00:52:32,220 --> 00:52:36,180
So restricting kind of the clinic in which you can fire and that sort of stuff.

882
00:52:36,180 --> 00:52:39,780
However, one of the really nice things about UCHealth, so I'll go back on my advertising

883
00:52:39,780 --> 00:52:42,500
pitch for why you should come to Colorado and work at Colorado if this is something

884
00:52:42,500 --> 00:52:46,660
you're interested in, UCHealth thought about this in advance.

885
00:52:46,660 --> 00:52:49,620
So years ago, they built a virtual health center.

886
00:52:49,620 --> 00:52:53,500
And if you want to look it up, you can look up the UCHealth virtual health center.

887
00:52:53,500 --> 00:52:58,060
The guy who, to my understanding, put this together and sort of was visionary behind

888
00:52:58,060 --> 00:53:03,060
it is a guy named Rich Zane, who's our chief innovation officer through the hospital.

889
00:53:03,060 --> 00:53:07,780
And what they did there is they have nurses and clinicians who work offsite, but who are

890
00:53:07,780 --> 00:53:12,900
available to look at these types of systems before they flow to people who are onsite

891
00:53:12,900 --> 00:53:14,260
providing care.

892
00:53:14,260 --> 00:53:19,260
And where this became really useful, so is anyone aware of kind of the EPIC sepsis model

893
00:53:19,260 --> 00:53:22,820
thing that blew up maybe a year or two ago?

894
00:53:22,820 --> 00:53:23,820
No.

895
00:53:23,820 --> 00:53:26,860
So EPIC is one of the major providers of electronic health record systems.

896
00:53:26,860 --> 00:53:31,300
They have a sepsis model, and that sepsis model is pretty noisy.

897
00:53:31,300 --> 00:53:36,380
It likes to, it alerts probably more frequently than it should, and it misses cases it shouldn't

898
00:53:36,380 --> 00:53:37,420
miss.

899
00:53:37,420 --> 00:53:43,020
So there was a team at CU before my time, led by Tal Bennett, that had evaluated this

900
00:53:43,020 --> 00:53:50,180
model and found that it had some predictive quality, but maybe it wasn't ready.

901
00:53:50,180 --> 00:53:52,460
I want to be careful what I assert here.

902
00:53:52,460 --> 00:53:57,940
It had some predictive quality, but deployed in practice at scale could have created a

903
00:53:57,940 --> 00:54:00,620
lot of unnecessary burden on providers.

904
00:54:00,620 --> 00:54:03,620
Well, what they did, because they have the virtual health center, is they're able to

905
00:54:03,620 --> 00:54:11,060
deploy that model plus others in the virtual health center, have it alert nurses and clinicians

906
00:54:11,060 --> 00:54:16,260
there, and then have them look at it carefully in the virtual health center and only send

907
00:54:16,260 --> 00:54:20,100
the notice over to the folks who are working at the bedside if it's actually going to be

908
00:54:20,100 --> 00:54:21,100
useful.

909
00:54:21,100 --> 00:54:23,620
So a reason that could be good to work at Colorado, if you're interested in kind of

910
00:54:23,620 --> 00:54:27,460
predictive analytics and deploying this stuff in practice, is you can have a model that's

911
00:54:27,460 --> 00:54:28,620
not perfect, right?

912
00:54:28,620 --> 00:54:33,820
It doesn't have to be good enough to hand to a provider at the bedside.

913
00:54:33,820 --> 00:54:37,460
Because of the virtual health center, you can really proof of concept it out there,

914
00:54:37,460 --> 00:54:40,060
improve it, understand how you can improve its predictive quality, and then deploy it

915
00:54:40,060 --> 00:54:41,060
when it's ready.

916
00:54:41,060 --> 00:54:42,660
But you can still get the benefit in the meantime.

917
00:54:42,660 --> 00:54:49,220
So I guess I'd say, yeah, I think our noise level is pretty low on these alerts, but we

918
00:54:49,220 --> 00:54:57,900
do have a system in place for noisier stuff if people want to deploy it.

919
00:54:57,900 --> 00:54:58,900
It's fun to be back in Athens.

920
00:54:58,900 --> 00:54:59,900
Time to go dogs.

921
00:54:59,900 --> 00:55:04,220
I don't know what else I should say, but I'm just excited to be back.

922
00:55:04,220 --> 00:55:26,100
I was really tickled when I got the invite, it was wonderful.

