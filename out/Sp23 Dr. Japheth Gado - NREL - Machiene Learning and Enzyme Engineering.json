{"text": " It's my pleasure to introduce Dr. Jafet Gado. He is currently a postdoctoral researcher at the National Renewable Energy Laboratory working as part of the Bio-Optimized Technologies for Keeping Thermoplastics Out of Landfills and the Environment, or BOTTLE for short, project, working on using machine learning to identify and engineer plastic degrading enzymes. And as part of that, he's currently a visiting researcher at Harvard Medical School working on part of the same project. Before this, he got his PhD at the University of Kentucky working on cellulose degrading enzymes and using machine learning to figure out those enzymes. And before that, he did his bachelor's degree in chemical engineering at Amadou Fellow University. So if everyone can welcome Dr. Gado, we'll get started. Hi, good to see everyone's faces. And yeah, I remember not too many days ago, I was a graduate student sitting and then listening to presentations like this. And I would say, oh, I'll never understand, you know, they look so strange. So while I was preparing this, I had that in mind. I was like, I hope I will I will present in a way that everyone can understand. Yeah. But thank you for having me. I am I'm excited to be here and just to talk about my work with you. So let's get into machine learning for discovery and engineering of plastic degrading enzymes. Everyone here, I believe, is familiar with the problem with plastics and the global crisis of plastic pollution. We've seen images like this where you have plastics and oceans and landfills and all that. And it's even beyond just plastics polluting our environment to plastics getting into places where we don't even think they would get into protected areas in the United States. National forests and wilderness are contaminated with plastics because microplastics are taken there by wind and rain. Deep in the ocean, we see plastics in coral reefs and this has been associated with disease so that there's a higher likelihood of disease where you have coral reefs with plastic contamination. And we have microplastics in the human body as well and the bloodstream and so on. And this leads to all kinds of disease. And we know that there is a need to redefine how we use plastics and how we work with them. Also, very importantly, the life cycle of a plastic item like this bottle is quite long. It's about 500 to 700 years to begin degrading it. So every time I order Chinese, I like to think of the fact that my great, great grandchildren, actually my great, great, great, great, great grandchildren will be going to school, assuming that the world, hopefully, is the way it is today, and they will still have these plastic waste in the environment. So it's important that we redefine how we use plastics. So a concept that we think about is what's called a circular plastics economy. And the plot on the top left, you see that plastic use has been going up as human population has been increasing, and as globalization is becoming a stronger phenomenon, we see plastic use exponentially going up. And a lot of what we do is a linear economy, where we take plastics from petroleum and the ground, and then we make the plastics. And when we're done with it, we eat one meal, 15 minutes, and then we throw it into the environment. And that is, that's a really awful way to work with plastics. Doing that, we're only going to accumulate the waste in the environment until we kill ourselves, or something like that. But a better idea is a circular economy, where we don't get any more plastics from petroleum, but we only use what's available now. And then we regenerate the waste in a way that's circular, so that we make new plastics from the waste. Current recycling methods enable what is only a pseudo-circular economy, because with each recycling process that you go through, the plastics have much lower quality, so the mechanical structure is compromised. And what that results in is that after several iterations of recycling, you end up with plastic that's virtually worthless, and it goes back into the environment. So it's just a delay in the linear process. And the key to a circular economy is to break the plastics into the monomeric forms, rather than mechanical recycling. And then we can use this monomeric forms of the plastic polymer to generate new plastics, like virgin material. And to solve this problem of plastic deconstruction into monomeric forms, a very productive or promising way is to use enzymes. One, we know that enzymes can break down polymers. We see this in nature. We see biomass, skutin, polysaccharides, and so on, broken down by enzymes. And plastic polymers resemble chemically, they resemble biomass polymers or polymers in nature. It's not surprising, it's not too surprising that the enzymatic machinery that can break down or deconstruct biological polymers can also break down synthetic man-made polymers. And so the idea is to, you know, find enzymes that are able to carry out the ester, to break the ester bonds and other similar bonds in plastic waste. So for the plastic that makes plastic bottles, it's used to make plastic bottles, polyethylene terephthalate or PET as it's called. There is a bacterium that was found in 2016 called Adrenalis hackeansis. And this bacterium surprisingly grows purely on PET as its sole carbon source. And it does this by utilizing two enzymes, PETase and METase. PET breaks down the PET polymer structure into the bi and mono forms, BHET and MHET as they're called for short. And then METase breaks down MHET or hydrolyzes MHET into TPA and ethylene glycol. And then terephthalic acid and ethylene glycol are simulated in the organism, in the organism's retinobolic pathway for energy. And then these, if you can see from the plot, the right there, the PETase enzyme led to a greater release of aromatic products as TPA and ethylene glycol compared to other similar hydrolyses. And this sort of opened the door for very exciting research on how can we find PET hydrolysis like PETase that can carry out this reaction. We also see that this PET hydrolase enzymes are found in the super family of alpha beta hydrolases. And they are similar to some of these well-known and well-studied alpha beta hydrolases like carboxyl esterases, cutanases, lipases, and so on. And this phylogenetic tree shows you can see a cluster of where most of the PETases are found at the top. And there are also several sort of evolutionarily distant but still related PETases or PET hydrolyses that are similar. Interestingly, we realized that sequence identity is not a good predictor of activity. So you could have enzymes that are very similar to known PETases that do not retain activity. For example, the two enzymes on the right, you have 611, the name of the enzyme, and 610. One is active, one is inactive, and they're 85% identity. Their structures virtually overlay practically on one another. You have similarly 601 and 604. One is active, one is not, 74% identity. And on the other hand, you have enzymes that are very different structurally and otherwise, as low as 14% identity and one retains activity and the other does not. So the simplest approach, which would be to just take the known PETases and do a BLAST search and pull up sequences and then go test the similar enzymes, would not be a good approach because sequence similarity or sequence identity is not a good prediction method. And so researchers have looked at other methods that can help identify what active PETases are. And the current state of the art is to use profile of hidden Markov models. So hidden Markov model, if you're familiar with such models, they are, they describe a protein sequence or protein sequence alignment as a Markov process and their associated emission probabilities with each state of the amino acids and insertions and deletions. So that, in essence, what you're asking in a rather simplistic way is, how does a sequence compared to the overall distribution of the profile, giving the amino acids that occur at each position, as well as insertions and deletions. And this paper published in 2018, they took, I think, eight known PETases at the time and they aligned them and then they showed the profile. And you can see that there are some positions where you have strong conservation of specific residues. And the hypothesis was that profile HMMs will capture these important positions and a search of sequence databases with profile HMM will identify, at the very least, proteins that are similar to known PETases, particularly at conserved positions. And we have used this hidden Markov model method, as well as with machine learning, to identify novel thermostable PET hydrolysis. And this paper was recently published in Nature Communications. So I will talk about some of the work that we've done here, as well as ongoing work that's not yet published. Our search methodology was to take the vast sequence database. We used NCBI non-redundant database and select thermal metagenomes from JGI. And we had about 220 million proteins that we're searching. And to search using hidden Markov models, as well as a support vector regression, a support vector classification model to predict thermostable PET hydrolysis. And then to screen those and select for, or to screen those for PET hydrolysis activity. And we're particularly interested in high temperature proteins, because at higher temperature, closer to the glass transition temperature of PET, you have increased accessibility of the enzyme to the substrate. So hypothetically, you would have improved performance or improved catalytic performance. So we wanted to identify proteins or enzymes that could withstand higher temperatures. So at the time I did this work, and I was a graduate student at the time, there were 17 known PET excesses. So I just called them PET 1 to 17, because I like life and I want to be organized. And here you see an alignment of these 17. There are some of them, I'm just showing regions of an alignment. And there's some regions where you have either insertions, lots of insertions, because you have a loop somewhere in that region. But overall, despite the fact that you have evolutionary distance sequences, some regions align specifically well. And we took special time in curating the alignment. So we used, I tried different alignment methods, and I picked the alignment methods that I thought had the best, the best performance in aligning, particularly the conserved residue. So the active site triad of the ion and holes and things like that. And we took this heated molecule model and then searched the sequence databases, about 250 million of them. And we returned, at the threshold that we set, we had about 3,500 hits. So the next thing was we wanted to select a smaller size of this hits that were predicted to be thermostable. And to predict thermostability, we're looking at how active is the enzyme at higher temperatures. A good measurement of that is the optimum temperature of activity. That's you evaluate activity at different temperatures and at the temperature at which you have maximum activities, the optimum temperature of activity. And from the literature, we see that the optimum temperature of activity correlates with the organism growth temperature. So organisms that are thermophiles or grow in high temperature environments tend to have proteins that are thermostable and their enzymes have optimum activity at higher temperatures. And the absence of optimum temperature data, we decided to use optimal growth temperature of the organism as a proxy to predict thermostability of the proteins. And for sequence features, we decided to use the sequence rather than the structure to predict thermostability because we wanted to do this in a high throughput fashion. And I first did a sample screen of different features to see what features correlate with the activity or with the thermostability. And I'm showing here just a few sequence features. And interestingly, we found that the composition of the amino acids, particularly relative composition, so aspartate relative to glutamate and so on, predicts the thermostability and every additional feature beyond the composition only provides marginal improvement in the performance. For our data set, we retrieved proteins based on the environment of the organism. So we had 8,000 sacrophilic, mesophilic, thermophilic and hypothermophilic. And it's 8,000 because we wanted to have a balanced class. So we just took the smallest class and then discarded the proteins from the other class. And we looked at different methods for predicting thermostability, different classification methods. Is this a binary classifier? Is giving a protein, is this thermophilic or hypothermophilic or is it mesophilic? And then we found that the support vector classifier compared to other methods, including k-nearest neighbor and random forest and so on, performed best. This is on GitHub currently. So it's a simple, easy to use code that can be applied to screening protein sequences. And you also notice that the accuracy of the methods, since these are based on amino acid features, the accuracy sort of correlates with the length of the sequence. Because as the sequence length increases, you have more confidence in the dipeptide composition and so on, and the amino acid features. This is the performance on a separate test data set, which the model did not see in training and validation. And overall, you see that there's about an 80% accuracy in predicting whether it's in one thermostability class or the other. And going forward, we took the thermo approach of the thermostability prediction model and apply that to the 3,500 hits that we had. And most, as you'd expect, would be mesostable. There are only a few thermostable proteins in the databases. We narrowed this down to about 74. And then we screened the 74 in vitro for activity on PET. And we looked at different temperature and different pH to understand the diversity of the variability of the activity as a function of the experimental conditions. And the darker colors indicates greater activity, the lighter colors indicate low activity. And there's some enzymes like LCCI-CCG that were previously described that retained, that demonstrated really high activity. And the enzymes that we found, we named them according to the phylogenetic clades that they fall into. There's 101 belonging in clade 1 and 2, 1 in clade 2, and so on. And you see that most of these are of low activity than the canonical LCCI-CCG, but some of them, particularly those in group 7, at specific temperature regions demonstrate similar activity. On the top left, I show a phylogenetic tree of these 74 enzymes. And we started with 220 million, of course. I say 250 million before, and I was rounding up. It's nice to say quarter of a billion instead of 220 million. So with the HMM, we narrowed that down to 3,500 PET hydrolases. The thermostability filter, we found 74 that were predicted to become a stable, we expressed 52 of them could be expressed and that we assayed them. And 38 of them were active on PET. Interestingly, 24 of these were novel, never presented before. Some of these were in completely different clusters from what's mostly known to be PET-exists. Most PET-exists fall in this group as polyesterase liposcutinase, according to the ESTA database classification. And we've obtained a patent for these 24 PET hydrolases as well. I wanted to also look at how the hidden Markov model alignment scores correlate with activity. And to discover the PET-exists, we were aligning unknown sequences or sequences with unknown function to the PET hydrolase HMM, and we select those with high HMM scores. And you see that there is generally some weak correlation between the hidden Markov model scores and measured activity. On the left, I'm just showing the specific hidden Markov model score. On the right, the golden scatter plots are the difference between an alignment with a hidden Markov model of known PET-exists versus a hidden Markov model of enzymes that are of similar sequence homology, but are validated to not be PET-exists. And you also see that on a classification framework, discriminating between active PET-exists and inactive sequence homologues, the model does not do really well. If you're familiar with receiver operating characteristic curves, you know that curves that are closer to the dashed line indicate perfect performance. The straight line is sort of random performance. And then we get AUCs of about 0.58 versus random scores of 0.5. So that's almost only slightly better than random, if you think of that. And so the question going forward was, can we use deep learning or machine learning? Because the hidden Markov model does not predict in a supervised way, it doesn't learn what activity is or how the activity relates to between one sequence and the other. It's just looking at the alignment. I wanted to see if we could predict the specific activity or the relative activity of a protein sequence with deep learning. And the beautiful thing of deep learning, or machine learning in general, is you can learn, giving the right set of features, you can learn to predict specifically attributes of proteins. For data sets, I began with going through the literature and pulling out data of experimentally measured PET hydrolysis. And this went from 28 studies, totaling about 449 PET hydrolysis. I should point out that these are both from naturals, where you have just wild type enzymes from the natural sequence landscape, as well as singles. So you take a particular enzyme and then make a single mutation. So maybe I position 100 mutated adenine to glycine or something like that. And then multiples, where you have multiple mutations, ranging from 2 to 21 mutations. And this, there are about 514 activity measurements for this 449 proteins. So you have multiple measurements for some proteins. And I wanted to see how machine learning can sort of learn from this data set to predict PET activity. The natural proteins shown here, you have sequence identity ranging from somewhere about 10% to 99% as well. And I should also point out that the conditions at which these data were generated vary. The PET substrate that was used varies, the temperature on the page varies, and you would expect that this will affect the measurements. And so that would prevent comparison between one data set and the other. So if you do direct regression, so you take the first data set and you train a model on that, you predict the exact PET hydrolysis activity that's measured at that conditions. Moving to another data set, you have a different condition, so you can't do direct regression. To overcome this limitation, the limitation that the disparate activity measurements and disparate conditions present, we introduced a strategy which learns to rank pairs from each data set. So I'm showing a practical example. So you have a study with different sequences, X1 to X4, and the measured activity at specific conditions of that study. And then you have another study that takes another group of PETIs and measures activity, and the activity values are different because of different conditions. What we do instead is we generate pairs. So from the first study, we generate all possible binary pairs. And then we predict the class, convert the regression raw values to classification tasks. And we ask the question, giving a pair of sequences, is the first sequence of better activity than the second? And this way we do away with trying to predict the exact activity. And also the model learning across all data sets, because we combine all of these data sets together, the model learns features that generally relate to PET hydrolysis activity across all the data sets. Going back, we see the different conditions, and we really can't predict PET hydrolysis activity on powder versus PET hydrolysis activity on, say, nanoparticles versus the activity at a specific pH. It will be nice to do that. Ideally, if you had a model that could tell you this is how PETases will work at a specific condition, but we can't do that because we're limited by the data. But by combining the data together and learning to rank across the data sets, we can learn potentially features that generally correlate with PET hydrolysis activity. And then for the prediction, what we do is we take each sequence in the pair, and then we generate features for that sequence. So the sequence represented as X, the features as Z, and your features can come from an unsupervised model, which I'll show in the next slide, or it can just be a simple one-hot encoding of the sequence. And then we take the difference vector of the two representations, and then we fit a simple logistic regression to that difference vector to predict the rank. And to evaluate this method, I used leave one group out cross-validation. So we took the 28 studies, and then we would train on all but one study and then test on another study. And since there are duplicates as well as high similarity between some of the sequences, we applied an 80% threshold and removed all sequences in the training set that share identity of up to 80%. 80% or more with sequences in the test set. And then we repeat this for every study until we've trained and tested on every single pair in the data set. And for features, because if you think about it, you want to have a way to represent your protein sequence. And that is representation learning or how you represent your sequence is very important. And one promising approach, at least in the literature, is to use semi-supervised methods to take unsupervised models that were trained to learn the language of proteins across the protein universe. And then use the embeddings from these models. One example is you have autoregressive models which take a protein sequence and learn from all amino acids up to a point and predict the next amino acid. Then you have math language models that mask out an amino acid and learn the context of that mass amino acid and predict what should be there. You also have models like directional autoencoders that are a bottleneck that take the input sequence and then compress them into a bottleneck latent space, represented as Z, and then learns to reconstruct the original input. And models like these have been shown to capture both structural as well as functional representations of proteins. This is from a paper in 2019 where they trained LSTMs on the protein universe, and they show that structural information as well as phylogenetic and even functional information is contained in their embeddings. And so to predict perihydrase activity, I retrieved embeddings from several of these models, particularly using some of these models that have been demonstrated to have state-of-the-art performance in downstream tasks. So we've got Unirep, an autoregressive LSTM, transformer models like Transception, autoregressive transformer, ASM, a convolution masked model called CARB, a mass model transformer, ProT5, ProGen2, and a variational autoencoder that learns from the multiple sequence alignments to predict the effect of mutations. As well, I also trained a variational autoencoder only on 18,000 proteins that are similar to perihydrases. So this we retrieved from a jackhammer search with perihydrase sequences. As you point out that all of these models except this two were trained on the protein universe. So somewhere between 25 million to a billion proteins in the protein databases, whereas these two models were trained only on the protein homologues of perihydrases, about 18,000 of them. And very interestingly, comparing the performance, on the left plot, I show the AUC distribution across the 28 studies. And you see that the one-hot representation of the multiple sequence alignment performs better than all but one of these models, which you would expect that language models should learn much richer representations of the proteins. But we see that one-hot encoding of the multiple sequence alignment is better. And particularly when you split the data into singles, multiples, and naturals, you see that the performance varies across different methods for these. We're mostly interested in naturals because we're going to be screening wild type sequences in the databases. And so the one-hot encoding has the best performance for naturals, which I think is interesting. So it means that, or it indicates that the information from the multiple sequence alignment is particularly important. And then encoding that and learning to predict Pettis activity directly from this one-hot encoding. So the model literally asks the question, what residue is at this position? And it assigns weights to specific residues at each position. I also looked at zero-shot prediction methods. And if you're familiar with zero-shot, it's where you have no training data at all. And you're just taking a model and predicting the fitness or so of your protein sequence. And on the machine learning side or deep learning side, these models usually will assign a probability to a sequence given what the model has seen in the training set. So think of it as you have an unsupervised method. Your input is the protein sequence from the protein universe. Your output is the protein sequence as well. And the model is either learning to reconstruct the inputs through a bottleneck or in another regressive fashion or a masked fashion. And the probability that that protein is similar to anything it's seen in the training set is an indication of its fitness. And you can also use zero-shot methods that are more based on just bioinformatics methods. So sequence similarity, for example, with the BLOSUM matrix or Healy-Markov models. And interestingly, you see that when you compare the BLOSUM similarity with one-pedis, IS-pedis or the canonical pedis, versus a BLOSUM similarity with a consensus sequence from the alignment, you get much better performance with consensus. Indicating that learning from the alignment, what positions are important and what residues are particularly conserved in the alignment is important. But generally, I know there's a lot of information here, but comparing the unsupervised methods, zero-shot, to the supervised or semi-supervised methods, we see that the one-hot encoding still outperforms virtually all methods, particularly for predicting naturals. The Healy-Markov model, although I previously showed that the correlation was weak with our data sets, compared to the entire data sets of the 20 data sets that we pulled out, the Healy-Markov model still shows particularly reasonable performance. And so we've made a model to predict pedis activity, which we call rank-pedis. And it takes as input the protein sequence. And instead of an unsupervised model, we just align that sequence to a pedis alignment. And we take the positions in that alignment and then one-hot encode that and take the difference of the one-hot encoding. So if there's a residue at a position, it gets a one. If there's not a residue, it's a zero. And when we take the difference, you have negative one or one, depending on where the position is. And we can predict the rank. Is this sequence a better pedis than some reference, say, IS pedis? We can also screen with the profile Healy-Markov model and get a score for that. And then we average the scores and use that to screen the sequence database. And so currently what we're working on is to use these methods for improved screening and improved mining of pet hydrolysis from the sequence databases. And it's amazing how fast these databases grow. There are currently about 3 billion proteins in the databases. And we want to screen these and rank these, and then we'll iteratively search to identify pedis. I will acknowledge my supervisor, principal investigator Greg Beckham, as well as collaborators at Harvard, Deborah Marks and Chris Sander, and postdocs and graduate students who have collaborated with me. Erica Erickson, Courtney, Nikki, and Ada. Thank you for listening, and I'll take questions. One thing we could do is to test different alignments and see which alignments give the best performance. But I think a structure-based alignment should work well. And what we're doing is to take alpha-4 structures of all the proteins we want to align, and then align the structure first, and then use that to guide the alignment as well. And the alignment will be wrong in some places, but what matters most is that it gets the most important positions correctly, the active site triad and conserved positions as well. Yeah. Local alignment versus global alignment. I think you're referring to... This. Yep. I'm showing positions that align well, but this is just a segment of the alignment. There are some positions or some regions that don't align well at all in this. And this is the 17 pedis at the time. The alignments get the most important positions. And I think that you need a multiple sequence alignment. In this case, the sequence alignment that's fed into the HMM to capture the sequence that's not aligned well. And I think that's what we're trying to do. Currently, there are about 75 pedis. So aligning those, you have even greater regions that don't align well. And I think that you need a multiple sequence alignment. In this case, the sequence alignment that's fed into the HMM to capture these conserved positions globally across all of the proteins that you're feeding in. A local alignment will miss that. Yeah. Most language models take unaligned sequences. Some language models take aligned sequences. I was using the unaligned sequence for all but one. The ESMMSA, yes. I have a question. I was wondering what's the size of your... As compared to the input in your variational monitor. For... Yeah. For the... Yes. So I did a... For EAVE, EAVE is a pre-trained or a proposed model from a different paper. And they used a specific architecture with a latent dimension of 50. Yeah. For this bespoke variational monitor that I trained, I optimized the latent space and found that 64 was optimum. And then I used 64 for that. The input is 400 and something positions by 21. So it's about 9,800, about 10,000. Yeah. And for comparison, all of the language models have latent dimensions of about 768 for ESMMSA, 1,284 for ESM1B and the others. And I think 1,500 for PUGIN2. Yeah. I have a question. So I... Yes, there were... The overall super family sort of classification, they're all alpha beta hydrolysis, but they're in different families. Yeah. As particularly as a function of the phylogenetic similarity. If you look at the tree, those ones with short branches or short leaves, particularly in groups four, five, six and seven, all fall in this polyesterase liposketenase group. And they overall have more greater similarity structure. And then in groups one, two and three, where you have more divergence, you have even greater conflicts in the structure. I was just wondering, the other question I had was, when you were doing the acid, you had some tips, you might have some tips, but was there a reason that people kind of folded on it? Most serine hydrolysis have a basic pH range. And most of them are either inactive at neutral pH or lower. And all of the pedases we've screened lose all activity at 4.5. So, yeah. Yes, yes. Oh, yes, yes, yes. We're looking at pet hydrolase activity. And so activity here is specifically, of course, these are enzymes, so they do something, probably in cutanases, lipases, yes. Not really. The original hypothesis, the first pedase that was found was in a bacterium in a plastic dump. You generally don't find it in a plastic dump. So, yeah. So, yeah. So, yeah. So, yeah. So, yeah. So, yeah. So, yeah. So, yeah. Not really. The original hypothesis, the first pedase that was found was in a bacterium in a plastic dump. But as we've moved further from that, we're finding pedases in different organisms. It was a pedase that was found in a human microbiome. So people were asking, was the person just eating a lot of plastics? The most likely explanation is that these did not evolve to be pedases because they were exposed to plastics, but they have alternative activities that are similar to pet hydrolase activity. And just because of the similarity in the structure. So, most esterases in the Syrian hydrolase family will possibly be pedases. Given the similarity. So, the nature communications paper we have published, Erickson, is one of the 28 data sets. When you hold out that data set, I think you get ABCs of 0.7. So, it does rank that data set well. And it does rank it better than the HMM, which was 0.58. And across all of the data sets, we have 0.79, I think, on average for natural. So, it does a good job of ranking the activity. So, we want to use this to, or we are currently using this to rank pet hydrolase activities across the entire sequence of space. So, I noticed in most of your screens, you're really focused on predicting thermostability and activity. But I know that from screening these enzymes, that if something has a really low expression, we're going to eliminate it very fast because it's impossible to reduce even if it's really thermostable and has a high activity. If there's just not enough of it, we're not going to really be able to use it. So, I'm wondering, like, where are you getting the expression data from? Do you have to actually express it in E. coli to get that data? Or is there any information from your models that could be used to sort of ensure its expression as well? That's a very good question. We did not include expression data in the training data at all, because that is sort of biased. If we're testing, if we're predicting the activity of an enzyme, it most likely expressed for that for it to be tested. It may not have been E. coli and may have been a different, even if it were E. coli, it would be a different expression system. I tried predicting expression, and there are machine learning methods that do this. They take language model embeddings, and then they try to predict solubility and expression in E. coli. I find that these methods, I found that these methods did not do very well on our data. In fact, the heat and marker model alignment score outperformed all of these in predicting expression for E. coli. And we also found that from our data sets that the sequences that had higher heat and marker model scores, alignment scores, expressed better. So we think that if we're selecting for things that align well with the PETAS HMM, we'll see better expression. But it's one of the problems that we were faced with. And I think we can only predict one thing at a time. Another thing we care about is pH. We're interested in lower pH because the product of PET degradation is terephthalic acid, which is acidic. And these enzymes don't work well at acidic conditions. So we want to find enzymes that are as acidic, acid tolerant as possible. So in a different project, I'm training deep models, deep learning models to predict acid tolerance and pH optimum. Currently, no. The most acidic. So we've tested these at different pH. And then most of them have optimal pH of 8, 9. If you lower it down to 6, below 6, most of them lose their activity. We took a few of them that retain activity at 6. And then we tested at 5 and 4. And only one of them had activity at 4.5. And if you go below 4.5, it loses activity. Oh, yes, we can. And we're doing that currently. It's not part of my talk, but I trained a deep learning model to predict the pH optimum from about 2 million proteins using the pH of the environment. So I took secreted enzymes and took the pH of the environment and then tried to train a model in that. And we're using deep learning methods as well as zero-shot predictions to engineer that, as well as search. So in addition to this model, we will use these pH prediction models to sort of rank the sequences that we pull out from the databases. Hopefully, we'll find things that are functional at low pH. Yes, yes. Hopefully, acidophiles should have lower pH optimum compared to other organisms. And so if a pedis is from an acidophile, then it's likely to have that. But see, now you're trying to work with four things. You're trying to predict expression. You're trying to predict activity. You're trying to predict thermostability. And you're trying to predict pH tolerance. And also, you're trying to predict substrate specificity. Does it function better on crystalline substrate versus amorphous substrate? Because if it does better on crystalline substrate, you can reduce the amount of mechanical preprocessing that goes into preparing the plastic waste. And so it's sort of an orthogonal prediction approach where these two things don't necessarily correlate. And if something is very acid tolerant, it doesn't mean it has activity. And the question of how do we combine all of these to search the sequence space is something that I'm interested in looking at. Yes, we're working on nylonases, polyurethanases, two enzymes I know that I'm working on. And there are other groups that are looking at different proteins, different enzymes, plastic enzymes as well. Pertosis seems to be the most interesting and has received the most attention in the literature. Probably because plastic bottles are polyethylene terephthalate is the most abundant man-made polymer. And so it's gotten a lot of attention. And I believe that similar approaches will yield successes in other plastic enzymes as well. So you're talking about, you know, there's a lot of different factors that need to be predicted here. There's different heights of the head. And from my experience, an enzyme that works really well on the endocrine axis doesn't necessarily work very well on crystalline heads. So we sort of talked about the idea of an enzyme cocktail where we're using a bunch of different head phases. There's just one plastic bottle because the plastic bottle doesn't have much. So I have a question about like how your approach is ideally would be applied to sort of creating an enzyme cocktail that could actually be used in similar ways. We don't have sufficient data to discriminate amorphous selecting or amorphous preferring pedases from crystalline. And I recognize that that would be a bias in the data set. If you have a lot of the different 28 studies, if you have most of them are amorphous pedase conditions were used in the screening. And so that would probably bias the models learning to amorphous conditions. But the hypothesis is that the model learns generally what makes a better pedase across conditions. But as we as we go forward, as we generate data, it will be interesting to start to play around with modeling approaches to see how that how we can discriminate these enzymes. Yeah. Oh, another question I have is just. So, now that you're sort of identifying all these novel cases, let's say, say, so can you just describe like what are the next steps in that process you've identified a novel headaches. Yeah. That's a, that's a good question on the public on the scientist side. We published a paper. Well, it's now on Google Scholar. Most scientists, that's what they care about and they're like we're done, we can move on. There's a patent for it, and companies are interested in these are using it in in their processes, but it most importantly forms a bedrock for further engineering, because as we see the different pedases have different performance and different So we could start to think about cocktails and then synthetic variants of these enzymes to improve performance we could start with. So you want to improve crystalline performance and crystalline substrate, the enzyme one of the enzymes that should really good performance 611. We could take 611 and start to do enzyme engineering on it and I know that there are groups that are working on that as well. That's a really good point. I, we have not done that. We have not fine tune language models on the pair of these data sets. That is because in the literature, sometimes that actually makes things worse, and you lose, instead of you, making it better you start to lose the global unsupervised features that we learned. And so, some other people suggest fine tuning the embedding so you have frozen embeddings and then you train a model fine tuned on that. But we're treading on, we're treading on very dangerous territory here since we have very small data as well. The risk of overfitting is large as well. Another thing is I could take the language embeddings and train the frozen language embeddings and train on an even more expansive model. Why did we use logistic regression, we could train, I have 449 sequences. Right, with the pairwise approach you can explode that and we have 18,000 pairs, but these are from 449 sequences. I think that, yeah, it's, it's important to not shoot yourself in the foot with overfitting. I did that, I did that, I did that on 200,000 hydrolases. And guess what, it made it worse. I did, I did so many things. I did, I worked on this, I played with so many iterations. The conclusion after one year of fine tuning and model architecture training and all of that was you're just overfitting Jaffa, stop shooting yourself in the foot, you're overfitting. It works, and especially if you, if you're honest with yourself and you do proper cross validation, you hold out some data set and you optimize, you find out it does well on 0.9 correlation on this data set. When you move to another data set, bang, it fails. So, looking at performance overall, it's important to not, to use the right set of models with your data. 449 proteins, you should be limiting yourself to maybe one layer, two layers max and not very deep models and things like that, yeah. Okay, thank you very much. Thank you. Thank you. Thank you. Thank you. Thank you.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 12.24, "text": " It's my pleasure to introduce Dr. Jafet Gado.", "tokens": [50364, 467, 311, 452, 6834, 281, 5366, 2491, 13, 508, 2792, 302, 460, 1573, 13, 50976], "temperature": 0.0, "avg_logprob": -0.3116166787069352, "compression_ratio": 1.300578034682081, "no_speech_prob": 0.19088327884674072}, {"id": 1, "seek": 0, "start": 12.24, "end": 18.5, "text": " He is currently a postdoctoral researcher at the National Renewable Energy Laboratory", "tokens": [50976, 634, 307, 4362, 257, 2183, 2595, 20946, 21751, 412, 264, 4862, 12883, 1023, 712, 14939, 40824, 51289], "temperature": 0.0, "avg_logprob": -0.3116166787069352, "compression_ratio": 1.300578034682081, "no_speech_prob": 0.19088327884674072}, {"id": 2, "seek": 0, "start": 18.5, "end": 24.6, "text": " working as part of the Bio-Optimized Technologies for Keeping Thermoplastics Out of Landfills", "tokens": [51289, 1364, 382, 644, 295, 264, 26840, 12, 46, 662, 332, 1602, 46993, 337, 30187, 38957, 404, 15459, 1167, 5925, 295, 6607, 69, 2565, 51594], "temperature": 0.0, "avg_logprob": -0.3116166787069352, "compression_ratio": 1.300578034682081, "no_speech_prob": 0.19088327884674072}, {"id": 3, "seek": 2460, "start": 24.6, "end": 30.28, "text": " and the Environment, or BOTTLE for short, project, working on using machine learning", "tokens": [50364, 293, 264, 35354, 11, 420, 363, 29525, 2634, 337, 2099, 11, 1716, 11, 1364, 322, 1228, 3479, 2539, 50648], "temperature": 0.0, "avg_logprob": -0.27852783203125, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.0446728840470314}, {"id": 4, "seek": 2460, "start": 30.28, "end": 36.44, "text": " to identify and engineer plastic degrading enzymes.", "tokens": [50648, 281, 5876, 293, 11403, 5900, 24740, 278, 29299, 13, 50956], "temperature": 0.0, "avg_logprob": -0.27852783203125, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.0446728840470314}, {"id": 5, "seek": 2460, "start": 36.44, "end": 40.8, "text": " And as part of that, he's currently a visiting researcher at Harvard Medical School working", "tokens": [50956, 400, 382, 644, 295, 300, 11, 415, 311, 4362, 257, 11700, 21751, 412, 13378, 15896, 5070, 1364, 51174], "temperature": 0.0, "avg_logprob": -0.27852783203125, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.0446728840470314}, {"id": 6, "seek": 2460, "start": 40.8, "end": 43.44, "text": " on part of the same project.", "tokens": [51174, 322, 644, 295, 264, 912, 1716, 13, 51306], "temperature": 0.0, "avg_logprob": -0.27852783203125, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.0446728840470314}, {"id": 7, "seek": 2460, "start": 43.44, "end": 49.08, "text": " Before this, he got his PhD at the University of Kentucky working on cellulose degrading", "tokens": [51306, 4546, 341, 11, 415, 658, 702, 14476, 412, 264, 3535, 295, 22369, 1364, 322, 2815, 425, 541, 24740, 278, 51588], "temperature": 0.0, "avg_logprob": -0.27852783203125, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.0446728840470314}, {"id": 8, "seek": 2460, "start": 49.08, "end": 54.36, "text": " enzymes and using machine learning to figure out those enzymes.", "tokens": [51588, 29299, 293, 1228, 3479, 2539, 281, 2573, 484, 729, 29299, 13, 51852], "temperature": 0.0, "avg_logprob": -0.27852783203125, "compression_ratio": 1.6942148760330578, "no_speech_prob": 0.0446728840470314}, {"id": 9, "seek": 5436, "start": 54.36, "end": 60.36, "text": " And before that, he did his bachelor's degree in chemical engineering at Amadou Fellow University.", "tokens": [50364, 400, 949, 300, 11, 415, 630, 702, 25947, 311, 4314, 294, 7313, 7043, 412, 2012, 345, 263, 44794, 3535, 13, 50664], "temperature": 0.0, "avg_logprob": -0.33871326707813837, "compression_ratio": 1.4123711340206186, "no_speech_prob": 0.0017812119331210852}, {"id": 10, "seek": 5436, "start": 60.36, "end": 65.36, "text": " So if everyone can welcome Dr. Gado, we'll get started.", "tokens": [50664, 407, 498, 1518, 393, 2928, 2491, 13, 460, 1573, 11, 321, 603, 483, 1409, 13, 50914], "temperature": 0.0, "avg_logprob": -0.33871326707813837, "compression_ratio": 1.4123711340206186, "no_speech_prob": 0.0017812119331210852}, {"id": 11, "seek": 5436, "start": 65.36, "end": 72.4, "text": " Hi, good to see everyone's faces.", "tokens": [50914, 2421, 11, 665, 281, 536, 1518, 311, 8475, 13, 51266], "temperature": 0.0, "avg_logprob": -0.33871326707813837, "compression_ratio": 1.4123711340206186, "no_speech_prob": 0.0017812119331210852}, {"id": 12, "seek": 5436, "start": 72.4, "end": 80.72, "text": " And yeah, I remember not too many days ago, I was a graduate student sitting and then", "tokens": [51266, 400, 1338, 11, 286, 1604, 406, 886, 867, 1708, 2057, 11, 286, 390, 257, 8080, 3107, 3798, 293, 550, 51682], "temperature": 0.0, "avg_logprob": -0.33871326707813837, "compression_ratio": 1.4123711340206186, "no_speech_prob": 0.0017812119331210852}, {"id": 13, "seek": 8072, "start": 80.88, "end": 82.64, "text": " listening to presentations like this.", "tokens": [50372, 4764, 281, 18964, 411, 341, 13, 50460], "temperature": 0.0, "avg_logprob": -0.2612180035523694, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.02293466590344906}, {"id": 14, "seek": 8072, "start": 82.64, "end": 88.72, "text": " And I would say, oh, I'll never understand, you know, they look so strange.", "tokens": [50460, 400, 286, 576, 584, 11, 1954, 11, 286, 603, 1128, 1223, 11, 291, 458, 11, 436, 574, 370, 5861, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2612180035523694, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.02293466590344906}, {"id": 15, "seek": 8072, "start": 88.72, "end": 92.28, "text": " So while I was preparing this, I had that in mind.", "tokens": [50764, 407, 1339, 286, 390, 10075, 341, 11, 286, 632, 300, 294, 1575, 13, 50942], "temperature": 0.0, "avg_logprob": -0.2612180035523694, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.02293466590344906}, {"id": 16, "seek": 8072, "start": 92.28, "end": 96.4, "text": " I was like, I hope I will I will present in a way that everyone can understand.", "tokens": [50942, 286, 390, 411, 11, 286, 1454, 286, 486, 286, 486, 1974, 294, 257, 636, 300, 1518, 393, 1223, 13, 51148], "temperature": 0.0, "avg_logprob": -0.2612180035523694, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.02293466590344906}, {"id": 17, "seek": 8072, "start": 96.4, "end": 98.4, "text": " Yeah. But thank you for having me.", "tokens": [51148, 865, 13, 583, 1309, 291, 337, 1419, 385, 13, 51248], "temperature": 0.0, "avg_logprob": -0.2612180035523694, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.02293466590344906}, {"id": 18, "seek": 8072, "start": 98.4, "end": 104.64, "text": " I am I'm excited to be here and just to talk about my work with you.", "tokens": [51248, 286, 669, 286, 478, 2919, 281, 312, 510, 293, 445, 281, 751, 466, 452, 589, 365, 291, 13, 51560], "temperature": 0.0, "avg_logprob": -0.2612180035523694, "compression_ratio": 1.5746606334841629, "no_speech_prob": 0.02293466590344906}, {"id": 19, "seek": 10464, "start": 104.64, "end": 111.4, "text": " So let's get into machine learning for discovery and engineering of plastic degrading enzymes.", "tokens": [50364, 407, 718, 311, 483, 666, 3479, 2539, 337, 12114, 293, 7043, 295, 5900, 24740, 278, 29299, 13, 50702], "temperature": 0.0, "avg_logprob": -0.24373433084198923, "compression_ratio": 1.5520833333333333, "no_speech_prob": 2.505621887394227e-05}, {"id": 20, "seek": 10464, "start": 114.48, "end": 123.88, "text": " Everyone here, I believe, is familiar with the problem with plastics and the global crisis of plastic pollution.", "tokens": [50856, 5198, 510, 11, 286, 1697, 11, 307, 4963, 365, 264, 1154, 365, 34356, 293, 264, 4338, 5869, 295, 5900, 16727, 13, 51326], "temperature": 0.0, "avg_logprob": -0.24373433084198923, "compression_ratio": 1.5520833333333333, "no_speech_prob": 2.505621887394227e-05}, {"id": 21, "seek": 10464, "start": 123.88, "end": 132.48, "text": " We've seen images like this where you have plastics and oceans and landfills and all that.", "tokens": [51326, 492, 600, 1612, 5267, 411, 341, 689, 291, 362, 34356, 293, 25004, 293, 2117, 69, 2565, 293, 439, 300, 13, 51756], "temperature": 0.0, "avg_logprob": -0.24373433084198923, "compression_ratio": 1.5520833333333333, "no_speech_prob": 2.505621887394227e-05}, {"id": 22, "seek": 13248, "start": 132.48, "end": 140.28, "text": " And it's even beyond just plastics polluting our environment to plastics getting into places where we don't even think they would get into", "tokens": [50364, 400, 309, 311, 754, 4399, 445, 34356, 6418, 10861, 527, 2823, 281, 34356, 1242, 666, 3190, 689, 321, 500, 380, 754, 519, 436, 576, 483, 666, 50754], "temperature": 0.0, "avg_logprob": -0.27337471383516904, "compression_ratio": 1.6032608695652173, "no_speech_prob": 7.248226756928489e-05}, {"id": 23, "seek": 13248, "start": 142.04, "end": 144.04, "text": " protected areas in the United States.", "tokens": [50842, 10594, 3179, 294, 264, 2824, 3040, 13, 50942], "temperature": 0.0, "avg_logprob": -0.27337471383516904, "compression_ratio": 1.6032608695652173, "no_speech_prob": 7.248226756928489e-05}, {"id": 24, "seek": 13248, "start": 144.83999999999997, "end": 154.0, "text": " National forests and wilderness are contaminated with plastics because microplastics are taken there by wind and rain.", "tokens": [50982, 4862, 21700, 293, 27613, 366, 34492, 365, 34356, 570, 4532, 564, 21598, 366, 2726, 456, 538, 2468, 293, 4830, 13, 51440], "temperature": 0.0, "avg_logprob": -0.27337471383516904, "compression_ratio": 1.6032608695652173, "no_speech_prob": 7.248226756928489e-05}, {"id": 25, "seek": 15400, "start": 154.44, "end": 166.36, "text": " Deep in the ocean, we see plastics in coral reefs and this has been associated with disease so that there's a higher likelihood of disease where you have coral reefs with", "tokens": [50386, 14895, 294, 264, 7810, 11, 321, 536, 34356, 294, 24955, 50054, 293, 341, 575, 668, 6615, 365, 4752, 370, 300, 456, 311, 257, 2946, 411, 2081, 71, 1816, 295, 4752, 689, 291, 362, 24955, 50054, 365, 50982], "temperature": 0.0, "avg_logprob": -0.3980125427246094, "compression_ratio": 1.8133333333333332, "no_speech_prob": 6.013138045091182e-05}, {"id": 26, "seek": 15400, "start": 167.04, "end": 174.64, "text": " plastic contamination. And we have microplastics in the human body as well and the bloodstream and so on. And this leads to all kinds of disease.", "tokens": [51016, 5900, 33012, 13, 400, 321, 362, 4532, 564, 21598, 294, 264, 1952, 1772, 382, 731, 293, 264, 3390, 9291, 293, 370, 322, 13, 400, 341, 6689, 281, 439, 3685, 295, 274, 908, 651, 13, 51396], "temperature": 0.0, "avg_logprob": -0.3980125427246094, "compression_ratio": 1.8133333333333332, "no_speech_prob": 6.013138045091182e-05}, {"id": 27, "seek": 15400, "start": 174.96, "end": 181.0, "text": " And we know that there is a need to redefine how we use plastics and how we work with them.", "tokens": [51412, 400, 321, 458, 300, 456, 307, 257, 643, 281, 38818, 533, 577, 321, 764, 34356, 293, 577, 321, 589, 365, 552, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3980125427246094, "compression_ratio": 1.8133333333333332, "no_speech_prob": 6.013138045091182e-05}, {"id": 28, "seek": 18100, "start": 181.12, "end": 187.36, "text": " Also, very importantly, the life cycle of a plastic item like this bottle", "tokens": [50370, 2743, 11, 588, 8906, 11, 264, 993, 6586, 295, 257, 5900, 3174, 411, 341, 7817, 50682], "temperature": 0.0, "avg_logprob": -0.40476029494713095, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0006561998161487281}, {"id": 29, "seek": 18100, "start": 188.28, "end": 200.24, "text": " is quite long. It's about 500 to 700 years to begin degrading it. So every time I order Chinese, I like to think of the fact that my great, great grandchildren, actually my great, great, great, great, great grandchildren", "tokens": [50728, 307, 1596, 938, 13, 467, 311, 466, 5923, 281, 15204, 924, 281, 1841, 24740, 278, 309, 13, 407, 633, 565, 286, 1668, 4649, 11, 286, 411, 281, 519, 295, 264, 1186, 300, 452, 869, 11, 869, 28112, 11, 767, 452, 869, 11, 869, 11, 869, 11, 869, 11, 869, 28112, 51326], "temperature": 0.0, "avg_logprob": -0.40476029494713095, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0006561998161487281}, {"id": 30, "seek": 18100, "start": 200.56, "end": 203.92000000000002, "text": " will be going to school, assuming that the world, hopefully, is", "tokens": [51342, 486, 312, 516, 281, 1395, 11, 11926, 300, 264, 1002, 11, 4696, 11, 307, 51510], "temperature": 0.0, "avg_logprob": -0.40476029494713095, "compression_ratio": 1.6272727272727272, "no_speech_prob": 0.0006561998161487281}, {"id": 31, "seek": 20392, "start": 203.92, "end": 210.51999999999998, "text": " the way it is today, and they will still have these plastic waste in the environment. So it's", "tokens": [50364, 264, 636, 309, 307, 965, 11, 293, 436, 486, 920, 362, 613, 5900, 5964, 294, 264, 2823, 13, 407, 309, 311, 50694], "temperature": 0.0, "avg_logprob": -0.40223706911687984, "compression_ratio": 1.6, "no_speech_prob": 0.00546581344678998}, {"id": 32, "seek": 20392, "start": 211.23999999999998, "end": 220.04, "text": " important that we redefine how we use plastics. So a concept that we think about is what's called a circular plastics economy.", "tokens": [50730, 1021, 300, 321, 38818, 533, 577, 321, 764, 34356, 13, 407, 257, 3410, 300, 321, 519, 466, 307, 437, 311, 1219, 257, 16476, 34356, 5010, 13, 51170], "temperature": 0.0, "avg_logprob": -0.40223706911687984, "compression_ratio": 1.6, "no_speech_prob": 0.00546581344678998}, {"id": 33, "seek": 20392, "start": 220.67999999999998, "end": 226.2, "text": " And the plot on the top left, you see that plastic use has been going up as", "tokens": [51202, 400, 264, 7542, 322, 264, 1192, 1411, 11, 291, 536, 300, 5900, 764, 575, 668, 516, 493, 382, 51478], "temperature": 0.0, "avg_logprob": -0.40223706911687984, "compression_ratio": 1.6, "no_speech_prob": 0.00546581344678998}, {"id": 34, "seek": 22620, "start": 226.2, "end": 228.35999999999999, "text": " human population has been increasing, and as", "tokens": [50364, 1952, 4415, 575, 668, 5662, 11, 293, 382, 50472], "temperature": 0.0, "avg_logprob": -0.4797391024502841, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.03297559171915054}, {"id": 35, "seek": 22620, "start": 229.39999999999998, "end": 236.23999999999998, "text": " globalization is becoming a stronger phenomenon, we see plastic use exponentially going up. And", "tokens": [50524, 40518, 307, 5617, 257, 7249, 14029, 11, 321, 536, 5900, 764, 37330, 516, 493, 13, 400, 50866], "temperature": 0.0, "avg_logprob": -0.4797391024502841, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.03297559171915054}, {"id": 36, "seek": 22620, "start": 237.11999999999998, "end": 245.92, "text": " a lot of what we do is a linear economy, where we take plastics from petroleum and the ground, and then we make the plastics. And when we're done with it,", "tokens": [50910, 257, 688, 295, 437, 321, 360, 307, 257, 8213, 5010, 11, 689, 321, 747, 34356, 490, 47641, 293, 264, 2727, 11, 293, 550, 321, 652, 264, 34356, 13, 400, 562, 321, 434, 1096, 365, 309, 11, 51350], "temperature": 0.0, "avg_logprob": -0.4797391024502841, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.03297559171915054}, {"id": 37, "seek": 22620, "start": 246.23999999999998, "end": 252.64, "text": " we eat one meal, 15 minutes, and then we throw it into the environment. And that is, that's a really awful way to", "tokens": [51366, 321, 1862, 472, 6791, 11, 2119, 2077, 11, 293, 550, 321, 3507, 309, 666, 264, 2823, 13, 400, 300, 307, 11, 300, 311, 257, 534, 11232, 5406, 88, 281, 51686], "temperature": 0.0, "avg_logprob": -0.4797391024502841, "compression_ratio": 1.6900826446280992, "no_speech_prob": 0.03297559171915054}, {"id": 38, "seek": 25264, "start": 252.64, "end": 263.08, "text": " work with plastics. Doing that, we're only going to accumulate the waste in the environment until we kill ourselves, or something like that. But a better idea is a circular economy, where we", "tokens": [50364, 589, 365, 34356, 13, 18496, 300, 11, 321, 434, 787, 516, 281, 33384, 264, 5964, 294, 264, 2823, 1826, 321, 1961, 4175, 11, 420, 746, 411, 300, 13, 583, 257, 1101, 1558, 307, 257, 16476, 5010, 11, 689, 321, 50886], "temperature": 0.0, "avg_logprob": -0.3679764157249814, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.005906985606998205}, {"id": 39, "seek": 25264, "start": 263.44, "end": 274.59999999999997, "text": " don't get any more plastics from petroleum, but we only use what's available now. And then we regenerate the waste in a way that's circular, so that we make new plastics from", "tokens": [50904, 500, 380, 483, 604, 544, 34356, 490, 47641, 11, 457, 321, 787, 764, 437, 311, 2435, 586, 13, 400, 550, 321, 26358, 473, 264, 5964, 294, 257, 636, 300, 311, 16476, 11, 370, 300, 321, 652, 777, 34356, 490, 51462], "temperature": 0.0, "avg_logprob": -0.3679764157249814, "compression_ratio": 1.6590909090909092, "no_speech_prob": 0.005906985606998205}, {"id": 40, "seek": 27460, "start": 274.6, "end": 284.0, "text": " the waste. Current recycling methods enable what is only a pseudo-circular economy, because with each recycling", "tokens": [50364, 264, 5964, 13, 15629, 23363, 7150, 9528, 437, 307, 787, 257, 35899, 12, 23568, 17792, 5010, 11, 570, 365, 1184, 319, 1344, 12455, 50834], "temperature": 0.0, "avg_logprob": -0.5013818034419307, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.015170946717262268}, {"id": 41, "seek": 27460, "start": 284.92, "end": 287.92, "text": " process that you go through, the plastics", "tokens": [50880, 1399, 300, 291, 352, 807, 11, 264, 34356, 51030], "temperature": 0.0, "avg_logprob": -0.5013818034419307, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.015170946717262268}, {"id": 42, "seek": 27460, "start": 289.12, "end": 294.6, "text": " have much lower quality, so the mechanical structure is compromised. And what that", "tokens": [51090, 362, 709, 3126, 3125, 11, 370, 264, 12070, 3877, 307, 32463, 13, 400, 437, 300, 51364], "temperature": 0.0, "avg_logprob": -0.5013818034419307, "compression_ratio": 1.4567901234567902, "no_speech_prob": 0.015170946717262268}, {"id": 43, "seek": 29460, "start": 294.6, "end": 303.20000000000005, "text": " results in is that after several iterations of recycling, you end up with plastic that's virtually worthless, and it goes back into the environment. So it's just a delay in", "tokens": [50364, 3542, 294, 307, 300, 934, 2940, 36540, 295, 23363, 11, 291, 917, 493, 365, 5900, 300, 311, 14103, 34857, 11, 293, 309, 1709, 646, 666, 264, 2823, 13, 407, 309, 311, 445, 257, 8577, 294, 50794], "temperature": 0.0, "avg_logprob": -0.4677989508516045, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.017679085955023766}, {"id": 44, "seek": 29460, "start": 303.84000000000003, "end": 319.84000000000003, "text": " the linear process. And the key to a circular economy is to break the plastics into the monomeric forms, rather than mechanical recycling. And then we can use this monomeric forms of the plastic polymer to generate new plastics, like", "tokens": [50826, 264, 8213, 1399, 13, 400, 264, 2141, 281, 257, 16476, 5010, 307, 281, 1821, 264, 34356, 666, 264, 1108, 14301, 299, 6422, 11, 2831, 813, 12070, 23363, 13, 400, 550, 321, 393, 764, 341, 1108, 14301, 299, 6422, 295, 264, 5900, 1180, 88, 936, 281, 8460, 777, 34356, 11, 287, 1035, 68, 51626], "temperature": 0.0, "avg_logprob": -0.4677989508516045, "compression_ratio": 1.6916666666666667, "no_speech_prob": 0.017679085955023766}, {"id": 45, "seek": 31984, "start": 320.11999999999995, "end": 320.79999999999995, "text": " virgin material.", "tokens": [50378, 26404, 2527, 13, 50412], "temperature": 0.0, "avg_logprob": -0.38316903024349575, "compression_ratio": 1.3974358974358974, "no_speech_prob": 0.0014534812653437257}, {"id": 46, "seek": 31984, "start": 322.32, "end": 337.23999999999995, "text": " And to solve this problem of plastic deconstruction into monomeric forms, a very productive or promising way is to use enzymes. One, we know that enzymes can break down polymers. We see this in nature.", "tokens": [50488, 400, 281, 5039, 341, 1154, 295, 5900, 49473, 3826, 666, 1108, 14301, 299, 6422, 11, 257, 588, 13304, 420, 20257, 636, 307, 281, 764, 29299, 13, 1485, 11, 321, 458, 300, 29299, 393, 1821, 760, 6754, 18552, 13, 492, 536, 341, 294, 3687, 13, 51234], "temperature": 0.0, "avg_logprob": -0.38316903024349575, "compression_ratio": 1.3974358974358974, "no_speech_prob": 0.0014534812653437257}, {"id": 47, "seek": 33724, "start": 337.24, "end": 341.56, "text": " We see biomass, skutin, polysaccharides, and so on, broken down by", "tokens": [50364, 492, 536, 47420, 11, 1110, 325, 259, 11, 6754, 82, 326, 7374, 1875, 11, 293, 370, 322, 11, 5463, 760, 538, 50580], "temperature": 0.0, "avg_logprob": -0.4964340527852376, "compression_ratio": 1.6, "no_speech_prob": 0.044622212648391724}, {"id": 48, "seek": 33724, "start": 343.76, "end": 349.0, "text": " enzymes. And plastic polymers resemble chemically, they resemble", "tokens": [50690, 29299, 13, 400, 5900, 6754, 18552, 36870, 4771, 984, 11, 436, 36870, 50952], "temperature": 0.0, "avg_logprob": -0.4964340527852376, "compression_ratio": 1.6, "no_speech_prob": 0.044622212648391724}, {"id": 49, "seek": 33724, "start": 350.08, "end": 359.6, "text": " biomass polymers or polymers in nature. It's not surprising, it's not too surprising that the enzymatic machinery that can break down or deconstruct", "tokens": [51006, 47420, 6754, 18552, 420, 6754, 18552, 294, 3687, 13, 467, 311, 406, 8830, 11, 309, 311, 406, 886, 8830, 300, 264, 16272, 25915, 27302, 300, 393, 1821, 760, 420, 49473, 1757, 51482], "temperature": 0.0, "avg_logprob": -0.4964340527852376, "compression_ratio": 1.6, "no_speech_prob": 0.044622212648391724}, {"id": 50, "seek": 35960, "start": 359.72, "end": 377.20000000000005, "text": " biological polymers can also break down synthetic man-made polymers. And so the idea is to, you know, find enzymes that are able to carry out the ester, to break the ester bonds and other similar bonds in plastic waste.", "tokens": [50370, 13910, 6754, 18552, 393, 611, 1821, 760, 23420, 587, 12, 10341, 6754, 18552, 13, 400, 370, 264, 1558, 307, 281, 11, 291, 458, 11, 915, 29299, 300, 366, 1075, 281, 3985, 484, 264, 871, 260, 11, 281, 1821, 264, 871, 260, 14713, 293, 661, 2531, 14713, 294, 5900, 5964, 13, 51244], "temperature": 0.0, "avg_logprob": -0.32657128793221935, "compression_ratio": 1.5, "no_speech_prob": 0.000698572548571974}, {"id": 51, "seek": 37720, "start": 377.32, "end": 385.71999999999997, "text": " So for the plastic that makes plastic bottles, it's used to make plastic bottles, polyethylene terephthalate or PET as it's called.", "tokens": [50370, 407, 337, 264, 5900, 300, 1669, 5900, 15923, 11, 309, 311, 1143, 281, 652, 5900, 15923, 11, 6754, 3293, 46830, 256, 323, 79, 49123, 473, 420, 21968, 382, 309, 311, 1219, 13, 50790], "temperature": 0.4, "avg_logprob": -0.6035054524739584, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.0060949260368943214}, {"id": 52, "seek": 37720, "start": 387.52, "end": 396.4, "text": " There is a bacterium that was found in 2016 called Adrenalis hackeansis. And this bacterium surprisingly grows purely on", "tokens": [50880, 821, 307, 257, 9755, 2197, 300, 390, 1352, 294, 6549, 1219, 1999, 1095, 304, 271, 324, 18627, 599, 271, 13, 400, 341, 9755, 2197, 17600, 13156, 17491, 322, 51324], "temperature": 0.4, "avg_logprob": -0.6035054524739584, "compression_ratio": 1.4911242603550297, "no_speech_prob": 0.0060949260368943214}, {"id": 53, "seek": 39640, "start": 396.4, "end": 409.76, "text": " PET as its sole carbon source. And it does this by utilizing two enzymes, PETase and METase. PET breaks down the PET polymer structure into the", "tokens": [50364, 21968, 382, 1080, 12321, 5954, 4009, 13, 400, 309, 775, 341, 538, 26775, 732, 29299, 11, 21968, 651, 293, 376, 4850, 651, 13, 21968, 9857, 760, 264, 21968, 20073, 3877, 666, 264, 51032], "temperature": 0.0, "avg_logprob": -0.5282501644558377, "compression_ratio": 1.2222222222222223, "no_speech_prob": 0.0345853716135025}, {"id": 54, "seek": 40976, "start": 409.76, "end": 425.68, "text": " bi and mono forms, BHET and MHET as they're called for short. And then METase breaks down MHET or hydrolyzes MHET into TPA and ethylene glycol. And then terephthalic acid and ethylene glycol are simulated in the organism, in the organism's retinobolic pathway for energy.", "tokens": [50364, 3228, 293, 35624, 6422, 11, 40342, 4850, 293, 376, 39, 4850, 382, 436, 434, 1219, 337, 2099, 13, 400, 550, 376, 4850, 651, 9857, 760, 376, 39, 4850, 420, 15435, 356, 12214, 376, 39, 4850, 666, 314, 10297, 293, 1030, 3495, 21323, 22633, 8768, 13, 400, 550, 256, 323, 79, 49123, 299, 8258, 293, 1030, 3495, 21323, 22633, 8768, 366, 41713, 294, 264, 24128, 11, 294, 264, 24128, 311, 1533, 259, 996, 78, 1050, 18590, 337, 2281, 13, 51160], "temperature": 0.0, "avg_logprob": -0.46774259427698645, "compression_ratio": 1.5397727272727273, "no_speech_prob": 0.06347963958978653}, {"id": 55, "seek": 42568, "start": 425.68, "end": 451.72, "text": " And then these, if you can see from the plot, the right there, the PETase enzyme led to a greater release of aromatic products as TPA and ethylene glycol compared to other similar hydrolyses.", "tokens": [50364, 400, 550, 613, 11, 498, 291, 393, 536, 490, 264, 7542, 11, 264, 558, 456, 11, 264, 21968, 651, 24521, 4684, 281, 257, 5044, 4374, 295, 45831, 3383, 382, 314, 10297, 293, 1030, 3495, 21323, 22633, 8768, 5347, 281, 661, 2531, 15435, 356, 6196, 13, 51666], "temperature": 0.0, "avg_logprob": -0.3987597251424984, "compression_ratio": 1.3642857142857143, "no_speech_prob": 0.1049710214138031}, {"id": 56, "seek": 45172, "start": 452.44000000000005, "end": 462.64000000000004, "text": " And this sort of opened the door for very exciting research on how can we find PET hydrolysis like PETase that can carry out this reaction.", "tokens": [50400, 400, 341, 1333, 295, 5625, 264, 2853, 337, 588, 4670, 2132, 322, 577, 393, 321, 915, 21968, 15435, 356, 17122, 411, 21968, 651, 300, 393, 3985, 484, 341, 5480, 13, 50910], "temperature": 0.0, "avg_logprob": -0.3468939722800741, "compression_ratio": 1.4, "no_speech_prob": 0.026734935119748116}, {"id": 57, "seek": 45172, "start": 465.28000000000003, "end": 469.96000000000004, "text": " We also see that this PET hydrolase enzymes are found in", "tokens": [51042, 492, 611, 536, 300, 341, 21968, 5796, 6623, 651, 29299, 366, 1352, 294, 51276], "temperature": 0.0, "avg_logprob": -0.3468939722800741, "compression_ratio": 1.4, "no_speech_prob": 0.026734935119748116}, {"id": 58, "seek": 46996, "start": 470.96, "end": 485.96, "text": " the super family of alpha beta hydrolases. And they are similar to some of these well-known and well-studied alpha beta hydrolases like carboxyl esterases, cutanases, lipases, and so on. And this phylogenetic tree shows", "tokens": [50414, 264, 1687, 1605, 295, 8961, 9861, 5796, 6623, 1957, 13, 400, 436, 366, 2531, 281, 512, 295, 613, 731, 12, 6861, 293, 731, 12, 28349, 1091, 8961, 9861, 5796, 6623, 1957, 411, 1032, 4995, 5088, 871, 260, 1957, 11, 1723, 282, 1957, 11, 8280, 1957, 11, 293, 370, 322, 13, 400, 341, 903, 88, 4987, 268, 3532, 4230, 3110, 51164], "temperature": 0.0, "avg_logprob": -0.31674691041310626, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.020892048254609108}, {"id": 59, "seek": 46996, "start": 487.79999999999995, "end": 495.84, "text": " you can see a cluster of where most of the PETases are found at the top. And there are also several sort of evolutionarily distant but still related", "tokens": [51256, 291, 393, 536, 257, 13630, 295, 689, 881, 295, 264, 21968, 1957, 366, 1352, 412, 264, 1192, 13, 400, 456, 366, 611, 2940, 1333, 295, 9303, 3289, 17275, 457, 920, 4077, 51658], "temperature": 0.0, "avg_logprob": -0.31674691041310626, "compression_ratio": 1.665158371040724, "no_speech_prob": 0.020892048254609108}, {"id": 60, "seek": 49584, "start": 496.15999999999997, "end": 500.47999999999996, "text": " PETases or PET hydrolyses that are similar.", "tokens": [50380, 21968, 1957, 420, 21968, 15435, 356, 6196, 300, 366, 2531, 13, 50596], "temperature": 0.0, "avg_logprob": -0.28566845980557526, "compression_ratio": 1.550561797752809, "no_speech_prob": 6.108360685175285e-05}, {"id": 61, "seek": 49584, "start": 502.44, "end": 508.76, "text": " Interestingly, we realized that sequence identity is not a good predictor of activity.", "tokens": [50694, 30564, 11, 321, 5334, 300, 8310, 6575, 307, 406, 257, 665, 6069, 284, 295, 5191, 13, 51010], "temperature": 0.0, "avg_logprob": -0.28566845980557526, "compression_ratio": 1.550561797752809, "no_speech_prob": 6.108360685175285e-05}, {"id": 62, "seek": 49584, "start": 509.44, "end": 518.1999999999999, "text": " So you could have enzymes that are very similar to known PETases that do not retain activity. For example, the two enzymes on the right, you have", "tokens": [51044, 407, 291, 727, 362, 29299, 300, 366, 588, 2531, 281, 2570, 21968, 1957, 300, 360, 406, 18340, 5191, 13, 1171, 1365, 11, 264, 732, 29299, 322, 264, 558, 11, 291, 362, 51482], "temperature": 0.0, "avg_logprob": -0.28566845980557526, "compression_ratio": 1.550561797752809, "no_speech_prob": 6.108360685175285e-05}, {"id": 63, "seek": 51820, "start": 519.08, "end": 526.72, "text": " 611, the name of the enzyme, and 610. One is active, one is inactive, and they're 85% identity. Their structures virtually overlay", "tokens": [50408, 1386, 5348, 11, 264, 1315, 295, 264, 24521, 11, 293, 1386, 3279, 13, 1485, 307, 4967, 11, 472, 307, 294, 12596, 11, 293, 436, 434, 14695, 4, 6575, 13, 6710, 9227, 14103, 31741, 50790], "temperature": 0.0, "avg_logprob": -0.2854889888389438, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.003945068921893835}, {"id": 64, "seek": 51820, "start": 527.32, "end": 535.72, "text": " practically on one another. You have similarly 601 and 604. One is active, one is not, 74% identity. And on the other hand, you have", "tokens": [50820, 15667, 322, 472, 1071, 13, 509, 362, 14138, 4060, 16, 293, 4060, 19, 13, 1485, 307, 4967, 11, 472, 307, 406, 11, 28868, 4, 6575, 13, 400, 322, 264, 661, 1011, 11, 291, 362, 51240], "temperature": 0.0, "avg_logprob": -0.2854889888389438, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.003945068921893835}, {"id": 65, "seek": 51820, "start": 536.36, "end": 545.12, "text": " enzymes that are very different structurally and otherwise, as low as 14% identity and one retains activity and the other does not.", "tokens": [51272, 29299, 300, 366, 588, 819, 6594, 6512, 293, 5911, 11, 382, 2295, 382, 3499, 4, 6575, 293, 472, 1533, 2315, 5191, 293, 264, 661, 775, 406, 13, 51710], "temperature": 0.0, "avg_logprob": -0.2854889888389438, "compression_ratio": 1.7633928571428572, "no_speech_prob": 0.003945068921893835}, {"id": 66, "seek": 54512, "start": 545.6, "end": 562.92, "text": " So the simplest approach, which would be to just take the known PETases and do a BLAST search and pull up sequences and then go test the similar enzymes, would not be a good approach because sequence similarity or sequence identity is not a good prediction method.", "tokens": [50388, 407, 264, 22811, 3109, 11, 597, 576, 312, 281, 445, 747, 264, 2570, 21968, 1957, 293, 360, 257, 15132, 20398, 3164, 293, 2235, 493, 22978, 293, 550, 352, 1500, 264, 2531, 29299, 11, 576, 406, 312, 257, 665, 3109, 570, 8310, 32194, 420, 8310, 6575, 307, 406, 257, 665, 17630, 3170, 13, 51254], "temperature": 0.0, "avg_logprob": -0.28660413197108675, "compression_ratio": 1.5714285714285714, "no_speech_prob": 1.892462205432821e-05}, {"id": 67, "seek": 56292, "start": 563.64, "end": 574.0799999999999, "text": " And so researchers have looked at other methods that can help identify what active PETases are. And", "tokens": [50400, 400, 370, 10309, 362, 2956, 412, 661, 7150, 300, 393, 854, 5876, 437, 4967, 21968, 1957, 366, 13, 400, 50922], "temperature": 0.0, "avg_logprob": -0.3562700907389323, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.01940169185400009}, {"id": 68, "seek": 56292, "start": 574.88, "end": 582.4399999999999, "text": " the current state of the art is to use profile of hidden Markov models. So hidden Markov model, if you're familiar with such models, they are, they describe", "tokens": [50962, 264, 2190, 1785, 295, 264, 1523, 307, 281, 764, 7964, 295, 7633, 3934, 5179, 5245, 13, 407, 7633, 3934, 5179, 2316, 11, 498, 291, 434, 4963, 365, 1270, 5245, 11, 436, 366, 11, 436, 6786, 51340], "temperature": 0.0, "avg_logprob": -0.3562700907389323, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.01940169185400009}, {"id": 69, "seek": 58244, "start": 582.96, "end": 589.2, "text": " a protein sequence or protein sequence alignment as a Markov process and their associated", "tokens": [50390, 257, 7944, 8310, 420, 7944, 8310, 18515, 382, 257, 3934, 5179, 1399, 293, 641, 6615, 50702], "temperature": 0.0, "avg_logprob": -0.26828255286583536, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.03356200456619263}, {"id": 70, "seek": 58244, "start": 589.72, "end": 604.8000000000001, "text": " emission probabilities with each state of the amino acids and insertions and deletions. So that, in essence, what you're asking in a rather simplistic way is, how does a sequence compared to the overall distribution of", "tokens": [50728, 29513, 33783, 365, 1184, 1785, 295, 264, 24674, 21667, 293, 8969, 626, 293, 1103, 302, 626, 13, 407, 300, 11, 294, 12801, 11, 437, 291, 434, 3365, 294, 257, 2831, 44199, 636, 307, 11, 577, 775, 257, 8310, 5347, 281, 264, 4787, 7316, 295, 51482], "temperature": 0.0, "avg_logprob": -0.26828255286583536, "compression_ratio": 1.5958549222797926, "no_speech_prob": 0.03356200456619263}, {"id": 71, "seek": 60480, "start": 605.64, "end": 612.0799999999999, "text": " the profile, giving the amino acids that occur at each position, as well as insertions and deletions. And", "tokens": [50406, 264, 7964, 11, 2902, 264, 24674, 21667, 300, 5160, 412, 1184, 2535, 11, 382, 731, 382, 8969, 626, 293, 1103, 302, 626, 13, 400, 50728], "temperature": 0.0, "avg_logprob": -0.34347998468499435, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.001081525580957532}, {"id": 72, "seek": 60480, "start": 613.0, "end": 621.24, "text": " this paper published in 2018, they took, I think, eight known PETases at the time and they aligned them and then they showed the", "tokens": [50774, 341, 3035, 6572, 294, 6096, 11, 436, 1890, 11, 286, 519, 11, 3180, 2570, 21968, 1957, 412, 264, 565, 293, 436, 17962, 552, 293, 550, 436, 4712, 264, 51186], "temperature": 0.0, "avg_logprob": -0.34347998468499435, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.001081525580957532}, {"id": 73, "seek": 60480, "start": 621.68, "end": 626.16, "text": " profile. And you can see that there are some positions where you have strong conservation", "tokens": [51208, 7964, 13, 400, 291, 393, 536, 300, 456, 366, 512, 8432, 689, 291, 362, 2068, 16185, 51432], "temperature": 0.0, "avg_logprob": -0.34347998468499435, "compression_ratio": 1.5728155339805825, "no_speech_prob": 0.001081525580957532}, {"id": 74, "seek": 62616, "start": 626.56, "end": 644.76, "text": " of specific residues. And the hypothesis was that profile HMMs will capture these important positions and a search of sequence databases with profile HMM will identify, at the very least, proteins that are similar to known PETases, particularly at conserved positions.", "tokens": [50384, 295, 2685, 13141, 1247, 13, 400, 264, 17291, 390, 300, 7964, 389, 17365, 82, 486, 7983, 613, 1021, 8432, 293, 257, 3164, 295, 8310, 22380, 365, 7964, 389, 17365, 486, 5876, 11, 412, 264, 588, 1935, 11, 15577, 300, 366, 2531, 281, 2570, 21968, 1957, 11, 4098, 412, 1014, 6913, 8432, 13, 51294], "temperature": 0.0, "avg_logprob": -0.23750834805624826, "compression_ratio": 1.4725274725274726, "no_speech_prob": 0.00045119610149413347}, {"id": 75, "seek": 64476, "start": 645.56, "end": 665.56, "text": " And we have used this hidden Markov model method, as well as with machine learning, to identify novel thermostable PET hydrolysis. And this paper was recently published in Nature Communications. So I will talk about some of the work that we've done here, as well as ongoing work that's not yet published.", "tokens": [50404, 400, 321, 362, 1143, 341, 7633, 3934, 5179, 2316, 3170, 11, 382, 731, 382, 365, 3479, 2539, 11, 281, 5876, 7613, 8810, 555, 712, 21968, 15435, 356, 17122, 13, 400, 341, 3035, 390, 3938, 6572, 294, 20159, 38998, 13, 407, 286, 486, 751, 466, 512, 295, 264, 589, 300, 321, 600, 1096, 510, 11, 382, 731, 382, 10452, 589, 300, 311, 406, 1939, 6572, 13, 51404], "temperature": 0.0, "avg_logprob": -0.3007555975430254, "compression_ratio": 1.4757281553398058, "no_speech_prob": 0.00031501540797762573}, {"id": 76, "seek": 66556, "start": 665.56, "end": 693.56, "text": " Our search methodology was to take the vast sequence database. We used NCBI non-redundant database and select thermal metagenomes from JGI. And we had about 220 million proteins that we're searching. And to search using hidden Markov models, as well as a support vector regression, a support vector classification model to predict thermostable PET hydrolysis.", "tokens": [50364, 2621, 3164, 24850, 390, 281, 747, 264, 8369, 8310, 8149, 13, 492, 1143, 20786, 11291, 2107, 12, 265, 769, 273, 394, 8149, 293, 3048, 15070, 1131, 4698, 18168, 490, 508, 26252, 13, 400, 321, 632, 466, 29387, 2459, 15577, 300, 321, 434, 10808, 13, 400, 281, 3164, 1228, 7633, 3934, 5179, 5245, 11, 382, 731, 382, 257, 1406, 8062, 1121, 735, 313, 11, 257, 1406, 8062, 21538, 2316, 281, 6069, 8810, 555, 712, 21968, 15435, 356, 17122, 13, 51764], "temperature": 0.0, "avg_logprob": -0.4284488864061309, "compression_ratio": 1.502092050209205, "no_speech_prob": 0.04669338837265968}, {"id": 77, "seek": 69356, "start": 693.56, "end": 718.56, "text": " And then to screen those and select for, or to screen those for PET hydrolysis activity. And we're particularly interested in high temperature proteins, because at higher temperature, closer to the glass transition temperature of PET, you have increased accessibility of the enzyme to the substrate.", "tokens": [50364, 400, 550, 281, 2568, 729, 293, 3048, 337, 11, 420, 281, 2568, 729, 337, 21968, 15435, 356, 17122, 5191, 13, 400, 321, 434, 4098, 3102, 294, 1090, 4292, 15577, 11, 570, 412, 2946, 4292, 11, 4966, 281, 264, 4276, 6034, 4292, 295, 21968, 11, 291, 362, 6505, 15002, 295, 264, 24521, 281, 264, 27585, 13, 51614], "temperature": 0.0, "avg_logprob": -0.3409374689651748, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.20669768750667572}, {"id": 78, "seek": 71856, "start": 719.56, "end": 728.56, "text": " So hypothetically, you would have improved performance or improved catalytic performance. So we wanted to identify proteins or enzymes that could withstand higher temperatures.", "tokens": [50414, 407, 24371, 22652, 11, 291, 576, 362, 9689, 3389, 420, 9689, 13192, 43658, 3389, 13, 407, 321, 1415, 281, 5876, 15577, 420, 29299, 300, 727, 31311, 2946, 12633, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2655251242897727, "compression_ratio": 1.3968253968253967, "no_speech_prob": 0.19917361438274384}, {"id": 79, "seek": 72856, "start": 728.56, "end": 744.56, "text": " So at the time I did this work, and I was a graduate student at the time, there were 17 known PET excesses. So I just called them PET 1 to 17, because I like life and I want to be organized.", "tokens": [50364, 407, 412, 264, 565, 286, 630, 341, 589, 11, 293, 286, 390, 257, 8080, 3107, 412, 264, 565, 11, 456, 645, 3282, 2570, 21968, 9310, 279, 13, 407, 286, 445, 1219, 552, 21968, 502, 281, 3282, 11, 570, 286, 411, 993, 293, 286, 528, 281, 312, 9983, 13, 51164], "temperature": 0.0, "avg_logprob": -0.26228614953848034, "compression_ratio": 1.3475177304964538, "no_speech_prob": 0.22252221405506134}, {"id": 80, "seek": 74456, "start": 745.56, "end": 758.56, "text": " And here you see an alignment of these 17. There are some of them, I'm just showing regions of an alignment. And there's some regions where you have either insertions, lots of insertions, because you have a loop somewhere in that region.", "tokens": [50414, 400, 510, 291, 536, 364, 18515, 295, 613, 3282, 13, 821, 366, 512, 295, 552, 11, 286, 478, 445, 4099, 10682, 295, 364, 18515, 13, 400, 456, 311, 512, 10682, 689, 291, 362, 2139, 8969, 626, 11, 3195, 295, 8969, 626, 11, 570, 291, 362, 257, 6367, 4079, 294, 300, 4458, 13, 51064], "temperature": 0.0, "avg_logprob": -0.24173147337777273, "compression_ratio": 1.6344827586206896, "no_speech_prob": 0.5574122071266174}, {"id": 81, "seek": 75856, "start": 758.56, "end": 775.56, "text": " But overall, despite the fact that you have evolutionary distance sequences, some regions align specifically well. And we took special time in curating the alignment. So we used, I tried different alignment methods, and I picked the alignment methods that I thought had the best,", "tokens": [50364, 583, 4787, 11, 7228, 264, 1186, 300, 291, 362, 27567, 4560, 22978, 11, 512, 10682, 7975, 4682, 731, 13, 400, 321, 1890, 2121, 565, 294, 1262, 990, 264, 18515, 13, 407, 321, 1143, 11, 286, 3031, 819, 18515, 7150, 11, 293, 286, 6183, 264, 18515, 7150, 300, 286, 1194, 632, 264, 1151, 11, 51214], "temperature": 0.0, "avg_logprob": -0.3268849416212602, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.06456451117992401}, {"id": 82, "seek": 75856, "start": 778.56, "end": 785.56, "text": " the best performance in aligning, particularly the conserved residue. So the active site triad of the ion and holes and things like that.", "tokens": [51364, 264, 1151, 3389, 294, 419, 9676, 11, 4098, 264, 1014, 6913, 34799, 13, 407, 264, 4967, 3621, 1376, 345, 295, 264, 17437, 293, 8118, 293, 721, 411, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3268849416212602, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.06456451117992401}, {"id": 83, "seek": 78556, "start": 786.56, "end": 799.56, "text": " And we took this heated molecule model and then searched the sequence databases, about 250 million of them. And we returned, at the threshold that we set, we had about 3,500 hits.", "tokens": [50414, 400, 321, 1890, 341, 18806, 15582, 2316, 293, 550, 22961, 264, 8310, 22380, 11, 466, 11650, 2459, 295, 552, 13, 400, 321, 8752, 11, 412, 264, 14678, 300, 321, 992, 11, 321, 632, 466, 805, 11, 7526, 8664, 13, 51064], "temperature": 0.0, "avg_logprob": -0.31079755827438, "compression_ratio": 1.366412213740458, "no_speech_prob": 0.0053843967616558075}, {"id": 84, "seek": 79956, "start": 800.56, "end": 809.56, "text": " So the next thing was we wanted to select a smaller size of this hits that were predicted to be thermostable. And to predict thermostability,", "tokens": [50414, 407, 264, 958, 551, 390, 321, 1415, 281, 3048, 257, 4356, 2744, 295, 341, 8664, 300, 645, 19147, 281, 312, 8810, 555, 712, 13, 400, 281, 6069, 8810, 555, 2310, 11, 50864], "temperature": 0.0, "avg_logprob": -0.24183011824084866, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.33081844449043274}, {"id": 85, "seek": 79956, "start": 810.56, "end": 818.56, "text": " we're looking at how active is the enzyme at higher temperatures. A good measurement of that is the optimum temperature of activity.", "tokens": [50914, 321, 434, 1237, 412, 577, 4967, 307, 264, 24521, 412, 2946, 12633, 13, 316, 665, 13160, 295, 300, 307, 264, 39326, 4292, 295, 5191, 13, 51314], "temperature": 0.0, "avg_logprob": -0.24183011824084866, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.33081844449043274}, {"id": 86, "seek": 81856, "start": 819.56, "end": 830.56, "text": " That's you evaluate activity at different temperatures and at the temperature at which you have maximum activities, the optimum temperature of activity. And from the literature, we see that the optimum temperature of activity correlates with", "tokens": [50414, 663, 311, 291, 13059, 5191, 412, 819, 12633, 293, 412, 264, 4292, 412, 597, 291, 362, 6674, 5354, 11, 264, 39326, 4292, 295, 5191, 13, 400, 490, 264, 10394, 11, 321, 536, 300, 264, 39326, 4292, 295, 5191, 13983, 1024, 365, 50964], "temperature": 0.0, "avg_logprob": -0.26217174530029297, "compression_ratio": 2.044642857142857, "no_speech_prob": 0.10222510248422623}, {"id": 87, "seek": 81856, "start": 832.56, "end": 844.56, "text": " the organism growth temperature. So organisms that are thermophiles or grow in high temperature environments tend to have proteins that are thermostable and their enzymes have optimum activity at higher temperatures.", "tokens": [51064, 264, 24128, 4599, 4292, 13, 407, 22110, 300, 366, 8810, 5317, 4680, 420, 1852, 294, 1090, 4292, 12388, 3928, 281, 362, 15577, 300, 366, 8810, 555, 712, 293, 641, 29299, 362, 39326, 5191, 412, 2946, 12633, 13, 51664], "temperature": 0.0, "avg_logprob": -0.26217174530029297, "compression_ratio": 2.044642857142857, "no_speech_prob": 0.10222510248422623}, {"id": 88, "seek": 84456, "start": 844.56, "end": 855.56, "text": " And the absence of optimum temperature data, we decided to use optimal growth temperature of the organism as a proxy to predict thermostability of the proteins.", "tokens": [50364, 400, 264, 17145, 295, 39326, 4292, 1412, 11, 321, 3047, 281, 764, 16252, 4599, 4292, 295, 264, 24128, 382, 257, 29690, 281, 6069, 8810, 555, 2310, 295, 264, 15577, 13, 50914], "temperature": 0.0, "avg_logprob": -0.21991199917263454, "compression_ratio": 1.5413533834586466, "no_speech_prob": 0.0010160173987969756}, {"id": 89, "seek": 84456, "start": 859.56, "end": 863.56, "text": " And for sequence features, we decided to use", "tokens": [51114, 400, 337, 8310, 4122, 11, 321, 3047, 281, 764, 51314], "temperature": 0.0, "avg_logprob": -0.21991199917263454, "compression_ratio": 1.5413533834586466, "no_speech_prob": 0.0010160173987969756}, {"id": 90, "seek": 86356, "start": 864.56, "end": 874.56, "text": " the sequence rather than the structure to predict thermostability because we wanted to do this in a high throughput fashion. And I first did a sample screen of", "tokens": [50414, 264, 8310, 2831, 813, 264, 3877, 281, 6069, 8810, 555, 2310, 570, 321, 1415, 281, 360, 341, 294, 257, 1090, 44629, 6700, 13, 400, 286, 700, 630, 257, 6889, 2568, 295, 50914], "temperature": 0.0, "avg_logprob": -0.24777423181841451, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.025149768218398094}, {"id": 91, "seek": 86356, "start": 876.56, "end": 885.56, "text": " different features to see what features correlate with the activity or with the thermostability. And I'm showing here just a few", "tokens": [51014, 819, 4122, 281, 536, 437, 4122, 48742, 365, 264, 5191, 420, 365, 264, 8810, 555, 2310, 13, 400, 286, 478, 4099, 510, 445, 257, 1326, 51464], "temperature": 0.0, "avg_logprob": -0.24777423181841451, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.025149768218398094}, {"id": 92, "seek": 88556, "start": 886.56, "end": 896.56, "text": " sequence features. And interestingly, we found that the composition of the amino acids, particularly relative composition, so aspartate relative to glutamate and so on,", "tokens": [50414, 8310, 4122, 13, 400, 25873, 11, 321, 1352, 300, 264, 12686, 295, 264, 24674, 21667, 11, 4098, 4972, 12686, 11, 370, 382, 6971, 473, 4972, 281, 33249, 335, 473, 293, 370, 322, 11, 50914], "temperature": 0.0, "avg_logprob": -0.21593319668489344, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023575546219944954}, {"id": 93, "seek": 88556, "start": 897.56, "end": 909.56, "text": " predicts the thermostability and every additional feature beyond the composition only provides marginal improvement in the performance. For our data set, we retrieved", "tokens": [50964, 6069, 82, 264, 8810, 555, 2310, 293, 633, 4497, 4111, 4399, 264, 12686, 787, 6417, 16885, 10444, 294, 264, 3389, 13, 1171, 527, 1412, 992, 11, 321, 19817, 937, 51564], "temperature": 0.0, "avg_logprob": -0.21593319668489344, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0023575546219944954}, {"id": 94, "seek": 90956, "start": 910.56, "end": 926.56, "text": " proteins based on the environment of the organism. So we had 8,000 sacrophilic, mesophilic, thermophilic and hypothermophilic. And it's 8,000 because we wanted to have a balanced class. So we just took the smallest class and then discarded the proteins from the other class.", "tokens": [50414, 15577, 2361, 322, 264, 2823, 295, 264, 24128, 13, 407, 321, 632, 1649, 11, 1360, 4899, 11741, 388, 299, 11, 3813, 5317, 388, 299, 11, 8810, 5317, 388, 299, 293, 24371, 966, 5317, 388, 299, 13, 400, 309, 311, 1649, 11, 1360, 570, 321, 1415, 281, 362, 257, 13902, 1508, 13, 407, 321, 445, 1890, 264, 16998, 1508, 293, 550, 45469, 264, 15577, 490, 264, 661, 1508, 13, 51214], "temperature": 0.0, "avg_logprob": -0.24275559849209255, "compression_ratio": 1.611764705882353, "no_speech_prob": 0.0018097138963639736}, {"id": 95, "seek": 92656, "start": 927.56, "end": 942.56, "text": " And we looked at different methods for predicting thermostability, different classification methods. Is this a binary classifier? Is giving a protein, is this thermophilic or hypothermophilic or is it mesophilic?", "tokens": [50414, 400, 321, 2956, 412, 819, 7150, 337, 32884, 8810, 555, 2310, 11, 819, 21538, 7150, 13, 1119, 341, 257, 17434, 1508, 9902, 30, 1119, 2902, 257, 7944, 11, 307, 341, 8810, 5317, 388, 299, 420, 24371, 966, 5317, 388, 299, 420, 307, 309, 3813, 5317, 388, 299, 30, 51164], "temperature": 0.0, "avg_logprob": -0.24161028861999512, "compression_ratio": 1.5588235294117647, "no_speech_prob": 0.024403641000390053}, {"id": 96, "seek": 94256, "start": 943.56, "end": 963.56, "text": " And then we found that the support vector classifier compared to other methods, including k-nearest neighbor and random forest and so on, performed best. This is on GitHub currently. So it's a simple, easy to use code that can be applied to screening protein sequences.", "tokens": [50414, 400, 550, 321, 1352, 300, 264, 1406, 8062, 1508, 9902, 5347, 281, 661, 7150, 11, 3009, 350, 12, 716, 17363, 5987, 293, 4974, 6719, 293, 370, 322, 11, 10332, 1151, 13, 639, 307, 322, 23331, 4362, 13, 407, 309, 311, 257, 2199, 11, 1858, 281, 764, 3089, 300, 393, 312, 6456, 281, 17732, 7944, 22978, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2714282353719076, "compression_ratio": 1.4308510638297873, "no_speech_prob": 0.0993586853146553}, {"id": 97, "seek": 96356, "start": 964.56, "end": 984.56, "text": " And you also notice that the accuracy of the methods, since these are based on amino acid features, the accuracy sort of correlates with the length of the sequence. Because as the sequence length increases, you have more confidence in the dipeptide composition and so on, and the amino acid features.", "tokens": [50414, 400, 291, 611, 3449, 300, 264, 14170, 295, 264, 7150, 11, 1670, 613, 366, 2361, 322, 24674, 8258, 4122, 11, 264, 14170, 1333, 295, 13983, 1024, 365, 264, 4641, 295, 264, 8310, 13, 1436, 382, 264, 8310, 4641, 8637, 11, 291, 362, 544, 6687, 294, 264, 1026, 494, 662, 482, 12686, 293, 370, 322, 11, 293, 264, 24674, 8258, 4122, 13, 51414], "temperature": 0.0, "avg_logprob": -0.23542236915001502, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.49159228801727295}, {"id": 98, "seek": 98456, "start": 985.56, "end": 1003.56, "text": " This is the performance on a separate test data set, which the model did not see in training and validation. And overall, you see that there's about an 80% accuracy in predicting whether it's in one thermostability class or the other.", "tokens": [50414, 639, 307, 264, 3389, 322, 257, 4994, 1500, 1412, 992, 11, 597, 264, 2316, 630, 406, 536, 294, 3097, 293, 24071, 13, 400, 4787, 11, 291, 536, 300, 456, 311, 466, 364, 4688, 4, 14170, 294, 32884, 1968, 309, 311, 294, 472, 8810, 555, 2310, 1508, 420, 264, 661, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18193273191098813, "compression_ratio": 1.4355828220858895, "no_speech_prob": 0.32734930515289307}, {"id": 99, "seek": 100356, "start": 1004.56, "end": 1023.56, "text": " And going forward, we took the thermo approach of the thermostability prediction model and apply that to the 3,500 hits that we had. And most, as you'd expect, would be mesostable. There are only a few thermostable proteins in the databases.", "tokens": [50414, 400, 516, 2128, 11, 321, 1890, 264, 8810, 78, 3109, 295, 264, 8810, 555, 2310, 17630, 2316, 293, 3079, 300, 281, 264, 805, 11, 7526, 8664, 300, 321, 632, 13, 400, 881, 11, 382, 291, 1116, 2066, 11, 576, 312, 3813, 555, 712, 13, 821, 366, 787, 257, 1326, 8810, 555, 712, 15577, 294, 264, 22380, 13, 51364], "temperature": 0.0, "avg_logprob": -0.23108452656229989, "compression_ratio": 1.4606060606060607, "no_speech_prob": 0.5423600673675537}, {"id": 100, "seek": 102356, "start": 1024.56, "end": 1042.56, "text": " We narrowed this down to about 74. And then we screened the 74 in vitro for activity on PET. And we looked at different temperature and different pH to understand the diversity of the variability of the activity as a function of the experimental conditions.", "tokens": [50414, 492, 9432, 292, 341, 760, 281, 466, 28868, 13, 400, 550, 321, 2568, 292, 264, 28868, 294, 9467, 340, 337, 5191, 322, 21968, 13, 400, 321, 2956, 412, 819, 4292, 293, 819, 21677, 281, 1223, 264, 8811, 295, 264, 35709, 295, 264, 5191, 382, 257, 2445, 295, 264, 17069, 4487, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2531502463600852, "compression_ratio": 1.5670731707317074, "no_speech_prob": 0.2223479449748993}, {"id": 101, "seek": 104256, "start": 1043.56, "end": 1056.56, "text": " And the darker colors indicates greater activity, the lighter colors indicate low activity. And there's some enzymes like LCCI-CCG that were previously described that retained, that demonstrated really high activity.", "tokens": [50414, 400, 264, 12741, 4577, 16203, 5044, 5191, 11, 264, 11546, 4577, 13330, 2295, 5191, 13, 400, 456, 311, 512, 29299, 411, 441, 11717, 40, 12, 11717, 38, 300, 645, 8046, 7619, 300, 33438, 11, 300, 18772, 534, 1090, 5191, 13, 51064], "temperature": 0.0, "avg_logprob": -0.35409719293767755, "compression_ratio": 1.469387755102041, "no_speech_prob": 0.6685269474983215}, {"id": 102, "seek": 105656, "start": 1057.56, "end": 1082.56, "text": " And the enzymes that we found, we named them according to the phylogenetic clades that they fall into. There's 101 belonging in clade 1 and 2, 1 in clade 2, and so on. And you see that most of these are of low activity than the canonical LCCI-CCG, but some of them, particularly those in group 7, at specific temperature regions demonstrate similar activity.", "tokens": [50414, 400, 264, 29299, 300, 321, 1352, 11, 321, 4926, 552, 4650, 281, 264, 903, 88, 4987, 268, 3532, 596, 2977, 300, 436, 2100, 666, 13, 821, 311, 21055, 22957, 294, 596, 762, 502, 293, 568, 11, 502, 294, 596, 762, 568, 11, 293, 370, 322, 13, 400, 291, 536, 300, 881, 295, 613, 366, 295, 2295, 5191, 813, 264, 46491, 441, 11717, 40, 12, 11717, 38, 11, 457, 512, 295, 552, 11, 4098, 729, 294, 1594, 1614, 11, 412, 2685, 4292, 10682, 11698, 2531, 5191, 13, 51664], "temperature": 0.0, "avg_logprob": -0.23715758853488497, "compression_ratio": 1.536480686695279, "no_speech_prob": 0.15598085522651672}, {"id": 103, "seek": 108256, "start": 1083.56, "end": 1098.56, "text": " On the top left, I show a phylogenetic tree of these 74 enzymes. And we started with 220 million, of course. I say 250 million before, and I was rounding up.", "tokens": [50414, 1282, 264, 1192, 1411, 11, 286, 855, 257, 903, 88, 4987, 268, 3532, 4230, 295, 613, 28868, 29299, 13, 400, 321, 1409, 365, 29387, 2459, 11, 295, 1164, 13, 286, 584, 11650, 2459, 949, 11, 293, 286, 390, 48237, 493, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21767231675445056, "compression_ratio": 1.394904458598726, "no_speech_prob": 0.15809649229049683}, {"id": 104, "seek": 108256, "start": 1098.56, "end": 1104.56, "text": " It's nice to say quarter of a billion instead of 220 million.", "tokens": [51164, 467, 311, 1481, 281, 584, 6555, 295, 257, 5218, 2602, 295, 29387, 2459, 13, 51464], "temperature": 0.0, "avg_logprob": -0.21767231675445056, "compression_ratio": 1.394904458598726, "no_speech_prob": 0.15809649229049683}, {"id": 105, "seek": 110456, "start": 1105.56, "end": 1121.56, "text": " So with the HMM, we narrowed that down to 3,500 PET hydrolases. The thermostability filter, we found 74 that were predicted to become a stable, we expressed 52 of them could be expressed and that we assayed them.", "tokens": [50414, 407, 365, 264, 389, 17365, 11, 321, 9432, 292, 300, 760, 281, 805, 11, 7526, 21968, 5796, 6623, 1957, 13, 440, 8810, 555, 2310, 6608, 11, 321, 1352, 28868, 300, 645, 19147, 281, 1813, 257, 8351, 11, 321, 12675, 18079, 295, 552, 727, 312, 12675, 293, 300, 321, 1256, 47315, 552, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3024571282523019, "compression_ratio": 1.367741935483871, "no_speech_prob": 0.6183995604515076}, {"id": 106, "seek": 112156, "start": 1122.56, "end": 1136.56, "text": " And 38 of them were active on PET. Interestingly, 24 of these were novel, never presented before. Some of these were in completely different clusters from what's mostly known to be PET-exists.", "tokens": [50414, 400, 12843, 295, 552, 645, 4967, 322, 21968, 13, 30564, 11, 4022, 295, 613, 645, 7613, 11, 1128, 8212, 949, 13, 2188, 295, 613, 645, 294, 2584, 819, 23313, 490, 437, 311, 5240, 2570, 281, 312, 21968, 12, 3121, 1751, 13, 51114], "temperature": 0.0, "avg_logprob": -0.33332706027560766, "compression_ratio": 1.4979757085020242, "no_speech_prob": 0.26866310834884644}, {"id": 107, "seek": 112156, "start": 1136.56, "end": 1148.56, "text": " Most PET-exists fall in this group as polyesterase liposcutinase, according to the ESTA database classification. And we've obtained a patent for these 24 PET hydrolases as well.", "tokens": [51114, 4534, 21968, 12, 3121, 1751, 2100, 294, 341, 1594, 382, 6754, 3011, 651, 8280, 329, 6672, 259, 651, 11, 4650, 281, 264, 462, 28075, 8149, 21538, 13, 400, 321, 600, 14879, 257, 20495, 337, 613, 4022, 21968, 5796, 6623, 1957, 382, 731, 13, 51714], "temperature": 0.0, "avg_logprob": -0.33332706027560766, "compression_ratio": 1.4979757085020242, "no_speech_prob": 0.26866310834884644}, {"id": 108, "seek": 114856, "start": 1149.56, "end": 1167.56, "text": " I wanted to also look at how the hidden Markov model alignment scores correlate with activity. And to discover the PET-exists, we were aligning unknown sequences or sequences with unknown function to the PET hydrolase HMM, and we select those with high HMM scores.", "tokens": [50414, 286, 1415, 281, 611, 574, 412, 577, 264, 7633, 3934, 5179, 2316, 18515, 13444, 48742, 365, 5191, 13, 400, 281, 4411, 264, 21968, 12, 3121, 1751, 11, 321, 645, 419, 9676, 9841, 22978, 420, 22978, 365, 9841, 2445, 281, 264, 21968, 5796, 6623, 651, 389, 17365, 11, 293, 321, 3048, 729, 365, 1090, 389, 17365, 13444, 13, 51314], "temperature": 0.0, "avg_logprob": -0.25749281586193645, "compression_ratio": 1.4748603351955307, "no_speech_prob": 0.009854953736066818}, {"id": 109, "seek": 116756, "start": 1168.56, "end": 1180.56, "text": " And you see that there is generally some weak correlation between the hidden Markov model scores and measured activity. On the left, I'm just showing the specific hidden Markov model score.", "tokens": [50414, 400, 291, 536, 300, 456, 307, 5101, 512, 5336, 20009, 1296, 264, 7633, 3934, 5179, 2316, 13444, 293, 12690, 5191, 13, 1282, 264, 1411, 11, 286, 478, 445, 4099, 264, 2685, 7633, 3934, 5179, 2316, 6175, 13, 51014], "temperature": 0.0, "avg_logprob": -0.22091130512516674, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.2505810856819153}, {"id": 110, "seek": 118056, "start": 1180.56, "end": 1198.56, "text": " On the right, the golden scatter plots are the difference between an alignment with a hidden Markov model of known PET-exists versus a hidden Markov model of enzymes that are of similar sequence homology, but are validated to not be PET-exists.", "tokens": [50364, 1282, 264, 558, 11, 264, 9729, 34951, 28609, 366, 264, 2649, 1296, 364, 18515, 365, 257, 7633, 3934, 5179, 2316, 295, 2570, 21968, 12, 3121, 1751, 5717, 257, 7633, 3934, 5179, 2316, 295, 29299, 300, 366, 295, 2531, 8310, 3655, 1793, 11, 457, 366, 40693, 281, 406, 312, 21968, 12, 3121, 1751, 13, 51264], "temperature": 0.0, "avg_logprob": -0.215482795447634, "compression_ratio": 1.5061728395061729, "no_speech_prob": 0.323438435792923}, {"id": 111, "seek": 119856, "start": 1199.56, "end": 1220.56, "text": " And you also see that on a classification framework, discriminating between active PET-exists and inactive sequence homologues, the model does not do really well. If you're familiar with receiver operating characteristic curves, you know that curves that are closer to the dashed line indicate perfect performance.", "tokens": [50414, 400, 291, 611, 536, 300, 322, 257, 21538, 8388, 11, 20828, 990, 1296, 4967, 21968, 12, 3121, 1751, 293, 294, 12596, 8310, 3655, 1132, 1247, 11, 264, 2316, 775, 406, 360, 534, 731, 13, 759, 291, 434, 4963, 365, 20086, 7447, 16282, 19490, 11, 291, 458, 300, 19490, 300, 366, 4966, 281, 264, 8240, 292, 1622, 13330, 2176, 3389, 13, 51464], "temperature": 0.0, "avg_logprob": -0.23589059710502625, "compression_ratio": 1.5023923444976077, "no_speech_prob": 0.533056914806366}, {"id": 112, "seek": 122056, "start": 1220.56, "end": 1233.56, "text": " The straight line is sort of random performance. And then we get AUCs of about 0.58 versus random scores of 0.5. So that's almost only slightly better than random, if you think of that.", "tokens": [50364, 440, 2997, 1622, 307, 1333, 295, 4974, 3389, 13, 400, 550, 321, 483, 7171, 33290, 295, 466, 1958, 13, 20419, 5717, 4974, 13444, 295, 1958, 13, 20, 13, 407, 300, 311, 1920, 787, 4748, 1101, 813, 4974, 11, 498, 291, 519, 295, 300, 13, 51014], "temperature": 0.0, "avg_logprob": -0.27391350269317627, "compression_ratio": 1.3120567375886525, "no_speech_prob": 0.27124977111816406}, {"id": 113, "seek": 123356, "start": 1234.56, "end": 1253.56, "text": " And so the question going forward was, can we use deep learning or machine learning? Because the hidden Markov model does not predict in a supervised way, it doesn't learn what activity is or how the activity relates to between one sequence and the other. It's just looking at the alignment.", "tokens": [50414, 400, 370, 264, 1168, 516, 2128, 390, 11, 393, 321, 764, 2452, 2539, 420, 3479, 2539, 30, 1436, 264, 7633, 3934, 5179, 2316, 775, 406, 6069, 294, 257, 46533, 636, 11, 309, 1177, 380, 1466, 437, 5191, 307, 420, 577, 264, 5191, 16155, 281, 1296, 472, 8310, 293, 264, 661, 13, 467, 311, 445, 1237, 412, 264, 18515, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24246794079977368, "compression_ratio": 1.5, "no_speech_prob": 0.2171158343553543}, {"id": 114, "seek": 125356, "start": 1254.56, "end": 1272.56, "text": " I wanted to see if we could predict the specific activity or the relative activity of a protein sequence with deep learning. And the beautiful thing of deep learning, or machine learning in general, is you can learn, giving the right set of features, you can learn to predict specifically attributes of proteins.", "tokens": [50414, 286, 1415, 281, 536, 498, 321, 727, 6069, 264, 2685, 5191, 420, 264, 4972, 5191, 295, 257, 7944, 8310, 365, 2452, 2539, 13, 400, 264, 2238, 551, 295, 2452, 2539, 11, 420, 3479, 2539, 294, 2674, 11, 307, 291, 393, 1466, 11, 2902, 264, 558, 992, 295, 4122, 11, 291, 393, 1466, 281, 6069, 4682, 17212, 295, 15577, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23636465224008713, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.19421201944351196}, {"id": 115, "seek": 127256, "start": 1272.56, "end": 1294.56, "text": " For data sets, I began with going through the literature and pulling out data of experimentally measured PET hydrolysis. And this went from 28 studies, totaling about 449 PET hydrolysis.", "tokens": [50364, 1171, 1412, 6352, 11, 286, 4283, 365, 516, 807, 264, 10394, 293, 8407, 484, 1412, 295, 5120, 379, 12690, 21968, 15435, 356, 17122, 13, 400, 341, 1437, 490, 7562, 5313, 11, 3217, 278, 466, 1017, 14938, 21968, 15435, 356, 17122, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2819011476304796, "compression_ratio": 1.3098591549295775, "no_speech_prob": 0.3483910858631134}, {"id": 116, "seek": 129456, "start": 1295.56, "end": 1314.56, "text": " I should point out that these are both from naturals, where you have just wild type enzymes from the natural sequence landscape, as well as singles. So you take a particular enzyme and then make a single mutation. So maybe I position 100 mutated adenine to glycine or something like that.", "tokens": [50414, 286, 820, 935, 484, 300, 613, 366, 1293, 490, 26389, 1124, 11, 689, 291, 362, 445, 4868, 2010, 29299, 490, 264, 3303, 8310, 9661, 11, 382, 731, 382, 36334, 13, 407, 291, 747, 257, 1729, 24521, 293, 550, 652, 257, 2167, 27960, 13, 407, 1310, 286, 2535, 2319, 5839, 770, 614, 268, 533, 281, 22633, 66, 533, 420, 746, 411, 300, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24473080490574692, "compression_ratio": 1.5319148936170213, "no_speech_prob": 0.7629381418228149}, {"id": 117, "seek": 131456, "start": 1314.56, "end": 1329.56, "text": " And then multiples, where you have multiple mutations, ranging from 2 to 21 mutations. And this, there are about 514 activity measurements for this 449 proteins. So you have multiple measurements for some proteins.", "tokens": [50364, 400, 550, 46099, 11, 689, 291, 362, 3866, 29243, 11, 25532, 490, 568, 281, 5080, 29243, 13, 400, 341, 11, 456, 366, 466, 1025, 7271, 5191, 15383, 337, 341, 1017, 14938, 15577, 13, 407, 291, 362, 3866, 15383, 337, 512, 15577, 13, 51114], "temperature": 0.0, "avg_logprob": -0.24203064130700153, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.3556153476238251}, {"id": 118, "seek": 132956, "start": 1329.56, "end": 1347.56, "text": " And I wanted to see how machine learning can sort of learn from this data set to predict PET activity. The natural proteins shown here, you have sequence identity ranging from somewhere about 10% to 99% as well.", "tokens": [50364, 400, 286, 1415, 281, 536, 577, 3479, 2539, 393, 1333, 295, 1466, 490, 341, 1412, 992, 281, 6069, 21968, 5191, 13, 440, 3303, 15577, 4898, 510, 11, 291, 362, 8310, 6575, 25532, 490, 4079, 466, 1266, 4, 281, 11803, 4, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22556692488650057, "compression_ratio": 1.3270440251572326, "no_speech_prob": 0.12931902706623077}, {"id": 119, "seek": 134756, "start": 1348.56, "end": 1363.56, "text": " And I should also point out that the conditions at which these data were generated vary. The PET substrate that was used varies, the temperature on the page varies, and you would expect that this will affect the measurements.", "tokens": [50414, 400, 286, 820, 611, 935, 484, 300, 264, 4487, 412, 597, 613, 1412, 645, 10833, 10559, 13, 440, 21968, 27585, 300, 390, 1143, 21716, 11, 264, 4292, 322, 264, 3028, 21716, 11, 293, 291, 576, 2066, 300, 341, 486, 3345, 264, 15383, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2511385653881317, "compression_ratio": 1.4610389610389611, "no_speech_prob": 0.3553347587585449}, {"id": 120, "seek": 136356, "start": 1364.56, "end": 1378.56, "text": " And so that would prevent comparison between one data set and the other. So if you do direct regression, so you take the first data set and you train a model on that, you predict the exact PET hydrolysis activity that's measured at that conditions.", "tokens": [50414, 400, 370, 300, 576, 4871, 9660, 1296, 472, 1412, 992, 293, 264, 661, 13, 407, 498, 291, 360, 2047, 24590, 11, 370, 291, 747, 264, 700, 1412, 992, 293, 291, 3847, 257, 2316, 322, 300, 11, 291, 6069, 264, 1900, 21968, 15435, 356, 17122, 5191, 300, 311, 12690, 412, 300, 4487, 13, 51114], "temperature": 0.0, "avg_logprob": -0.24448296962640223, "compression_ratio": 1.6570048309178744, "no_speech_prob": 0.26251012086868286}, {"id": 121, "seek": 136356, "start": 1379.56, "end": 1382.56, "text": " Moving to another data set, you have a different condition, so you can't do direct regression.", "tokens": [51164, 14242, 281, 1071, 1412, 992, 11, 291, 362, 257, 819, 4188, 11, 370, 291, 393, 380, 360, 2047, 24590, 13, 51314], "temperature": 0.0, "avg_logprob": -0.24448296962640223, "compression_ratio": 1.6570048309178744, "no_speech_prob": 0.26251012086868286}, {"id": 122, "seek": 138256, "start": 1383.56, "end": 1398.56, "text": " To overcome this limitation, the limitation that the disparate activity measurements and disparate conditions present, we introduced a strategy which learns to rank pairs from each data set.", "tokens": [50414, 1407, 10473, 341, 27432, 11, 264, 27432, 300, 264, 14548, 473, 5191, 15383, 293, 14548, 473, 4487, 1974, 11, 321, 7268, 257, 5206, 597, 27152, 281, 6181, 15494, 490, 1184, 1412, 992, 13, 51164], "temperature": 0.0, "avg_logprob": -0.26965978983286265, "compression_ratio": 1.4728682170542635, "no_speech_prob": 0.03408133611083031}, {"id": 123, "seek": 139856, "start": 1399.56, "end": 1409.56, "text": " So I'm showing a practical example. So you have a study with different sequences, X1 to X4, and the measured activity at specific conditions of that study.", "tokens": [50414, 407, 286, 478, 4099, 257, 8496, 1365, 13, 407, 291, 362, 257, 2979, 365, 819, 22978, 11, 1783, 16, 281, 1783, 19, 11, 293, 264, 12690, 5191, 412, 2685, 4487, 295, 300, 2979, 13, 50914], "temperature": 0.0, "avg_logprob": -0.292019759907442, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.1774805784225464}, {"id": 124, "seek": 139856, "start": 1410.56, "end": 1418.56, "text": " And then you have another study that takes another group of PETIs and measures activity, and the activity values are different because of different conditions.", "tokens": [50964, 400, 550, 291, 362, 1071, 2979, 300, 2516, 1071, 1594, 295, 21968, 6802, 293, 8000, 5191, 11, 293, 264, 5191, 4190, 366, 819, 570, 295, 819, 4487, 13, 51364], "temperature": 0.0, "avg_logprob": -0.292019759907442, "compression_ratio": 1.6321243523316062, "no_speech_prob": 0.1774805784225464}, {"id": 125, "seek": 141856, "start": 1418.56, "end": 1425.56, "text": " What we do instead is we generate pairs. So from the first study, we generate all possible binary pairs.", "tokens": [50364, 708, 321, 360, 2602, 307, 321, 8460, 15494, 13, 407, 490, 264, 700, 2979, 11, 321, 8460, 439, 1944, 17434, 15494, 13, 50714], "temperature": 0.0, "avg_logprob": -0.25086795419886493, "compression_ratio": 1.6310160427807487, "no_speech_prob": 0.03671586886048317}, {"id": 126, "seek": 141856, "start": 1426.56, "end": 1433.56, "text": " And then we predict the class, convert the regression raw values to classification tasks.", "tokens": [50764, 400, 550, 321, 6069, 264, 1508, 11, 7620, 264, 24590, 8936, 4190, 281, 21538, 9608, 13, 51114], "temperature": 0.0, "avg_logprob": -0.25086795419886493, "compression_ratio": 1.6310160427807487, "no_speech_prob": 0.03671586886048317}, {"id": 127, "seek": 141856, "start": 1434.56, "end": 1441.56, "text": " And we ask the question, giving a pair of sequences, is the first sequence of better activity than the second?", "tokens": [51164, 400, 321, 1029, 264, 1168, 11, 2902, 257, 6119, 295, 22978, 11, 307, 264, 700, 8310, 295, 1101, 5191, 813, 264, 1150, 30, 51514], "temperature": 0.0, "avg_logprob": -0.25086795419886493, "compression_ratio": 1.6310160427807487, "no_speech_prob": 0.03671586886048317}, {"id": 128, "seek": 144156, "start": 1441.56, "end": 1445.56, "text": " And this way we do away with trying to predict the exact activity.", "tokens": [50364, 400, 341, 636, 321, 360, 1314, 365, 1382, 281, 6069, 264, 1900, 5191, 13, 50564], "temperature": 0.0, "avg_logprob": -0.27349132602497683, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.041422612965106964}, {"id": 129, "seek": 144156, "start": 1446.56, "end": 1458.56, "text": " And also the model learning across all data sets, because we combine all of these data sets together, the model learns features that generally relate to PET hydrolysis activity across all the data sets.", "tokens": [50614, 400, 611, 264, 2316, 2539, 2108, 439, 1412, 6352, 11, 570, 321, 10432, 439, 295, 613, 1412, 6352, 1214, 11, 264, 2316, 27152, 4122, 300, 5101, 10961, 281, 21968, 15435, 356, 17122, 5191, 2108, 439, 264, 1412, 6352, 13, 51214], "temperature": 0.0, "avg_logprob": -0.27349132602497683, "compression_ratio": 1.6011904761904763, "no_speech_prob": 0.041422612965106964}, {"id": 130, "seek": 145856, "start": 1458.56, "end": 1471.56, "text": " Going back, we see the different conditions, and we really can't predict PET hydrolysis activity on powder versus PET hydrolysis activity on, say, nanoparticles versus the activity at a specific pH.", "tokens": [50364, 10963, 646, 11, 321, 536, 264, 819, 4487, 11, 293, 321, 534, 393, 380, 6069, 21968, 15435, 356, 17122, 5191, 322, 6341, 5717, 21968, 15435, 356, 17122, 5191, 322, 11, 584, 11, 14067, 404, 446, 5350, 5717, 264, 5191, 412, 257, 2685, 21677, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1995760064376028, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.040824659168720245}, {"id": 131, "seek": 145856, "start": 1472.56, "end": 1482.56, "text": " It will be nice to do that. Ideally, if you had a model that could tell you this is how PETases will work at a specific condition, but we can't do that because we're limited by the data.", "tokens": [51064, 467, 486, 312, 1481, 281, 360, 300, 13, 40817, 11, 498, 291, 632, 257, 2316, 300, 727, 980, 291, 341, 307, 577, 21968, 1957, 486, 589, 412, 257, 2685, 4188, 11, 457, 321, 393, 380, 360, 300, 570, 321, 434, 5567, 538, 264, 1412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1995760064376028, "compression_ratio": 1.6812227074235808, "no_speech_prob": 0.040824659168720245}, {"id": 132, "seek": 148256, "start": 1483.56, "end": 1495.56, "text": " But by combining the data together and learning to rank across the data sets, we can learn potentially features that generally correlate with PET hydrolysis activity.", "tokens": [50414, 583, 538, 21928, 264, 1412, 1214, 293, 2539, 281, 6181, 2108, 264, 1412, 6352, 11, 321, 393, 1466, 7263, 4122, 300, 5101, 48742, 365, 21968, 15435, 356, 17122, 5191, 13, 51014], "temperature": 0.0, "avg_logprob": -0.21394006032792348, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.02367336116731167}, {"id": 133, "seek": 148256, "start": 1496.56, "end": 1506.56, "text": " And then for the prediction, what we do is we take each sequence in the pair, and then we generate features for that sequence.", "tokens": [51064, 400, 550, 337, 264, 17630, 11, 437, 321, 360, 307, 321, 747, 1184, 8310, 294, 264, 6119, 11, 293, 550, 321, 8460, 4122, 337, 300, 8310, 13, 51564], "temperature": 0.0, "avg_logprob": -0.21394006032792348, "compression_ratio": 1.5837837837837838, "no_speech_prob": 0.02367336116731167}, {"id": 134, "seek": 150656, "start": 1506.56, "end": 1517.56, "text": " So the sequence represented as X, the features as Z, and your features can come from an unsupervised model, which I'll show in the next slide, or it can just be a simple one-hot encoding of the sequence.", "tokens": [50364, 407, 264, 8310, 10379, 382, 1783, 11, 264, 4122, 382, 1176, 11, 293, 428, 4122, 393, 808, 490, 364, 2693, 12879, 24420, 2316, 11, 597, 286, 603, 855, 294, 264, 958, 4137, 11, 420, 309, 393, 445, 312, 257, 2199, 472, 12, 12194, 43430, 295, 264, 8310, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22150691350301108, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.01882777363061905}, {"id": 135, "seek": 150656, "start": 1518.56, "end": 1525.56, "text": " And then we take the difference vector of the two representations, and then we fit a simple logistic regression to that difference vector to predict the rank.", "tokens": [50964, 400, 550, 321, 747, 264, 2649, 8062, 295, 264, 732, 33358, 11, 293, 550, 321, 3318, 257, 2199, 3565, 3142, 24590, 281, 300, 2649, 8062, 281, 6069, 264, 6181, 13, 51314], "temperature": 0.0, "avg_logprob": -0.22150691350301108, "compression_ratio": 1.699530516431925, "no_speech_prob": 0.01882777363061905}, {"id": 136, "seek": 152556, "start": 1526.56, "end": 1532.56, "text": " And to evaluate this method, I used leave one group out cross-validation.", "tokens": [50414, 400, 281, 13059, 341, 3170, 11, 286, 1143, 1856, 472, 1594, 484, 3278, 12, 3337, 327, 399, 13, 50714], "temperature": 0.0, "avg_logprob": -0.21565778067942415, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.05105108767747879}, {"id": 137, "seek": 152556, "start": 1533.56, "end": 1539.56, "text": " So we took the 28 studies, and then we would train on all but one study and then test on another study.", "tokens": [50764, 407, 321, 1890, 264, 7562, 5313, 11, 293, 550, 321, 576, 3847, 322, 439, 457, 472, 2979, 293, 550, 1500, 322, 1071, 2979, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21565778067942415, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.05105108767747879}, {"id": 138, "seek": 152556, "start": 1540.56, "end": 1552.56, "text": " And since there are duplicates as well as high similarity between some of the sequences, we applied an 80% threshold and removed all sequences in the training set that share identity of up to 80%.", "tokens": [51114, 400, 1670, 456, 366, 17154, 1024, 382, 731, 382, 1090, 32194, 1296, 512, 295, 264, 22978, 11, 321, 6456, 364, 4688, 4, 14678, 293, 7261, 439, 22978, 294, 264, 3097, 992, 300, 2073, 6575, 295, 493, 281, 4688, 6856, 51714], "temperature": 0.0, "avg_logprob": -0.21565778067942415, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.05105108767747879}, {"id": 139, "seek": 155256, "start": 1553.56, "end": 1556.56, "text": " 80% or more with sequences in the test set.", "tokens": [50414, 4688, 4, 420, 544, 365, 22978, 294, 264, 1500, 992, 13, 50564], "temperature": 0.0, "avg_logprob": -0.25896713733673093, "compression_ratio": 1.3217391304347825, "no_speech_prob": 0.001674126018770039}, {"id": 140, "seek": 155256, "start": 1557.56, "end": 1566.56, "text": " And then we repeat this for every study until we've trained and tested on every single pair in the data set.", "tokens": [50614, 400, 550, 321, 7149, 341, 337, 633, 2979, 1826, 321, 600, 8895, 293, 8246, 322, 633, 2167, 6119, 294, 264, 1412, 992, 13, 51064], "temperature": 0.0, "avg_logprob": -0.25896713733673093, "compression_ratio": 1.3217391304347825, "no_speech_prob": 0.001674126018770039}, {"id": 141, "seek": 156656, "start": 1567.56, "end": 1574.56, "text": " And for features, because if you think about it, you want to have a way to represent your protein sequence.", "tokens": [50414, 400, 337, 4122, 11, 570, 498, 291, 519, 466, 309, 11, 291, 528, 281, 362, 257, 636, 281, 2906, 428, 7944, 8310, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23089599609375, "compression_ratio": 1.76, "no_speech_prob": 0.09129901975393295}, {"id": 142, "seek": 156656, "start": 1575.56, "end": 1581.56, "text": " And that is representation learning or how you represent your sequence is very important.", "tokens": [50814, 400, 300, 307, 10290, 2539, 420, 577, 291, 2906, 428, 8310, 307, 588, 1021, 13, 51114], "temperature": 0.0, "avg_logprob": -0.23089599609375, "compression_ratio": 1.76, "no_speech_prob": 0.09129901975393295}, {"id": 143, "seek": 156656, "start": 1582.56, "end": 1594.56, "text": " And one promising approach, at least in the literature, is to use semi-supervised methods to take unsupervised models that were trained to learn the language of proteins across the protein universe.", "tokens": [51164, 400, 472, 20257, 3109, 11, 412, 1935, 294, 264, 10394, 11, 307, 281, 764, 12909, 12, 48172, 24420, 7150, 281, 747, 2693, 12879, 24420, 5245, 300, 645, 8895, 281, 1466, 264, 2856, 295, 15577, 2108, 264, 7944, 6445, 13, 51764], "temperature": 0.0, "avg_logprob": -0.23089599609375, "compression_ratio": 1.76, "no_speech_prob": 0.09129901975393295}, {"id": 144, "seek": 159456, "start": 1595.56, "end": 1599.56, "text": " And then use the embeddings from these models.", "tokens": [50414, 400, 550, 764, 264, 12240, 29432, 490, 613, 5245, 13, 50614], "temperature": 0.0, "avg_logprob": -0.20839155637300932, "compression_ratio": 1.758974358974359, "no_speech_prob": 0.0011692788684740663}, {"id": 145, "seek": 159456, "start": 1600.56, "end": 1609.56, "text": " One example is you have autoregressive models which take a protein sequence and learn from all amino acids up to a point and predict the next amino acid.", "tokens": [50664, 1485, 1365, 307, 291, 362, 1476, 418, 3091, 488, 5245, 597, 747, 257, 7944, 8310, 293, 1466, 490, 439, 24674, 21667, 493, 281, 257, 935, 293, 6069, 264, 958, 24674, 8258, 13, 51114], "temperature": 0.0, "avg_logprob": -0.20839155637300932, "compression_ratio": 1.758974358974359, "no_speech_prob": 0.0011692788684740663}, {"id": 146, "seek": 159456, "start": 1610.56, "end": 1618.56, "text": " Then you have math language models that mask out an amino acid and learn the context of that mass amino acid and predict what should be there.", "tokens": [51164, 1396, 291, 362, 5221, 2856, 5245, 300, 6094, 484, 364, 24674, 8258, 293, 1466, 264, 4319, 295, 300, 2758, 24674, 8258, 293, 6069, 437, 820, 312, 456, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20839155637300932, "compression_ratio": 1.758974358974359, "no_speech_prob": 0.0011692788684740663}, {"id": 147, "seek": 161856, "start": 1618.56, "end": 1634.56, "text": " You also have models like directional autoencoders that are a bottleneck that take the input sequence and then compress them into a bottleneck latent space, represented as Z, and then learns to reconstruct the original input.", "tokens": [50364, 509, 611, 362, 5245, 411, 42242, 8399, 22660, 378, 433, 300, 366, 257, 44641, 547, 300, 747, 264, 4846, 8310, 293, 550, 14778, 552, 666, 257, 44641, 547, 48994, 1901, 11, 10379, 382, 1176, 11, 293, 550, 27152, 281, 31499, 264, 3380, 4846, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23900794982910156, "compression_ratio": 1.705, "no_speech_prob": 0.017431803047657013}, {"id": 148, "seek": 161856, "start": 1635.56, "end": 1643.56, "text": " And models like these have been shown to capture both structural as well as functional representations of proteins.", "tokens": [51214, 400, 5245, 411, 613, 362, 668, 4898, 281, 7983, 1293, 15067, 382, 731, 382, 11745, 33358, 295, 15577, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23900794982910156, "compression_ratio": 1.705, "no_speech_prob": 0.017431803047657013}, {"id": 149, "seek": 164356, "start": 1643.56, "end": 1657.56, "text": " This is from a paper in 2019 where they trained LSTMs on the protein universe, and they show that structural information as well as phylogenetic and even functional information is contained in their embeddings.", "tokens": [50364, 639, 307, 490, 257, 3035, 294, 6071, 689, 436, 8895, 441, 6840, 26386, 322, 264, 7944, 6445, 11, 293, 436, 855, 300, 15067, 1589, 382, 731, 382, 903, 88, 4987, 268, 3532, 293, 754, 11745, 1589, 307, 16212, 294, 641, 12240, 29432, 13, 51064], "temperature": 0.0, "avg_logprob": -0.22100495277567112, "compression_ratio": 1.390728476821192, "no_speech_prob": 0.008982500061392784}, {"id": 150, "seek": 165756, "start": 1658.56, "end": 1677.56, "text": " And so to predict perihydrase activity, I retrieved embeddings from several of these models, particularly using some of these models that have been demonstrated to have state-of-the-art performance in downstream tasks.", "tokens": [50414, 400, 370, 281, 6069, 680, 72, 3495, 16753, 651, 5191, 11, 286, 19817, 937, 12240, 29432, 490, 2940, 295, 613, 5245, 11, 4098, 1228, 512, 295, 613, 5245, 300, 362, 668, 18772, 281, 362, 1785, 12, 2670, 12, 3322, 12, 446, 3389, 294, 30621, 9608, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2859748649597168, "compression_ratio": 1.4533333333333334, "no_speech_prob": 0.15374566614627838}, {"id": 151, "seek": 167756, "start": 1678.56, "end": 1705.56, "text": " So we've got Unirep, an autoregressive LSTM, transformer models like Transception, autoregressive transformer, ASM, a convolution masked model called CARB, a mass model transformer, ProT5, ProGen2, and a variational autoencoder that learns from the multiple sequence alignments", "tokens": [50414, 407, 321, 600, 658, 1156, 621, 79, 11, 364, 1476, 418, 3091, 488, 441, 6840, 44, 11, 31782, 5245, 411, 6531, 7311, 11, 1476, 418, 3091, 488, 31782, 11, 7469, 44, 11, 257, 45216, 45249, 2316, 1219, 15939, 33, 11, 257, 2758, 2316, 31782, 11, 1705, 51, 20, 11, 1705, 26647, 17, 11, 293, 257, 3034, 1478, 8399, 22660, 19866, 300, 27152, 490, 264, 3866, 8310, 7975, 1117, 51764], "temperature": 0.0, "avg_logprob": -0.3185013665093316, "compression_ratio": 1.5136612021857923, "no_speech_prob": 0.3168196976184845}, {"id": 152, "seek": 170556, "start": 1706.56, "end": 1708.56, "text": " to predict the effect of mutations.", "tokens": [50414, 281, 6069, 264, 1802, 295, 29243, 13, 50514], "temperature": 0.0, "avg_logprob": -0.2164402946096952, "compression_ratio": 1.3821656050955413, "no_speech_prob": 0.08503749966621399}, {"id": 153, "seek": 170556, "start": 1709.56, "end": 1715.56, "text": " As well, I also trained a variational autoencoder only on 18,000 proteins that are similar to perihydrases.", "tokens": [50564, 1018, 731, 11, 286, 611, 8895, 257, 3034, 1478, 8399, 22660, 19866, 787, 322, 2443, 11, 1360, 15577, 300, 366, 2531, 281, 680, 72, 3495, 16753, 1957, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2164402946096952, "compression_ratio": 1.3821656050955413, "no_speech_prob": 0.08503749966621399}, {"id": 154, "seek": 170556, "start": 1716.56, "end": 1720.56, "text": " So this we retrieved from a jackhammer search with perihydrase sequences.", "tokens": [50914, 407, 341, 321, 19817, 937, 490, 257, 7109, 39985, 3164, 365, 680, 72, 3495, 16753, 651, 22978, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2164402946096952, "compression_ratio": 1.3821656050955413, "no_speech_prob": 0.08503749966621399}, {"id": 155, "seek": 172056, "start": 1720.56, "end": 1725.56, "text": " As you point out that all of these models except this two were trained on the protein universe.", "tokens": [50364, 1018, 291, 935, 484, 300, 439, 295, 613, 5245, 3993, 341, 732, 645, 8895, 322, 264, 7944, 6445, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2554539478186405, "compression_ratio": 1.6647058823529413, "no_speech_prob": 0.07464669644832611}, {"id": 156, "seek": 172056, "start": 1726.56, "end": 1739.56, "text": " So somewhere between 25 million to a billion proteins in the protein databases, whereas these two models were trained only on the protein homologues of perihydrases, about 18,000 of them.", "tokens": [50664, 407, 4079, 1296, 3552, 2459, 281, 257, 5218, 15577, 294, 264, 7944, 22380, 11, 9735, 613, 732, 5245, 645, 8895, 787, 322, 264, 7944, 3655, 1132, 1247, 295, 680, 72, 3495, 16753, 1957, 11, 466, 2443, 11, 1360, 295, 552, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2554539478186405, "compression_ratio": 1.6647058823529413, "no_speech_prob": 0.07464669644832611}, {"id": 157, "seek": 173956, "start": 1740.56, "end": 1749.56, "text": " And very interestingly, comparing the performance, on the left plot, I show the AUC distribution across the 28 studies.", "tokens": [50414, 400, 588, 25873, 11, 15763, 264, 3389, 11, 322, 264, 1411, 7542, 11, 286, 855, 264, 7171, 34, 7316, 2108, 264, 7562, 5313, 13, 50864], "temperature": 0.0, "avg_logprob": -0.2712376458304269, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.09641756862401962}, {"id": 158, "seek": 173956, "start": 1750.56, "end": 1766.56, "text": " And you see that the one-hot representation of the multiple sequence alignment performs better than all but one of these models, which you would expect that language models should learn much richer representations of the proteins.", "tokens": [50914, 400, 291, 536, 300, 264, 472, 12, 12194, 10290, 295, 264, 3866, 8310, 18515, 26213, 1101, 813, 439, 457, 472, 295, 613, 5245, 11, 597, 291, 576, 2066, 300, 2856, 5245, 820, 1466, 709, 29021, 33358, 295, 264, 15577, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2712376458304269, "compression_ratio": 1.6055045871559632, "no_speech_prob": 0.09641756862401962}, {"id": 159, "seek": 176656, "start": 1766.56, "end": 1770.56, "text": " But we see that one-hot encoding of the multiple sequence alignment is better.", "tokens": [50364, 583, 321, 536, 300, 472, 12, 12194, 43430, 295, 264, 3866, 8310, 18515, 307, 1101, 13, 50564], "temperature": 0.0, "avg_logprob": -0.2287168110886665, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.005553673021495342}, {"id": 160, "seek": 176656, "start": 1771.56, "end": 1780.56, "text": " And particularly when you split the data into singles, multiples, and naturals, you see that the performance varies across different methods for these.", "tokens": [50614, 400, 4098, 562, 291, 7472, 264, 1412, 666, 36334, 11, 46099, 11, 293, 26389, 1124, 11, 291, 536, 300, 264, 3389, 21716, 2108, 819, 7150, 337, 613, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2287168110886665, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.005553673021495342}, {"id": 161, "seek": 176656, "start": 1781.56, "end": 1787.56, "text": " We're mostly interested in naturals because we're going to be screening wild type sequences in the databases.", "tokens": [51114, 492, 434, 5240, 3102, 294, 26389, 1124, 570, 321, 434, 516, 281, 312, 17732, 4868, 2010, 22978, 294, 264, 22380, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2287168110886665, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.005553673021495342}, {"id": 162, "seek": 178756, "start": 1787.56, "end": 1796.56, "text": " And so the one-hot encoding has the best performance for naturals, which I think is interesting.", "tokens": [50364, 400, 370, 264, 472, 12, 12194, 43430, 575, 264, 1151, 3389, 337, 26389, 1124, 11, 597, 286, 519, 307, 1880, 13, 50814], "temperature": 0.0, "avg_logprob": -0.26322273586107336, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.027992088347673416}, {"id": 163, "seek": 178756, "start": 1797.56, "end": 1804.56, "text": " So it means that, or it indicates that the information from the multiple sequence alignment is particularly important.", "tokens": [50864, 407, 309, 1355, 300, 11, 420, 309, 16203, 300, 264, 1589, 490, 264, 3866, 8310, 18515, 307, 4098, 1021, 13, 51214], "temperature": 0.0, "avg_logprob": -0.26322273586107336, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.027992088347673416}, {"id": 164, "seek": 178756, "start": 1805.56, "end": 1812.56, "text": " And then encoding that and learning to predict Pettis activity directly from this one-hot encoding.", "tokens": [51264, 400, 550, 43430, 300, 293, 2539, 281, 6069, 430, 3093, 271, 5191, 3838, 490, 341, 472, 12, 12194, 43430, 13, 51614], "temperature": 0.0, "avg_logprob": -0.26322273586107336, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.027992088347673416}, {"id": 165, "seek": 181256, "start": 1812.56, "end": 1817.56, "text": " So the model literally asks the question, what residue is at this position?", "tokens": [50364, 407, 264, 2316, 3736, 8962, 264, 1168, 11, 437, 34799, 307, 412, 341, 2535, 30, 50614], "temperature": 0.0, "avg_logprob": -0.23287366784137228, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0004044273809995502}, {"id": 166, "seek": 181256, "start": 1818.56, "end": 1822.56, "text": " And it assigns weights to specific residues at each position.", "tokens": [50664, 400, 309, 6269, 82, 17443, 281, 2685, 13141, 1247, 412, 1184, 2535, 13, 50864], "temperature": 0.0, "avg_logprob": -0.23287366784137228, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0004044273809995502}, {"id": 167, "seek": 181256, "start": 1825.56, "end": 1828.56, "text": " I also looked at zero-shot prediction methods.", "tokens": [51014, 286, 611, 2956, 412, 4018, 12, 18402, 17630, 7150, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23287366784137228, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0004044273809995502}, {"id": 168, "seek": 181256, "start": 1829.56, "end": 1833.56, "text": " And if you're familiar with zero-shot, it's where you have no training data at all.", "tokens": [51214, 400, 498, 291, 434, 4963, 365, 4018, 12, 18402, 11, 309, 311, 689, 291, 362, 572, 3097, 1412, 412, 439, 13, 51414], "temperature": 0.0, "avg_logprob": -0.23287366784137228, "compression_ratio": 1.4806629834254144, "no_speech_prob": 0.0004044273809995502}, {"id": 169, "seek": 183356, "start": 1834.56, "end": 1841.56, "text": " And you're just taking a model and predicting the fitness or so of your protein sequence.", "tokens": [50414, 400, 291, 434, 445, 1940, 257, 2316, 293, 32884, 264, 15303, 420, 370, 295, 428, 7944, 8310, 13, 50764], "temperature": 0.0, "avg_logprob": -0.22027749674660818, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.0005702328053303063}, {"id": 170, "seek": 183356, "start": 1842.56, "end": 1855.56, "text": " And on the machine learning side or deep learning side, these models usually will assign a probability to a sequence given what the model has seen in the training set.", "tokens": [50814, 400, 322, 264, 3479, 2539, 1252, 420, 2452, 2539, 1252, 11, 613, 5245, 2673, 486, 6269, 257, 8482, 281, 257, 8310, 2212, 437, 264, 2316, 575, 1612, 294, 264, 3097, 992, 13, 51464], "temperature": 0.0, "avg_logprob": -0.22027749674660818, "compression_ratio": 1.5575757575757576, "no_speech_prob": 0.0005702328053303063}, {"id": 171, "seek": 185556, "start": 1856.56, "end": 1860.56, "text": " So think of it as you have an unsupervised method.", "tokens": [50414, 407, 519, 295, 309, 382, 291, 362, 364, 2693, 12879, 24420, 3170, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1970896822340945, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.008830027654767036}, {"id": 172, "seek": 185556, "start": 1861.56, "end": 1863.56, "text": " Your input is the protein sequence from the protein universe.", "tokens": [50664, 2260, 4846, 307, 264, 7944, 8310, 490, 264, 7944, 6445, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1970896822340945, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.008830027654767036}, {"id": 173, "seek": 185556, "start": 1864.56, "end": 1865.56, "text": " Your output is the protein sequence as well.", "tokens": [50814, 2260, 5598, 307, 264, 7944, 8310, 382, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1970896822340945, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.008830027654767036}, {"id": 174, "seek": 185556, "start": 1866.56, "end": 1873.56, "text": " And the model is either learning to reconstruct the inputs through a bottleneck or in another regressive fashion or a masked fashion.", "tokens": [50914, 400, 264, 2316, 307, 2139, 2539, 281, 31499, 264, 15743, 807, 257, 44641, 547, 420, 294, 1071, 1121, 22733, 6700, 420, 257, 45249, 6700, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1970896822340945, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.008830027654767036}, {"id": 175, "seek": 185556, "start": 1874.56, "end": 1882.56, "text": " And the probability that that protein is similar to anything it's seen in the training set is an indication of its fitness.", "tokens": [51314, 400, 264, 8482, 300, 300, 7944, 307, 2531, 281, 1340, 309, 311, 1612, 294, 264, 3097, 992, 307, 364, 18877, 295, 1080, 15303, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1970896822340945, "compression_ratio": 1.7584745762711864, "no_speech_prob": 0.008830027654767036}, {"id": 176, "seek": 188256, "start": 1883.56, "end": 1889.56, "text": " And you can also use zero-shot methods that are more based on just bioinformatics methods.", "tokens": [50414, 400, 291, 393, 611, 764, 4018, 12, 18402, 7150, 300, 366, 544, 2361, 322, 445, 12198, 37811, 30292, 7150, 13, 50714], "temperature": 0.0, "avg_logprob": -0.26097837769158994, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.000984805403277278}, {"id": 177, "seek": 188256, "start": 1890.56, "end": 1895.56, "text": " So sequence similarity, for example, with the BLOSUM matrix or Healy-Markov models.", "tokens": [50764, 407, 8310, 32194, 11, 337, 1365, 11, 365, 264, 15132, 4367, 14340, 8141, 420, 634, 5222, 12, 15168, 5179, 5245, 13, 51014], "temperature": 0.0, "avg_logprob": -0.26097837769158994, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.000984805403277278}, {"id": 178, "seek": 188256, "start": 1896.56, "end": 1910.56, "text": " And interestingly, you see that when you compare the BLOSUM similarity with one-pedis, IS-pedis or the canonical pedis, versus a BLOSUM similarity with a consensus sequence from the alignment, you get much better performance with consensus.", "tokens": [51064, 400, 25873, 11, 291, 536, 300, 562, 291, 6794, 264, 15132, 4367, 14340, 32194, 365, 472, 12, 3452, 271, 11, 6205, 12, 3452, 271, 420, 264, 46491, 5670, 271, 11, 5717, 257, 15132, 4367, 14340, 32194, 365, 257, 19115, 8310, 490, 264, 18515, 11, 291, 483, 709, 1101, 3389, 365, 19115, 13, 51764], "temperature": 0.0, "avg_logprob": -0.26097837769158994, "compression_ratio": 1.6733870967741935, "no_speech_prob": 0.000984805403277278}, {"id": 179, "seek": 191056, "start": 1911.56, "end": 1920.56, "text": " Indicating that learning from the alignment, what positions are important and what residues are particularly conserved in the alignment is important.", "tokens": [50414, 2333, 30541, 300, 2539, 490, 264, 18515, 11, 437, 8432, 366, 1021, 293, 437, 13141, 1247, 366, 4098, 1014, 6913, 294, 264, 18515, 307, 1021, 13, 50864], "temperature": 0.0, "avg_logprob": -0.27285069138256474, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.0008166477200575173}, {"id": 180, "seek": 191056, "start": 1921.56, "end": 1933.56, "text": " But generally, I know there's a lot of information here, but comparing the unsupervised methods, zero-shot, to the supervised or semi-supervised methods,", "tokens": [50914, 583, 5101, 11, 286, 458, 456, 311, 257, 688, 295, 1589, 510, 11, 457, 15763, 264, 2693, 12879, 24420, 7150, 11, 4018, 12, 18402, 11, 281, 264, 46533, 420, 12909, 12, 48172, 24420, 7150, 11, 51514], "temperature": 0.0, "avg_logprob": -0.27285069138256474, "compression_ratio": 1.6378378378378378, "no_speech_prob": 0.0008166477200575173}, {"id": 181, "seek": 193356, "start": 1934.56, "end": 1942.56, "text": " we see that the one-hot encoding still outperforms virtually all methods, particularly for predicting naturals.", "tokens": [50414, 321, 536, 300, 264, 472, 12, 12194, 43430, 920, 484, 26765, 82, 14103, 439, 7150, 11, 4098, 337, 32884, 26389, 1124, 13, 50814], "temperature": 0.0, "avg_logprob": -0.23388966427573674, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00912320613861084}, {"id": 182, "seek": 193356, "start": 1943.56, "end": 1958.56, "text": " The Healy-Markov model, although I previously showed that the correlation was weak with our data sets, compared to the entire data sets of the 20 data sets that we pulled out, the Healy-Markov model still shows particularly reasonable performance.", "tokens": [50864, 440, 634, 5222, 12, 15168, 5179, 2316, 11, 4878, 286, 8046, 4712, 300, 264, 20009, 390, 5336, 365, 527, 1412, 6352, 11, 5347, 281, 264, 2302, 1412, 6352, 295, 264, 945, 1412, 6352, 300, 321, 7373, 484, 11, 264, 634, 5222, 12, 15168, 5179, 2316, 920, 3110, 4098, 10585, 3389, 13, 51614], "temperature": 0.0, "avg_logprob": -0.23388966427573674, "compression_ratio": 1.6543778801843319, "no_speech_prob": 0.00912320613861084}, {"id": 183, "seek": 195856, "start": 1959.56, "end": 1965.56, "text": " And so we've made a model to predict pedis activity, which we call rank-pedis.", "tokens": [50414, 400, 370, 321, 600, 1027, 257, 2316, 281, 6069, 5670, 271, 5191, 11, 597, 321, 818, 6181, 12, 3452, 271, 13, 50714], "temperature": 0.0, "avg_logprob": -0.22879837299215383, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.026742050424218178}, {"id": 184, "seek": 195856, "start": 1966.56, "end": 1969.56, "text": " And it takes as input the protein sequence.", "tokens": [50764, 400, 309, 2516, 382, 4846, 264, 7944, 8310, 13, 50914], "temperature": 0.0, "avg_logprob": -0.22879837299215383, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.026742050424218178}, {"id": 185, "seek": 195856, "start": 1970.56, "end": 1976.56, "text": " And instead of an unsupervised model, we just align that sequence to a pedis alignment.", "tokens": [50964, 400, 2602, 295, 364, 2693, 12879, 24420, 2316, 11, 321, 445, 7975, 300, 8310, 281, 257, 5670, 271, 18515, 13, 51264], "temperature": 0.0, "avg_logprob": -0.22879837299215383, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.026742050424218178}, {"id": 186, "seek": 195856, "start": 1977.56, "end": 1985.56, "text": " And we take the positions in that alignment and then one-hot encode that and take the difference of the one-hot encoding.", "tokens": [51314, 400, 321, 747, 264, 8432, 294, 300, 18515, 293, 550, 472, 12, 12194, 2058, 1429, 300, 293, 747, 264, 2649, 295, 264, 472, 12, 12194, 43430, 13, 51714], "temperature": 0.0, "avg_logprob": -0.22879837299215383, "compression_ratio": 1.7202072538860103, "no_speech_prob": 0.026742050424218178}, {"id": 187, "seek": 198556, "start": 1985.56, "end": 1991.56, "text": " So if there's a residue at a position, it gets a one. If there's not a residue, it's a zero.", "tokens": [50364, 407, 498, 456, 311, 257, 34799, 412, 257, 2535, 11, 309, 2170, 257, 472, 13, 759, 456, 311, 406, 257, 34799, 11, 309, 311, 257, 4018, 13, 50664], "temperature": 0.0, "avg_logprob": -0.23724639415740967, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0005441769608296454}, {"id": 188, "seek": 198556, "start": 1992.56, "end": 1998.56, "text": " And when we take the difference, you have negative one or one, depending on where the position is.", "tokens": [50714, 400, 562, 321, 747, 264, 2649, 11, 291, 362, 3671, 472, 420, 472, 11, 5413, 322, 689, 264, 2535, 307, 13, 51014], "temperature": 0.0, "avg_logprob": -0.23724639415740967, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0005441769608296454}, {"id": 189, "seek": 198556, "start": 1999.56, "end": 2004.56, "text": " And we can predict the rank. Is this sequence a better pedis than some reference, say, IS pedis?", "tokens": [51064, 400, 321, 393, 6069, 264, 6181, 13, 1119, 341, 8310, 257, 1101, 5670, 271, 813, 512, 6408, 11, 584, 11, 6205, 5670, 271, 30, 51314], "temperature": 0.0, "avg_logprob": -0.23724639415740967, "compression_ratio": 1.6089385474860336, "no_speech_prob": 0.0005441769608296454}, {"id": 190, "seek": 200456, "start": 2005.56, "end": 2010.56, "text": " We can also screen with the profile Healy-Markov model and get a score for that.", "tokens": [50414, 492, 393, 611, 2568, 365, 264, 7964, 634, 5222, 12, 15168, 5179, 2316, 293, 483, 257, 6175, 337, 300, 13, 50664], "temperature": 0.0, "avg_logprob": -0.2293037043677436, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.02228066325187683}, {"id": 191, "seek": 200456, "start": 2011.56, "end": 2015.56, "text": " And then we average the scores and use that to screen the sequence database.", "tokens": [50714, 400, 550, 321, 4274, 264, 13444, 293, 764, 300, 281, 2568, 264, 8310, 8149, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2293037043677436, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.02228066325187683}, {"id": 192, "seek": 200456, "start": 2016.56, "end": 2028.56, "text": " And so currently what we're working on is to use these methods for improved screening and improved mining of pet hydrolysis from the sequence databases.", "tokens": [50964, 400, 370, 4362, 437, 321, 434, 1364, 322, 307, 281, 764, 613, 7150, 337, 9689, 17732, 293, 9689, 15512, 295, 3817, 15435, 356, 17122, 490, 264, 8310, 22380, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2293037043677436, "compression_ratio": 1.648936170212766, "no_speech_prob": 0.02228066325187683}, {"id": 193, "seek": 202856, "start": 2028.56, "end": 2034.56, "text": " And it's amazing how fast these databases grow. There are currently about 3 billion proteins in the databases.", "tokens": [50364, 400, 309, 311, 2243, 577, 2370, 613, 22380, 1852, 13, 821, 366, 4362, 466, 805, 5218, 15577, 294, 264, 22380, 13, 50664], "temperature": 0.0, "avg_logprob": -0.26180364820692276, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0011334804585203528}, {"id": 194, "seek": 202856, "start": 2035.56, "end": 2040.56, "text": " And we want to screen these and rank these, and then we'll iteratively search to identify pedis.", "tokens": [50714, 400, 321, 528, 281, 2568, 613, 293, 6181, 613, 11, 293, 550, 321, 603, 17138, 19020, 3164, 281, 5876, 5670, 271, 13, 50964], "temperature": 0.0, "avg_logprob": -0.26180364820692276, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0011334804585203528}, {"id": 195, "seek": 202856, "start": 2043.56, "end": 2057.56, "text": " I will acknowledge my supervisor, principal investigator Greg Beckham, as well as collaborators at Harvard, Deborah Marks and Chris Sander, and postdocs and graduate students who have collaborated with me.", "tokens": [51114, 286, 486, 10692, 452, 24610, 11, 9716, 38330, 11490, 19184, 4822, 11, 382, 731, 382, 39789, 412, 13378, 11, 39695, 3934, 82, 293, 6688, 318, 4483, 11, 293, 2183, 39966, 82, 293, 8080, 1731, 567, 362, 42463, 365, 385, 13, 51814], "temperature": 0.0, "avg_logprob": -0.26180364820692276, "compression_ratio": 1.541044776119403, "no_speech_prob": 0.0011334804585203528}, {"id": 196, "seek": 205856, "start": 2058.56, "end": 2060.56, "text": " Erica Erickson, Courtney, Nikki, and Ada.", "tokens": [50364, 37429, 3300, 618, 3015, 11, 33489, 11, 37907, 11, 293, 32276, 13, 50464], "temperature": 0.0, "avg_logprob": -0.35877020018441336, "compression_ratio": 1.011111111111111, "no_speech_prob": 0.0653870478272438}, {"id": 197, "seek": 205856, "start": 2061.56, "end": 2064.56, "text": " Thank you for listening, and I'll take questions.", "tokens": [50514, 1044, 291, 337, 4764, 11, 293, 286, 603, 747, 1651, 13, 50664], "temperature": 0.0, "avg_logprob": -0.35877020018441336, "compression_ratio": 1.011111111111111, "no_speech_prob": 0.0653870478272438}, {"id": 198, "seek": 208856, "start": 2088.56, "end": 2101.56, "text": " One thing we could do is to test different alignments and see which alignments give the best performance.", "tokens": [50364, 1485, 551, 321, 727, 360, 307, 281, 1500, 819, 7975, 1117, 293, 536, 597, 7975, 1117, 976, 264, 1151, 3389, 13, 51014], "temperature": 0.0, "avg_logprob": -0.24291677474975587, "compression_ratio": 1.715736040609137, "no_speech_prob": 0.42545613646507263}, {"id": 199, "seek": 208856, "start": 2102.56, "end": 2105.56, "text": " But I think a structure-based alignment should work well.", "tokens": [51064, 583, 286, 519, 257, 3877, 12, 6032, 18515, 820, 589, 731, 13, 51214], "temperature": 0.0, "avg_logprob": -0.24291677474975587, "compression_ratio": 1.715736040609137, "no_speech_prob": 0.42545613646507263}, {"id": 200, "seek": 208856, "start": 2106.56, "end": 2115.56, "text": " And what we're doing is to take alpha-4 structures of all the proteins we want to align, and then align the structure first, and then use that to guide the alignment as well.", "tokens": [51264, 400, 437, 321, 434, 884, 307, 281, 747, 8961, 12, 19, 9227, 295, 439, 264, 15577, 321, 528, 281, 7975, 11, 293, 550, 7975, 264, 3877, 700, 11, 293, 550, 764, 300, 281, 5934, 264, 18515, 382, 731, 13, 51714], "temperature": 0.0, "avg_logprob": -0.24291677474975587, "compression_ratio": 1.715736040609137, "no_speech_prob": 0.42545613646507263}, {"id": 201, "seek": 211556, "start": 2115.56, "end": 2127.56, "text": " And the alignment will be wrong in some places, but what matters most is that it gets the most important positions correctly, the active site triad and conserved positions as well.", "tokens": [50414, 400, 264, 18515, 486, 312, 2085, 294, 512, 3190, 11, 457, 437, 7001, 881, 307, 300, 309, 2170, 264, 881, 1021, 8432, 8944, 11, 264, 4967, 3621, 1376, 345, 293, 1014, 6913, 8432, 382, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2388944381322616, "compression_ratio": 1.4173228346456692, "no_speech_prob": 0.012214944697916508}, {"id": 202, "seek": 214556, "start": 2146.56, "end": 2147.56, "text": " Yeah.", "tokens": [50414, 865, 13, 50464], "temperature": 0.0, "avg_logprob": -0.5246431827545166, "compression_ratio": 1.0454545454545454, "no_speech_prob": 0.2649573087692261}, {"id": 203, "seek": 214556, "start": 2167.56, "end": 2170.56, "text": " Local alignment versus global alignment.", "tokens": [51464, 22755, 18515, 5717, 4338, 18515, 13, 51614], "temperature": 0.0, "avg_logprob": -0.5246431827545166, "compression_ratio": 1.0454545454545454, "no_speech_prob": 0.2649573087692261}, {"id": 204, "seek": 217056, "start": 2171.56, "end": 2173.56, "text": " I think you're referring to...", "tokens": [50414, 286, 519, 291, 434, 13761, 281, 485, 50514], "temperature": 0.0, "avg_logprob": -0.495888258281507, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.0058168675750494}, {"id": 205, "seek": 217056, "start": 2176.56, "end": 2177.56, "text": " This.", "tokens": [50664, 639, 13, 50714], "temperature": 0.0, "avg_logprob": -0.495888258281507, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.0058168675750494}, {"id": 206, "seek": 217056, "start": 2177.56, "end": 2178.56, "text": " Yep.", "tokens": [50714, 7010, 13, 50764], "temperature": 0.0, "avg_logprob": -0.495888258281507, "compression_ratio": 0.8367346938775511, "no_speech_prob": 0.0058168675750494}, {"id": 207, "seek": 217856, "start": 2179.56, "end": 2185.56, "text": " I'm showing positions that align well, but this is just a segment of the alignment.", "tokens": [50414, 286, 478, 4099, 8432, 300, 7975, 731, 11, 457, 341, 307, 445, 257, 9469, 295, 264, 18515, 13, 50714], "temperature": 0.0, "avg_logprob": -0.32616539001464845, "compression_ratio": 1.6711409395973154, "no_speech_prob": 0.00857232604175806}, {"id": 208, "seek": 217856, "start": 2186.56, "end": 2191.56, "text": " There are some positions or some regions that don't align well at all in this.", "tokens": [50764, 821, 366, 512, 8432, 420, 512, 10682, 300, 500, 380, 7975, 731, 412, 439, 294, 341, 13, 51014], "temperature": 0.0, "avg_logprob": -0.32616539001464845, "compression_ratio": 1.6711409395973154, "no_speech_prob": 0.00857232604175806}, {"id": 209, "seek": 217856, "start": 2192.56, "end": 2195.56, "text": " And this is the 17 pedis at the time.", "tokens": [51064, 400, 341, 307, 264, 3282, 5670, 271, 412, 264, 565, 13, 51214], "temperature": 0.0, "avg_logprob": -0.32616539001464845, "compression_ratio": 1.6711409395973154, "no_speech_prob": 0.00857232604175806}, {"id": 210, "seek": 217856, "start": 2196.56, "end": 2198.56, "text": " The alignments get the most important positions.", "tokens": [51264, 440, 7975, 1117, 483, 264, 881, 1021, 8432, 13, 51364], "temperature": 0.0, "avg_logprob": -0.32616539001464845, "compression_ratio": 1.6711409395973154, "no_speech_prob": 0.00857232604175806}, {"id": 211, "seek": 219856, "start": 2199.56, "end": 2201.56, "text": " And I think that you need a multiple sequence alignment.", "tokens": [50414, 400, 286, 519, 300, 291, 643, 257, 3866, 8310, 18515, 13, 50514], "temperature": 0.0, "avg_logprob": -0.70406494140625, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08602042496204376}, {"id": 212, "seek": 219856, "start": 2202.56, "end": 2208.56, "text": " In this case, the sequence alignment that's fed into the HMM to capture the sequence that's not aligned well.", "tokens": [50564, 682, 341, 1389, 11, 264, 8310, 18515, 300, 311, 4636, 666, 264, 389, 17365, 281, 7983, 264, 8310, 300, 311, 406, 17962, 731, 13, 50864], "temperature": 0.0, "avg_logprob": -0.70406494140625, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08602042496204376}, {"id": 213, "seek": 219856, "start": 2209.56, "end": 2211.56, "text": " And I think that's what we're trying to do.", "tokens": [50914, 400, 286, 519, 300, 311, 437, 321, 434, 1382, 281, 360, 13, 51014], "temperature": 0.0, "avg_logprob": -0.70406494140625, "compression_ratio": 1.5789473684210527, "no_speech_prob": 0.08602042496204376}, {"id": 214, "seek": 221156, "start": 2211.56, "end": 2213.56, "text": " Currently, there are about 75 pedis.", "tokens": [50364, 19964, 11, 456, 366, 466, 9562, 5670, 271, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2266278156014376, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.07042188942432404}, {"id": 215, "seek": 221156, "start": 2214.56, "end": 2218.56, "text": " So aligning those, you have even greater regions that don't align well.", "tokens": [50514, 407, 419, 9676, 729, 11, 291, 362, 754, 5044, 10682, 300, 500, 380, 7975, 731, 13, 50714], "temperature": 0.0, "avg_logprob": -0.2266278156014376, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.07042188942432404}, {"id": 216, "seek": 221156, "start": 2219.56, "end": 2222.56, "text": " And I think that you need a multiple sequence alignment.", "tokens": [50764, 400, 286, 519, 300, 291, 643, 257, 3866, 8310, 18515, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2266278156014376, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.07042188942432404}, {"id": 217, "seek": 221156, "start": 2223.56, "end": 2233.56, "text": " In this case, the sequence alignment that's fed into the HMM to capture these conserved positions globally across all of the proteins that you're feeding in.", "tokens": [50964, 682, 341, 1389, 11, 264, 8310, 18515, 300, 311, 4636, 666, 264, 389, 17365, 281, 7983, 613, 1014, 6913, 8432, 18958, 2108, 439, 295, 264, 15577, 300, 291, 434, 12919, 294, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2266278156014376, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.07042188942432404}, {"id": 218, "seek": 221156, "start": 2234.56, "end": 2236.56, "text": " A local alignment will miss that.", "tokens": [51514, 316, 2654, 18515, 486, 1713, 300, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2266278156014376, "compression_ratio": 1.579646017699115, "no_speech_prob": 0.07042188942432404}, {"id": 219, "seek": 224156, "start": 2242.56, "end": 2243.56, "text": " Yeah.", "tokens": [50414, 865, 13, 50464], "temperature": 0.0, "avg_logprob": -0.2735212092496911, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.05259040743112564}, {"id": 220, "seek": 224156, "start": 2252.56, "end": 2256.56, "text": " Most language models take unaligned sequences.", "tokens": [50914, 4534, 2856, 5245, 747, 517, 304, 16690, 22978, 13, 51114], "temperature": 0.0, "avg_logprob": -0.2735212092496911, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.05259040743112564}, {"id": 221, "seek": 224156, "start": 2257.56, "end": 2260.56, "text": " Some language models take aligned sequences.", "tokens": [51164, 2188, 2856, 5245, 747, 17962, 22978, 13, 51314], "temperature": 0.0, "avg_logprob": -0.2735212092496911, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.05259040743112564}, {"id": 222, "seek": 224156, "start": 2263.56, "end": 2266.56, "text": " I was using the unaligned sequence for all but one.", "tokens": [51464, 286, 390, 1228, 264, 517, 304, 16690, 8310, 337, 439, 457, 472, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2735212092496911, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.05259040743112564}, {"id": 223, "seek": 224156, "start": 2267.56, "end": 2269.56, "text": " The ESMMSA, yes.", "tokens": [51664, 440, 12564, 17365, 8886, 11, 2086, 13, 51764], "temperature": 0.0, "avg_logprob": -0.2735212092496911, "compression_ratio": 1.5514018691588785, "no_speech_prob": 0.05259040743112564}, {"id": 224, "seek": 227156, "start": 2271.56, "end": 2272.56, "text": " I have a question.", "tokens": [50364, 286, 362, 257, 1168, 13, 50414], "temperature": 0.0, "avg_logprob": -0.49965911441379124, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.009556281380355358}, {"id": 225, "seek": 227156, "start": 2273.56, "end": 2275.56, "text": " I was wondering what's the size of your...", "tokens": [50464, 286, 390, 6359, 437, 311, 264, 2744, 295, 428, 485, 50564], "temperature": 0.0, "avg_logprob": -0.49965911441379124, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.009556281380355358}, {"id": 226, "seek": 227156, "start": 2277.56, "end": 2281.56, "text": " As compared to the input in your variational monitor.", "tokens": [50664, 1018, 5347, 281, 264, 4846, 294, 428, 3034, 1478, 6002, 13, 50864], "temperature": 0.0, "avg_logprob": -0.49965911441379124, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.009556281380355358}, {"id": 227, "seek": 227156, "start": 2284.56, "end": 2285.56, "text": " For...", "tokens": [51014, 1171, 485, 51064], "temperature": 0.0, "avg_logprob": -0.49965911441379124, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.009556281380355358}, {"id": 228, "seek": 227156, "start": 2286.56, "end": 2287.56, "text": " Yeah. For the...", "tokens": [51114, 865, 13, 1171, 264, 485, 51164], "temperature": 0.0, "avg_logprob": -0.49965911441379124, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.009556281380355358}, {"id": 229, "seek": 227156, "start": 2291.56, "end": 2293.56, "text": " Yes. So I did a...", "tokens": [51364, 1079, 13, 407, 286, 630, 257, 485, 51464], "temperature": 0.0, "avg_logprob": -0.49965911441379124, "compression_ratio": 1.2845528455284554, "no_speech_prob": 0.009556281380355358}, {"id": 230, "seek": 229356, "start": 2294.56, "end": 2298.56, "text": " For EAVE, EAVE is a pre-trained or a proposed model from a different paper.", "tokens": [50414, 1171, 35747, 7540, 11, 35747, 7540, 307, 257, 659, 12, 17227, 2001, 420, 257, 10348, 2316, 490, 257, 819, 3035, 13, 50614], "temperature": 0.0, "avg_logprob": -0.24725738129058442, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.006484110839664936}, {"id": 231, "seek": 229356, "start": 2299.56, "end": 2304.56, "text": " And they used a specific architecture with a latent dimension of 50.", "tokens": [50664, 400, 436, 1143, 257, 2685, 9482, 365, 257, 48994, 10139, 295, 2625, 13, 50914], "temperature": 0.0, "avg_logprob": -0.24725738129058442, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.006484110839664936}, {"id": 232, "seek": 229356, "start": 2305.56, "end": 2317.56, "text": " Yeah. For this bespoke variational monitor that I trained, I optimized the latent space and found that 64 was optimum.", "tokens": [50964, 865, 13, 1171, 341, 4097, 48776, 3034, 1478, 6002, 300, 286, 8895, 11, 286, 26941, 264, 48994, 1901, 293, 1352, 300, 12145, 390, 39326, 13, 51564], "temperature": 0.0, "avg_logprob": -0.24725738129058442, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.006484110839664936}, {"id": 233, "seek": 229356, "start": 2318.56, "end": 2319.56, "text": " And then I used 64 for that.", "tokens": [51614, 400, 550, 286, 1143, 12145, 337, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.24725738129058442, "compression_ratio": 1.482233502538071, "no_speech_prob": 0.006484110839664936}, {"id": 234, "seek": 231956, "start": 2320.56, "end": 2327.56, "text": " The input is 400 and something positions by 21. So it's about 9,800, about 10,000.", "tokens": [50414, 440, 4846, 307, 8423, 293, 746, 8432, 538, 5080, 13, 407, 309, 311, 466, 1722, 11, 14423, 11, 466, 1266, 11, 1360, 13, 50764], "temperature": 0.0, "avg_logprob": -0.24714956283569336, "compression_ratio": 1.3351063829787233, "no_speech_prob": 0.0007206877926364541}, {"id": 235, "seek": 231956, "start": 2328.56, "end": 2339.56, "text": " Yeah. And for comparison, all of the language models have latent dimensions of about 768 for ESMMSA, 1,284 for ESM1B and the others.", "tokens": [50814, 865, 13, 400, 337, 9660, 11, 439, 295, 264, 2856, 5245, 362, 48994, 12819, 295, 466, 24733, 23, 337, 12564, 17365, 8886, 11, 502, 11, 11205, 19, 337, 12564, 44, 16, 33, 293, 264, 2357, 13, 51364], "temperature": 0.0, "avg_logprob": -0.24714956283569336, "compression_ratio": 1.3351063829787233, "no_speech_prob": 0.0007206877926364541}, {"id": 236, "seek": 231956, "start": 2341.56, "end": 2343.56, "text": " And I think 1,500 for PUGIN2. Yeah.", "tokens": [51464, 400, 286, 519, 502, 11, 7526, 337, 44098, 38, 1464, 17, 13, 865, 13, 51564], "temperature": 0.0, "avg_logprob": -0.24714956283569336, "compression_ratio": 1.3351063829787233, "no_speech_prob": 0.0007206877926364541}, {"id": 237, "seek": 234956, "start": 2349.56, "end": 2350.56, "text": " I have a question.", "tokens": [50364, 286, 362, 257, 220, 20343, 313, 13, 50414], "temperature": 0.0, "avg_logprob": -0.5352822517862126, "compression_ratio": 1.2913385826771653, "no_speech_prob": 0.0037049734964966774}, {"id": 238, "seek": 234956, "start": 2352.56, "end": 2353.56, "text": " So I...", "tokens": [50514, 407, 286, 485, 50564], "temperature": 0.0, "avg_logprob": -0.5352822517862126, "compression_ratio": 1.2913385826771653, "no_speech_prob": 0.0037049734964966774}, {"id": 239, "seek": 234956, "start": 2360.56, "end": 2361.56, "text": " Yes, there were...", "tokens": [50914, 1079, 11, 456, 645, 485, 50964], "temperature": 0.0, "avg_logprob": -0.5352822517862126, "compression_ratio": 1.2913385826771653, "no_speech_prob": 0.0037049734964966774}, {"id": 240, "seek": 234956, "start": 2364.56, "end": 2372.56, "text": " The overall super family sort of classification, they're all alpha beta hydrolysis, but they're in different families.", "tokens": [51114, 440, 4787, 1687, 1605, 1333, 295, 21538, 11, 436, 434, 439, 8961, 9861, 15435, 356, 17122, 11, 457, 436, 434, 294, 819, 4466, 13, 51514], "temperature": 0.0, "avg_logprob": -0.5352822517862126, "compression_ratio": 1.2913385826771653, "no_speech_prob": 0.0037049734964966774}, {"id": 241, "seek": 237256, "start": 2373.56, "end": 2374.56, "text": " Yeah.", "tokens": [50414, 865, 13, 50464], "temperature": 0.0, "avg_logprob": -0.366903803898738, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.019977685064077377}, {"id": 242, "seek": 237256, "start": 2376.56, "end": 2381.56, "text": " As particularly as a function of the phylogenetic similarity.", "tokens": [50564, 1018, 4098, 382, 257, 2445, 295, 264, 903, 88, 4987, 268, 3532, 32194, 13, 50814], "temperature": 0.0, "avg_logprob": -0.366903803898738, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.019977685064077377}, {"id": 243, "seek": 237256, "start": 2383.56, "end": 2396.56, "text": " If you look at the tree, those ones with short branches or short leaves, particularly in groups four, five, six and seven, all fall in this polyesterase liposketenase group.", "tokens": [50914, 759, 291, 574, 412, 264, 4230, 11, 729, 2306, 365, 2099, 14770, 420, 2099, 5510, 11, 4098, 294, 3935, 1451, 11, 1732, 11, 2309, 293, 3407, 11, 439, 2100, 294, 341, 6754, 3011, 651, 8280, 329, 330, 1147, 651, 1594, 13, 51564], "temperature": 0.0, "avg_logprob": -0.366903803898738, "compression_ratio": 1.4431137724550898, "no_speech_prob": 0.019977685064077377}, {"id": 244, "seek": 239656, "start": 2397.56, "end": 2401.56, "text": " And they overall have more greater similarity structure.", "tokens": [50414, 400, 436, 4787, 362, 544, 5044, 32194, 3877, 13, 50614], "temperature": 0.0, "avg_logprob": -0.31048057079315183, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.002978211035951972}, {"id": 245, "seek": 239656, "start": 2402.56, "end": 2410.56, "text": " And then in groups one, two and three, where you have more divergence, you have even greater conflicts in the structure.", "tokens": [50664, 400, 550, 294, 3935, 472, 11, 732, 293, 1045, 11, 689, 291, 362, 544, 47387, 11, 291, 362, 754, 5044, 19807, 294, 264, 3877, 13, 51064], "temperature": 0.0, "avg_logprob": -0.31048057079315183, "compression_ratio": 1.5128205128205128, "no_speech_prob": 0.002978211035951972}, {"id": 246, "seek": 241056, "start": 2411.56, "end": 2424.56, "text": " I was just wondering, the other question I had was, when you were doing the acid, you had some tips, you might have some tips, but was there a reason that people kind of folded on it?", "tokens": [50414, 286, 390, 445, 6359, 11, 264, 661, 1168, 286, 632, 390, 11, 562, 291, 645, 884, 264, 257, 537, 67, 11, 291, 632, 512, 6082, 11, 291, 275, 910, 83, 362, 512, 6082, 11, 457, 390, 456, 257, 1778, 300, 561, 733, 295, 283, 2641, 292, 322, 309, 30, 51064], "temperature": 0.0, "avg_logprob": -0.7343834930995725, "compression_ratio": 1.4076923076923078, "no_speech_prob": 0.007812230382114649}, {"id": 247, "seek": 242456, "start": 2425.56, "end": 2430.56, "text": " Most serine hydrolysis have a basic pH range.", "tokens": [50414, 4534, 816, 533, 15435, 356, 17122, 362, 257, 3875, 21677, 3613, 13, 50664], "temperature": 0.0, "avg_logprob": -0.37884251824740706, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.06003232300281525}, {"id": 248, "seek": 242456, "start": 2431.56, "end": 2434.56, "text": " And most of them are either inactive at neutral pH or lower.", "tokens": [50714, 400, 881, 295, 552, 366, 2139, 294, 12596, 412, 10598, 21677, 420, 3126, 13, 50864], "temperature": 0.0, "avg_logprob": -0.37884251824740706, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.06003232300281525}, {"id": 249, "seek": 242456, "start": 2435.56, "end": 2439.56, "text": " And all of the pedases we've screened lose all activity at 4.5.", "tokens": [50914, 400, 439, 295, 264, 5670, 1957, 321, 600, 2568, 292, 3624, 439, 5191, 412, 1017, 13, 20, 13, 51114], "temperature": 0.0, "avg_logprob": -0.37884251824740706, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.06003232300281525}, {"id": 250, "seek": 242456, "start": 2440.56, "end": 2441.56, "text": " So, yeah.", "tokens": [51164, 407, 11, 1338, 13, 51214], "temperature": 0.0, "avg_logprob": -0.37884251824740706, "compression_ratio": 1.3432835820895523, "no_speech_prob": 0.06003232300281525}, {"id": 251, "seek": 244156, "start": 2441.56, "end": 2442.56, "text": " Yes, yes.", "tokens": [50364, 1079, 11, 2086, 13, 50414], "temperature": 0.0, "avg_logprob": -0.3626892335953251, "compression_ratio": 1.3706293706293706, "no_speech_prob": 0.00884557981044054}, {"id": 252, "seek": 244156, "start": 2443.56, "end": 2444.56, "text": " Oh, yes, yes, yes.", "tokens": [50464, 876, 11, 2086, 11, 2086, 11, 2086, 13, 50514], "temperature": 0.0, "avg_logprob": -0.3626892335953251, "compression_ratio": 1.3706293706293706, "no_speech_prob": 0.00884557981044054}, {"id": 253, "seek": 244156, "start": 2444.56, "end": 2446.56, "text": " We're looking at pet hydrolase activity.", "tokens": [50514, 492, 434, 1237, 412, 3817, 5796, 6623, 651, 5191, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3626892335953251, "compression_ratio": 1.3706293706293706, "no_speech_prob": 0.00884557981044054}, {"id": 254, "seek": 244156, "start": 2447.56, "end": 2455.56, "text": " And so activity here is specifically, of course, these are enzymes, so they do something, probably in cutanases, lipases, yes.", "tokens": [50664, 400, 370, 5191, 510, 307, 4682, 11, 295, 1164, 11, 613, 366, 29299, 11, 370, 436, 360, 746, 11, 1391, 294, 1723, 282, 1957, 11, 8280, 1957, 11, 2086, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3626892335953251, "compression_ratio": 1.3706293706293706, "no_speech_prob": 0.00884557981044054}, {"id": 255, "seek": 245556, "start": 2456.56, "end": 2457.56, "text": " Not really.", "tokens": [50414, 1726, 534, 13, 50464], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 256, "seek": 245556, "start": 2457.56, "end": 2464.56, "text": " The original hypothesis, the first pedase that was found was in a bacterium in a plastic dump.", "tokens": [50464, 440, 3380, 2477, 2259, 392, 9374, 11, 264, 700, 5670, 651, 300, 390, 1352, 390, 294, 257, 9755, 2197, 294, 257, 5900, 11430, 13, 50814], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 257, "seek": 245556, "start": 2464.56, "end": 2466.56, "text": " You generally don't find it in a plastic dump.", "tokens": [50814, 509, 5101, 500, 380, 915, 309, 294, 257, 5900, 11430, 13, 50914], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 258, "seek": 245556, "start": 2466.56, "end": 2467.56, "text": " So, yeah.", "tokens": [50914, 407, 11, 1338, 13, 50964], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 259, "seek": 245556, "start": 2467.56, "end": 2468.56, "text": " So, yeah.", "tokens": [50964, 407, 11, 1338, 13, 51014], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 260, "seek": 245556, "start": 2468.56, "end": 2469.56, "text": " So, yeah.", "tokens": [51014, 407, 11, 1338, 13, 51064], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 261, "seek": 245556, "start": 2469.56, "end": 2470.56, "text": " So, yeah.", "tokens": [51064, 407, 11, 1338, 13, 51114], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 262, "seek": 245556, "start": 2470.56, "end": 2471.56, "text": " So, yeah.", "tokens": [51114, 407, 11, 1338, 13, 51164], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 263, "seek": 245556, "start": 2471.56, "end": 2472.56, "text": " So, yeah.", "tokens": [51164, 407, 11, 1338, 13, 51214], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 264, "seek": 245556, "start": 2472.56, "end": 2473.56, "text": " So, yeah.", "tokens": [51214, 407, 11, 1338, 13, 51264], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 265, "seek": 245556, "start": 2473.56, "end": 2474.56, "text": " So, yeah.", "tokens": [51264, 407, 11, 1338, 13, 51314], "temperature": 0.0, "avg_logprob": -0.5761158719975897, "compression_ratio": 1.9098360655737705, "no_speech_prob": 0.029695121571421623}, {"id": 266, "seek": 247456, "start": 2475.56, "end": 2477.56, "text": " Not really.", "tokens": [50414, 1726, 534, 13, 50514], "temperature": 0.0, "avg_logprob": -0.23006608269431375, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.14062891900539398}, {"id": 267, "seek": 247456, "start": 2477.56, "end": 2485.56, "text": " The original hypothesis, the first pedase that was found was in a bacterium in a plastic dump.", "tokens": [50514, 440, 3380, 17291, 11, 264, 700, 5670, 651, 300, 390, 1352, 390, 294, 257, 9755, 2197, 294, 257, 5900, 11430, 13, 50914], "temperature": 0.0, "avg_logprob": -0.23006608269431375, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.14062891900539398}, {"id": 268, "seek": 247456, "start": 2486.56, "end": 2491.56, "text": " But as we've moved further from that, we're finding pedases in different organisms.", "tokens": [50964, 583, 382, 321, 600, 4259, 3052, 490, 300, 11, 321, 434, 5006, 5670, 1957, 294, 819, 22110, 13, 51214], "temperature": 0.0, "avg_logprob": -0.23006608269431375, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.14062891900539398}, {"id": 269, "seek": 247456, "start": 2491.56, "end": 2498.56, "text": " It was a pedase that was found in a human microbiome.", "tokens": [51214, 467, 390, 257, 5670, 651, 300, 390, 1352, 294, 257, 1952, 33234, 423, 13, 51564], "temperature": 0.0, "avg_logprob": -0.23006608269431375, "compression_ratio": 1.515527950310559, "no_speech_prob": 0.14062891900539398}, {"id": 270, "seek": 249856, "start": 2499.56, "end": 2506.56, "text": " So people were asking, was the person just eating a lot of plastics?", "tokens": [50414, 407, 561, 645, 3365, 11, 390, 264, 954, 445, 3936, 257, 688, 295, 34356, 30, 50764], "temperature": 0.0, "avg_logprob": -0.26133048016092053, "compression_ratio": 1.625, "no_speech_prob": 0.04195048660039902}, {"id": 271, "seek": 249856, "start": 2506.56, "end": 2520.56, "text": " The most likely explanation is that these did not evolve to be pedases because they were exposed to plastics, but they have alternative activities that are similar to pet hydrolase activity.", "tokens": [50764, 440, 881, 3700, 10835, 307, 300, 613, 630, 406, 16693, 281, 312, 5670, 1957, 570, 436, 645, 9495, 281, 34356, 11, 457, 436, 362, 8535, 5354, 300, 366, 2531, 281, 3817, 5796, 6623, 651, 5191, 13, 51464], "temperature": 0.0, "avg_logprob": -0.26133048016092053, "compression_ratio": 1.625, "no_speech_prob": 0.04195048660039902}, {"id": 272, "seek": 249856, "start": 2520.56, "end": 2522.56, "text": " And just because of the similarity in the structure.", "tokens": [51464, 400, 445, 570, 295, 264, 32194, 294, 264, 3877, 13, 51564], "temperature": 0.0, "avg_logprob": -0.26133048016092053, "compression_ratio": 1.625, "no_speech_prob": 0.04195048660039902}, {"id": 273, "seek": 252256, "start": 2522.56, "end": 2531.56, "text": " So, most esterases in the Syrian hydrolase family will possibly be pedases.", "tokens": [50364, 407, 11, 881, 871, 260, 1957, 294, 264, 24081, 5796, 6623, 651, 1605, 486, 6264, 312, 5670, 1957, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3052452350484914, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.04399598017334938}, {"id": 274, "seek": 252256, "start": 2531.56, "end": 2550.56, "text": " Given the similarity.", "tokens": [50814, 18600, 264, 32194, 13, 51764], "temperature": 0.0, "avg_logprob": -0.3052452350484914, "compression_ratio": 1.127906976744186, "no_speech_prob": 0.04399598017334938}, {"id": 275, "seek": 255056, "start": 2551.56, "end": 2558.56, "text": " So, the nature communications paper we have published, Erickson, is one of the 28 data sets.", "tokens": [50414, 407, 11, 264, 3687, 15163, 3035, 321, 362, 6572, 11, 3300, 618, 3015, 11, 307, 472, 295, 264, 7562, 1412, 6352, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3207902174729567, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.21974597871303558}, {"id": 276, "seek": 255056, "start": 2558.56, "end": 2561.56, "text": " When you hold out that data set, I think you get ABCs of 0.7.", "tokens": [50764, 1133, 291, 1797, 484, 300, 1412, 992, 11, 286, 519, 291, 483, 22342, 82, 295, 1958, 13, 22, 13, 50914], "temperature": 0.0, "avg_logprob": -0.3207902174729567, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.21974597871303558}, {"id": 277, "seek": 255056, "start": 2561.56, "end": 2564.56, "text": " So, it does rank that data set well.", "tokens": [50914, 407, 11, 309, 775, 6181, 300, 1412, 992, 731, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3207902174729567, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.21974597871303558}, {"id": 278, "seek": 255056, "start": 2564.56, "end": 2568.56, "text": " And it does rank it better than the HMM, which was 0.58.", "tokens": [51064, 400, 309, 775, 6181, 309, 1101, 813, 264, 389, 17365, 11, 597, 390, 1958, 13, 20419, 13, 51264], "temperature": 0.0, "avg_logprob": -0.3207902174729567, "compression_ratio": 1.4090909090909092, "no_speech_prob": 0.21974597871303558}, {"id": 279, "seek": 256856, "start": 2568.56, "end": 2576.56, "text": " And across all of the data sets, we have 0.79, I think, on average for natural.", "tokens": [50364, 400, 2108, 439, 295, 264, 1412, 6352, 11, 321, 362, 1958, 13, 32042, 11, 286, 519, 11, 322, 4274, 337, 3303, 13, 50764], "temperature": 0.0, "avg_logprob": -0.25629900542783063, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.1601680964231491}, {"id": 280, "seek": 256856, "start": 2576.56, "end": 2580.56, "text": " So, it does a good job of ranking the activity.", "tokens": [50764, 407, 11, 309, 775, 257, 665, 1691, 295, 17833, 264, 5191, 13, 50964], "temperature": 0.0, "avg_logprob": -0.25629900542783063, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.1601680964231491}, {"id": 281, "seek": 256856, "start": 2580.56, "end": 2589.56, "text": " So, we want to use this to, or we are currently using this to rank pet hydrolase activities across the entire sequence of space.", "tokens": [50964, 407, 11, 321, 528, 281, 764, 341, 281, 11, 420, 321, 366, 4362, 1228, 341, 281, 6181, 3817, 5796, 6623, 651, 5354, 2108, 264, 2302, 8310, 295, 1901, 13, 51414], "temperature": 0.0, "avg_logprob": -0.25629900542783063, "compression_ratio": 1.471264367816092, "no_speech_prob": 0.1601680964231491}, {"id": 282, "seek": 258956, "start": 2590.56, "end": 2597.56, "text": " So, I noticed in most of your screens, you're really focused on predicting thermostability and activity.", "tokens": [50414, 407, 11, 286, 5694, 294, 881, 295, 428, 11171, 11, 291, 434, 534, 5178, 322, 32884, 8810, 555, 2310, 293, 5191, 13, 50764], "temperature": 0.0, "avg_logprob": -0.3079864462626349, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.871764063835144}, {"id": 283, "seek": 258956, "start": 2597.56, "end": 2610.56, "text": " But I know that from screening these enzymes, that if something has a really low expression, we're going to eliminate it very fast because it's impossible to reduce even if it's really thermostable and has a high activity.", "tokens": [50764, 583, 286, 458, 300, 490, 17732, 613, 29299, 11, 300, 498, 746, 575, 257, 534, 2295, 6114, 11, 321, 434, 516, 281, 13819, 309, 588, 2370, 570, 309, 311, 6243, 281, 5407, 754, 498, 309, 311, 534, 8810, 555, 712, 293, 575, 257, 1090, 5191, 13, 51414], "temperature": 0.0, "avg_logprob": -0.3079864462626349, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.871764063835144}, {"id": 284, "seek": 258956, "start": 2610.56, "end": 2613.56, "text": " If there's just not enough of it, we're not going to really be able to use it.", "tokens": [51414, 759, 456, 311, 445, 406, 1547, 295, 309, 11, 321, 434, 406, 516, 281, 534, 312, 1075, 281, 764, 309, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3079864462626349, "compression_ratio": 1.7276595744680852, "no_speech_prob": 0.871764063835144}, {"id": 285, "seek": 261356, "start": 2614.56, "end": 2618.56, "text": " So, I'm wondering, like, where are you getting the expression data from?", "tokens": [50414, 407, 11, 286, 478, 6359, 11, 411, 11, 689, 366, 291, 1242, 264, 6114, 1412, 490, 30, 50614], "temperature": 0.0, "avg_logprob": -0.2526045454309342, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.04334057494997978}, {"id": 286, "seek": 261356, "start": 2618.56, "end": 2622.56, "text": " Do you have to actually express it in E. coli to get that data?", "tokens": [50614, 1144, 291, 362, 281, 767, 5109, 309, 294, 462, 13, 1173, 72, 281, 483, 300, 1412, 30, 50814], "temperature": 0.0, "avg_logprob": -0.2526045454309342, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.04334057494997978}, {"id": 287, "seek": 261356, "start": 2622.56, "end": 2628.56, "text": " Or is there any information from your models that could be used to sort of ensure its expression as well?", "tokens": [50814, 1610, 307, 456, 604, 1589, 490, 428, 5245, 300, 727, 312, 1143, 281, 1333, 295, 5586, 1080, 6114, 382, 731, 30, 51114], "temperature": 0.0, "avg_logprob": -0.2526045454309342, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.04334057494997978}, {"id": 288, "seek": 261356, "start": 2628.56, "end": 2630.56, "text": " That's a very good question.", "tokens": [51114, 663, 311, 257, 588, 665, 1168, 13, 51214], "temperature": 0.0, "avg_logprob": -0.2526045454309342, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.04334057494997978}, {"id": 289, "seek": 261356, "start": 2630.56, "end": 2637.56, "text": " We did not include expression data in the training data at all, because that is sort of biased.", "tokens": [51214, 492, 630, 406, 4090, 6114, 1412, 294, 264, 3097, 1412, 412, 439, 11, 570, 300, 307, 1333, 295, 28035, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2526045454309342, "compression_ratio": 1.5956521739130434, "no_speech_prob": 0.04334057494997978}, {"id": 290, "seek": 263756, "start": 2638.56, "end": 2645.56, "text": " If we're testing, if we're predicting the activity of an enzyme, it most likely expressed for that for it to be tested.", "tokens": [50414, 759, 321, 434, 4997, 11, 498, 321, 434, 32884, 264, 5191, 295, 364, 24521, 11, 309, 881, 3700, 12675, 337, 300, 337, 309, 281, 312, 8246, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2411280990740575, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.09661481529474258}, {"id": 291, "seek": 263756, "start": 2645.56, "end": 2651.56, "text": " It may not have been E. coli and may have been a different, even if it were E. coli, it would be a different expression system.", "tokens": [50764, 467, 815, 406, 362, 668, 462, 13, 1173, 72, 293, 815, 362, 668, 257, 819, 11, 754, 498, 309, 645, 462, 13, 1173, 72, 11, 309, 576, 312, 257, 819, 6114, 1185, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2411280990740575, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.09661481529474258}, {"id": 292, "seek": 263756, "start": 2651.56, "end": 2657.56, "text": " I tried predicting expression, and there are machine learning methods that do this.", "tokens": [51064, 286, 3031, 32884, 6114, 11, 293, 456, 366, 3479, 2539, 7150, 300, 360, 341, 13, 51364], "temperature": 0.0, "avg_logprob": -0.2411280990740575, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.09661481529474258}, {"id": 293, "seek": 263756, "start": 2657.56, "end": 2662.56, "text": " They take language model embeddings, and then they try to predict solubility and expression in E. coli.", "tokens": [51364, 814, 747, 2856, 2316, 12240, 29432, 11, 293, 550, 436, 853, 281, 6069, 1404, 836, 1140, 293, 6114, 294, 462, 13, 1173, 72, 13, 51614], "temperature": 0.0, "avg_logprob": -0.2411280990740575, "compression_ratio": 1.7901234567901234, "no_speech_prob": 0.09661481529474258}, {"id": 294, "seek": 266256, "start": 2663.56, "end": 2667.56, "text": " I find that these methods, I found that these methods did not do very well on our data.", "tokens": [50414, 286, 915, 300, 613, 7150, 11, 286, 1352, 300, 613, 7150, 630, 406, 360, 588, 731, 322, 527, 1412, 13, 50614], "temperature": 0.0, "avg_logprob": -0.2874287028371552, "compression_ratio": 1.7783505154639174, "no_speech_prob": 0.005218946374952793}, {"id": 295, "seek": 266256, "start": 2667.56, "end": 2676.56, "text": " In fact, the heat and marker model alignment score outperformed all of these in predicting expression for E. coli.", "tokens": [50614, 682, 1186, 11, 264, 3738, 293, 1849, 5767, 2316, 18515, 6175, 484, 610, 22892, 439, 295, 613, 294, 32884, 6114, 337, 462, 13, 1173, 72, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2874287028371552, "compression_ratio": 1.7783505154639174, "no_speech_prob": 0.005218946374952793}, {"id": 296, "seek": 266256, "start": 2676.56, "end": 2684.56, "text": " And we also found that from our data sets that the sequences that had higher heat and marker model scores, alignment scores, expressed better.", "tokens": [51064, 400, 321, 611, 1352, 300, 490, 527, 1412, 6352, 300, 264, 22978, 300, 632, 2946, 3738, 293, 15247, 2316, 13444, 11, 18515, 13444, 11, 12675, 1101, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2874287028371552, "compression_ratio": 1.7783505154639174, "no_speech_prob": 0.005218946374952793}, {"id": 297, "seek": 268456, "start": 2684.56, "end": 2692.56, "text": " So we think that if we're selecting for things that align well with the PETAS HMM, we'll see better expression.", "tokens": [50364, 407, 321, 519, 300, 498, 321, 434, 18182, 337, 721, 300, 7975, 731, 365, 264, 21968, 3160, 389, 17365, 11, 321, 603, 536, 1101, 6114, 13, 50764], "temperature": 0.0, "avg_logprob": -0.2147206822666553, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012224853038787842}, {"id": 298, "seek": 268456, "start": 2692.56, "end": 2695.56, "text": " But it's one of the problems that we were faced with.", "tokens": [50764, 583, 309, 311, 472, 295, 264, 2740, 300, 321, 645, 11446, 365, 13, 50914], "temperature": 0.0, "avg_logprob": -0.2147206822666553, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012224853038787842}, {"id": 299, "seek": 268456, "start": 2695.56, "end": 2698.56, "text": " And I think we can only predict one thing at a time.", "tokens": [50914, 400, 286, 519, 321, 393, 787, 6069, 472, 551, 412, 257, 565, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2147206822666553, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012224853038787842}, {"id": 300, "seek": 268456, "start": 2698.56, "end": 2700.56, "text": " Another thing we care about is pH.", "tokens": [51064, 3996, 551, 321, 1127, 466, 307, 21677, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2147206822666553, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012224853038787842}, {"id": 301, "seek": 268456, "start": 2700.56, "end": 2708.56, "text": " We're interested in lower pH because the product of PET degradation is terephthalic acid, which is acidic.", "tokens": [51164, 492, 434, 3102, 294, 3126, 21677, 570, 264, 1674, 295, 21968, 40519, 307, 256, 323, 79, 49123, 299, 8258, 11, 597, 307, 39514, 13, 51564], "temperature": 0.0, "avg_logprob": -0.2147206822666553, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012224853038787842}, {"id": 302, "seek": 268456, "start": 2708.56, "end": 2711.56, "text": " And these enzymes don't work well at acidic conditions.", "tokens": [51564, 400, 613, 29299, 500, 380, 589, 731, 412, 39514, 4487, 13, 51714], "temperature": 0.0, "avg_logprob": -0.2147206822666553, "compression_ratio": 1.606177606177606, "no_speech_prob": 0.012224853038787842}, {"id": 303, "seek": 271156, "start": 2711.56, "end": 2717.56, "text": " So we want to find enzymes that are as acidic, acid tolerant as possible.", "tokens": [50364, 407, 321, 528, 281, 915, 29299, 300, 366, 382, 39514, 11, 8258, 45525, 382, 1944, 13, 50664], "temperature": 0.0, "avg_logprob": -0.24073477658358486, "compression_ratio": 1.4258064516129032, "no_speech_prob": 0.0008829465368762612}, {"id": 304, "seek": 271156, "start": 2717.56, "end": 2732.56, "text": " So in a different project, I'm training deep models, deep learning models to predict acid tolerance and pH optimum.", "tokens": [50664, 407, 294, 257, 819, 1716, 11, 286, 478, 3097, 2452, 5245, 11, 2452, 2539, 5245, 281, 6069, 8258, 23368, 293, 21677, 39326, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24073477658358486, "compression_ratio": 1.4258064516129032, "no_speech_prob": 0.0008829465368762612}, {"id": 305, "seek": 271156, "start": 2732.56, "end": 2734.56, "text": " Currently, no. The most acidic.", "tokens": [51414, 19964, 11, 572, 13, 440, 881, 39514, 13, 51514], "temperature": 0.0, "avg_logprob": -0.24073477658358486, "compression_ratio": 1.4258064516129032, "no_speech_prob": 0.0008829465368762612}, {"id": 306, "seek": 273456, "start": 2734.56, "end": 2737.56, "text": " So we've tested these at different pH.", "tokens": [50364, 407, 321, 600, 8246, 613, 412, 819, 21677, 13, 50514], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 307, "seek": 273456, "start": 2737.56, "end": 2741.56, "text": " And then most of them have optimal pH of 8, 9.", "tokens": [50514, 400, 550, 881, 295, 552, 362, 16252, 21677, 295, 1649, 11, 1722, 13, 50714], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 308, "seek": 273456, "start": 2741.56, "end": 2745.56, "text": " If you lower it down to 6, below 6, most of them lose their activity.", "tokens": [50714, 759, 291, 3126, 309, 760, 281, 1386, 11, 2507, 1386, 11, 881, 295, 552, 3624, 641, 5191, 13, 50914], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 309, "seek": 273456, "start": 2745.56, "end": 2748.56, "text": " We took a few of them that retain activity at 6.", "tokens": [50914, 492, 1890, 257, 1326, 295, 552, 300, 18340, 5191, 412, 1386, 13, 51064], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 310, "seek": 273456, "start": 2748.56, "end": 2750.56, "text": " And then we tested at 5 and 4.", "tokens": [51064, 400, 550, 321, 8246, 412, 1025, 293, 1017, 13, 51164], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 311, "seek": 273456, "start": 2750.56, "end": 2753.56, "text": " And only one of them had activity at 4.5.", "tokens": [51164, 400, 787, 472, 295, 552, 632, 5191, 412, 1017, 13, 20, 13, 51314], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 312, "seek": 273456, "start": 2753.56, "end": 2757.56, "text": " And if you go below 4.5, it loses activity.", "tokens": [51314, 400, 498, 291, 352, 2507, 1017, 13, 20, 11, 309, 18293, 5191, 13, 51514], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 313, "seek": 273456, "start": 2757.56, "end": 2758.56, "text": " Oh, yes, we can.", "tokens": [51514, 876, 11, 2086, 11, 321, 393, 13, 51564], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 314, "seek": 273456, "start": 2758.56, "end": 2761.56, "text": " And we're doing that currently.", "tokens": [51564, 400, 321, 434, 884, 300, 4362, 13, 51714], "temperature": 0.0, "avg_logprob": -0.20120959281921386, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.39924830198287964}, {"id": 315, "seek": 276156, "start": 2761.56, "end": 2770.56, "text": " It's not part of my talk, but I trained a deep learning model to predict the pH optimum from about 2 million proteins using the pH of the environment.", "tokens": [50364, 467, 311, 406, 644, 295, 452, 751, 11, 457, 286, 8895, 257, 2452, 2539, 2316, 281, 6069, 264, 21677, 39326, 490, 466, 568, 2459, 15577, 1228, 264, 21677, 295, 264, 2823, 13, 50814], "temperature": 0.0, "avg_logprob": -0.19177450135696766, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.0074579291976988316}, {"id": 316, "seek": 276156, "start": 2770.56, "end": 2777.56, "text": " So I took secreted enzymes and took the pH of the environment and then tried to train a model in that.", "tokens": [50814, 407, 286, 1890, 4054, 292, 29299, 293, 1890, 264, 21677, 295, 264, 2823, 293, 550, 3031, 281, 3847, 257, 2316, 294, 300, 13, 51164], "temperature": 0.0, "avg_logprob": -0.19177450135696766, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.0074579291976988316}, {"id": 317, "seek": 276156, "start": 2777.56, "end": 2785.56, "text": " And we're using deep learning methods as well as zero-shot predictions to engineer that, as well as search.", "tokens": [51164, 400, 321, 434, 1228, 2452, 2539, 7150, 382, 731, 382, 4018, 12, 18402, 21264, 281, 11403, 300, 11, 382, 731, 382, 3164, 13, 51564], "temperature": 0.0, "avg_logprob": -0.19177450135696766, "compression_ratio": 1.7109004739336493, "no_speech_prob": 0.0074579291976988316}, {"id": 318, "seek": 278556, "start": 2785.56, "end": 2796.56, "text": " So in addition to this model, we will use these pH prediction models to sort of rank the sequences that we pull out from the databases.", "tokens": [50364, 407, 294, 4500, 281, 341, 2316, 11, 321, 486, 764, 613, 21677, 17630, 5245, 281, 1333, 295, 6181, 264, 22978, 300, 321, 2235, 484, 490, 264, 22380, 13, 50914], "temperature": 0.0, "avg_logprob": -0.19695277895246233, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0036483255680650473}, {"id": 319, "seek": 278556, "start": 2796.56, "end": 2803.56, "text": " Hopefully, we'll find things that are functional at low pH.", "tokens": [50914, 10429, 11, 321, 603, 915, 721, 300, 366, 11745, 412, 2295, 21677, 13, 51264], "temperature": 0.0, "avg_logprob": -0.19695277895246233, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0036483255680650473}, {"id": 320, "seek": 278556, "start": 2803.56, "end": 2805.56, "text": " Yes, yes.", "tokens": [51264, 1079, 11, 2086, 13, 51364], "temperature": 0.0, "avg_logprob": -0.19695277895246233, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0036483255680650473}, {"id": 321, "seek": 278556, "start": 2805.56, "end": 2812.56, "text": " Hopefully, acidophiles should have lower pH optimum compared to other organisms.", "tokens": [51364, 10429, 11, 8258, 5317, 4680, 820, 362, 3126, 21677, 39326, 5347, 281, 661, 22110, 13, 51714], "temperature": 0.0, "avg_logprob": -0.19695277895246233, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.0036483255680650473}, {"id": 322, "seek": 281256, "start": 2812.56, "end": 2816.56, "text": " And so if a pedis is from an acidophile, then it's likely to have that.", "tokens": [50364, 400, 370, 498, 257, 5670, 271, 307, 490, 364, 8258, 49242, 11, 550, 309, 311, 3700, 281, 362, 300, 13, 50564], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 323, "seek": 281256, "start": 2816.56, "end": 2819.56, "text": " But see, now you're trying to work with four things.", "tokens": [50564, 583, 536, 11, 586, 291, 434, 1382, 281, 589, 365, 1451, 721, 13, 50714], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 324, "seek": 281256, "start": 2819.56, "end": 2821.56, "text": " You're trying to predict expression.", "tokens": [50714, 509, 434, 1382, 281, 6069, 6114, 13, 50814], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 325, "seek": 281256, "start": 2821.56, "end": 2822.56, "text": " You're trying to predict activity.", "tokens": [50814, 509, 434, 1382, 281, 6069, 5191, 13, 50864], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 326, "seek": 281256, "start": 2822.56, "end": 2824.56, "text": " You're trying to predict thermostability.", "tokens": [50864, 509, 434, 1382, 281, 6069, 8810, 555, 2310, 13, 50964], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 327, "seek": 281256, "start": 2824.56, "end": 2826.56, "text": " And you're trying to predict pH tolerance.", "tokens": [50964, 400, 291, 434, 1382, 281, 6069, 21677, 23368, 13, 51064], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 328, "seek": 281256, "start": 2826.56, "end": 2831.56, "text": " And also, you're trying to predict substrate specificity.", "tokens": [51064, 400, 611, 11, 291, 434, 1382, 281, 6069, 27585, 2685, 507, 13, 51314], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 329, "seek": 281256, "start": 2831.56, "end": 2837.56, "text": " Does it function better on crystalline substrate versus amorphous substrate?", "tokens": [51314, 4402, 309, 2445, 1101, 322, 31924, 533, 27585, 5717, 15543, 950, 563, 27585, 30, 51614], "temperature": 0.0, "avg_logprob": -0.18131838807272255, "compression_ratio": 1.908256880733945, "no_speech_prob": 0.12751983106136322}, {"id": 330, "seek": 283756, "start": 2838.56, "end": 2849.56, "text": " Because if it does better on crystalline substrate, you can reduce the amount of mechanical preprocessing that goes into preparing the plastic waste.", "tokens": [50414, 1436, 498, 309, 775, 1101, 322, 31924, 533, 27585, 11, 291, 393, 5407, 264, 2372, 295, 12070, 2666, 340, 780, 278, 300, 1709, 666, 10075, 264, 5900, 5964, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17853603098127577, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.05661383271217346}, {"id": 331, "seek": 283756, "start": 2849.56, "end": 2857.56, "text": " And so it's sort of an orthogonal prediction approach where these two things don't necessarily correlate.", "tokens": [50964, 400, 370, 309, 311, 1333, 295, 364, 41488, 17630, 3109, 689, 613, 732, 721, 500, 380, 4725, 48742, 13, 51364], "temperature": 0.0, "avg_logprob": -0.17853603098127577, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.05661383271217346}, {"id": 332, "seek": 283756, "start": 2857.56, "end": 2862.56, "text": " And if something is very acid tolerant, it doesn't mean it has activity.", "tokens": [51364, 400, 498, 746, 307, 588, 8258, 45525, 11, 309, 1177, 380, 914, 309, 575, 5191, 13, 51614], "temperature": 0.0, "avg_logprob": -0.17853603098127577, "compression_ratio": 1.5769230769230769, "no_speech_prob": 0.05661383271217346}, {"id": 333, "seek": 286256, "start": 2862.56, "end": 2883.56, "text": " And the question of how do we combine all of these to search the sequence space is something that I'm interested in looking at.", "tokens": [50364, 400, 264, 1168, 295, 577, 360, 321, 10432, 439, 295, 613, 281, 3164, 264, 8310, 1901, 307, 746, 300, 286, 478, 3102, 294, 1237, 412, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2302956832082648, "compression_ratio": 1.4466666666666668, "no_speech_prob": 0.013628453947603703}, {"id": 334, "seek": 286256, "start": 2883.56, "end": 2891.56, "text": " Yes, we're working on nylonases, polyurethanases, two enzymes I know that I'm working on.", "tokens": [51414, 1079, 11, 321, 434, 1364, 322, 43503, 1957, 11, 6754, 540, 392, 14292, 279, 11, 732, 29299, 286, 458, 300, 286, 478, 1364, 322, 13, 51814], "temperature": 0.0, "avg_logprob": -0.2302956832082648, "compression_ratio": 1.4466666666666668, "no_speech_prob": 0.013628453947603703}, {"id": 335, "seek": 289156, "start": 2891.56, "end": 2896.56, "text": " And there are other groups that are looking at different proteins, different enzymes, plastic enzymes as well.", "tokens": [50364, 400, 456, 366, 661, 3935, 300, 366, 1237, 412, 819, 15577, 11, 819, 29299, 11, 5900, 29299, 382, 731, 13, 50614], "temperature": 0.0, "avg_logprob": -0.3914758658703463, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.19173167645931244}, {"id": 336, "seek": 289156, "start": 2896.56, "end": 2906.56, "text": " Pertosis seems to be the most interesting and has received the most attention in the literature.", "tokens": [50614, 430, 911, 8211, 2544, 281, 312, 264, 881, 1880, 293, 575, 4613, 264, 881, 3202, 294, 264, 10394, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3914758658703463, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.19173167645931244}, {"id": 337, "seek": 289156, "start": 2906.56, "end": 2914.56, "text": " Probably because plastic bottles are polyethylene terephthalate is the most abundant man-made polymer.", "tokens": [51114, 9210, 570, 5900, 15923, 366, 6754, 3293, 46830, 256, 323, 79, 49123, 473, 307, 264, 881, 30657, 587, 12, 10341, 20073, 13, 51514], "temperature": 0.0, "avg_logprob": -0.3914758658703463, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.19173167645931244}, {"id": 338, "seek": 289156, "start": 2914.56, "end": 2918.56, "text": " And so it's gotten a lot of attention.", "tokens": [51514, 400, 370, 309, 311, 5768, 257, 688, 295, 3202, 13, 51714], "temperature": 0.0, "avg_logprob": -0.3914758658703463, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.19173167645931244}, {"id": 339, "seek": 291856, "start": 2918.56, "end": 2927.56, "text": " And I believe that similar approaches will yield successes in other plastic enzymes as well.", "tokens": [50364, 400, 286, 1697, 300, 2531, 11587, 486, 11257, 26101, 294, 661, 5900, 29299, 382, 731, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3670439286665483, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.3841771185398102}, {"id": 340, "seek": 291856, "start": 2927.56, "end": 2932.56, "text": " So you're talking about, you know, there's a lot of different factors that need to be predicted here.", "tokens": [50814, 407, 291, 434, 1417, 466, 11, 291, 458, 11, 456, 311, 257, 688, 295, 819, 6771, 300, 643, 281, 312, 19147, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3670439286665483, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.3841771185398102}, {"id": 341, "seek": 291856, "start": 2932.56, "end": 2935.56, "text": " There's different heights of the head.", "tokens": [51064, 821, 311, 819, 25930, 295, 264, 1378, 13, 51214], "temperature": 0.0, "avg_logprob": -0.3670439286665483, "compression_ratio": 1.4294478527607362, "no_speech_prob": 0.3841771185398102}, {"id": 342, "seek": 293556, "start": 2936.56, "end": 2942.56, "text": " And from my experience, an enzyme that works really well on the endocrine axis doesn't necessarily work very well on crystalline heads.", "tokens": [50414, 400, 490, 452, 1752, 11, 364, 24521, 300, 1985, 534, 731, 322, 264, 917, 3617, 533, 10298, 1177, 380, 4725, 589, 588, 731, 322, 13662, 1889, 8050, 13, 50714], "temperature": 0.0, "avg_logprob": -0.526476628071553, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.8490796685218811}, {"id": 343, "seek": 293556, "start": 2942.56, "end": 2949.56, "text": " So we sort of talked about the idea of an enzyme cocktail where we're using a bunch of different head phases.", "tokens": [50714, 407, 321, 1333, 295, 2825, 466, 264, 1558, 295, 364, 24521, 26382, 689, 321, 434, 1228, 257, 3840, 295, 819, 1378, 18764, 13, 51064], "temperature": 0.0, "avg_logprob": -0.526476628071553, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.8490796685218811}, {"id": 344, "seek": 293556, "start": 2949.56, "end": 2954.56, "text": " There's just one plastic bottle because the plastic bottle doesn't have much.", "tokens": [51064, 821, 311, 445, 472, 5900, 7817, 570, 264, 5900, 7817, 1177, 380, 362, 709, 13, 51314], "temperature": 0.0, "avg_logprob": -0.526476628071553, "compression_ratio": 1.5833333333333333, "no_speech_prob": 0.8490796685218811}, {"id": 345, "seek": 295456, "start": 2954.56, "end": 2965.56, "text": " So I have a question about like how your approach is ideally would be applied to sort of creating an enzyme cocktail that could actually be used in similar ways.", "tokens": [50364, 407, 286, 362, 257, 1168, 466, 411, 577, 428, 3109, 307, 22915, 576, 312, 6456, 281, 1333, 295, 4084, 364, 24521, 26382, 300, 727, 767, 312, 1143, 294, 2531, 2098, 13, 50914], "temperature": 0.0, "avg_logprob": -0.33626831351936637, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.005817745812237263}, {"id": 346, "seek": 295456, "start": 2965.56, "end": 2973.56, "text": " We don't have sufficient data to discriminate amorphous selecting or amorphous preferring pedases from crystalline.", "tokens": [50914, 492, 500, 380, 362, 11563, 1412, 281, 47833, 15543, 950, 563, 18182, 420, 15543, 950, 563, 4382, 2937, 5670, 1957, 490, 31924, 533, 13, 51314], "temperature": 0.0, "avg_logprob": -0.33626831351936637, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.005817745812237263}, {"id": 347, "seek": 295456, "start": 2973.56, "end": 2977.56, "text": " And I recognize that that would be a bias in the data set.", "tokens": [51314, 400, 286, 5521, 300, 300, 576, 312, 257, 12577, 294, 264, 1412, 992, 13, 51514], "temperature": 0.0, "avg_logprob": -0.33626831351936637, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.005817745812237263}, {"id": 348, "seek": 297756, "start": 2978.56, "end": 2985.56, "text": " If you have a lot of the different 28 studies, if you have most of them are amorphous pedase conditions were used in the screening.", "tokens": [50414, 759, 291, 362, 257, 688, 295, 264, 819, 7562, 5313, 11, 498, 291, 362, 881, 295, 552, 366, 15543, 950, 563, 5670, 651, 4487, 645, 1143, 294, 264, 17732, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23937426822286256, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.07798552513122559}, {"id": 349, "seek": 297756, "start": 2985.56, "end": 2990.56, "text": " And so that would probably bias the models learning to amorphous conditions.", "tokens": [50764, 400, 370, 300, 576, 1391, 12577, 264, 5245, 2539, 281, 15543, 950, 563, 4487, 13, 51014], "temperature": 0.0, "avg_logprob": -0.23937426822286256, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.07798552513122559}, {"id": 350, "seek": 297756, "start": 2990.56, "end": 2996.56, "text": " But the hypothesis is that the model learns generally what makes a better pedase across conditions.", "tokens": [51014, 583, 264, 17291, 307, 300, 264, 2316, 27152, 5101, 437, 1669, 257, 1101, 5670, 651, 2108, 4487, 13, 51314], "temperature": 0.0, "avg_logprob": -0.23937426822286256, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.07798552513122559}, {"id": 351, "seek": 299656, "start": 2996.56, "end": 3010.56, "text": " But as we as we go forward, as we generate data, it will be interesting to start to play around with modeling approaches to see how that how we can discriminate these enzymes.", "tokens": [50364, 583, 382, 321, 382, 321, 352, 2128, 11, 382, 321, 8460, 1412, 11, 309, 486, 312, 1880, 281, 722, 281, 862, 926, 365, 15983, 11587, 281, 536, 577, 300, 577, 321, 393, 47833, 613, 29299, 13, 51064], "temperature": 0.0, "avg_logprob": -0.27788970470428465, "compression_ratio": 1.4, "no_speech_prob": 0.028420569375157356}, {"id": 352, "seek": 301056, "start": 3010.56, "end": 3020.56, "text": " Yeah.", "tokens": [50364, 865, 13, 50864], "temperature": 0.0, "avg_logprob": -0.4249522844950358, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.5541619062423706}, {"id": 353, "seek": 301056, "start": 3020.56, "end": 3023.56, "text": " Oh, another question I have is just.", "tokens": [50864, 876, 11, 1071, 1168, 286, 362, 307, 445, 13, 51014], "temperature": 0.0, "avg_logprob": -0.4249522844950358, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.5541619062423706}, {"id": 354, "seek": 301056, "start": 3023.56, "end": 3036.56, "text": " So, now that you're sort of identifying all these novel cases, let's say, say, so can you just describe like what are the next steps in that process you've identified a novel headaches.", "tokens": [51014, 407, 11, 586, 300, 291, 434, 1333, 295, 16696, 439, 613, 7613, 3331, 11, 718, 311, 584, 11, 584, 11, 370, 393, 291, 445, 6786, 411, 437, 366, 264, 958, 4439, 294, 300, 1399, 291, 600, 9234, 257, 7613, 35046, 13, 51664], "temperature": 0.0, "avg_logprob": -0.4249522844950358, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.5541619062423706}, {"id": 355, "seek": 303656, "start": 3037.56, "end": 3041.56, "text": " Yeah.", "tokens": [50414, 865, 13, 50614], "temperature": 0.0, "avg_logprob": -0.4418487548828125, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.2119385451078415}, {"id": 356, "seek": 303656, "start": 3041.56, "end": 3052.56, "text": " That's a, that's a good question on the public on the scientist side. We published a paper. Well, it's now on Google Scholar. Most scientists, that's what they care about and they're like we're done, we can move on.", "tokens": [50614, 663, 311, 257, 11, 300, 311, 257, 665, 1168, 322, 264, 1908, 322, 264, 12662, 1252, 13, 492, 6572, 257, 3035, 13, 1042, 11, 309, 311, 586, 322, 3329, 2065, 15276, 13, 4534, 7708, 11, 300, 311, 437, 436, 1127, 466, 293, 436, 434, 411, 321, 434, 1096, 11, 321, 393, 1286, 322, 13, 51164], "temperature": 0.0, "avg_logprob": -0.4418487548828125, "compression_ratio": 1.4635761589403973, "no_speech_prob": 0.2119385451078415}, {"id": 357, "seek": 305256, "start": 3052.56, "end": 3075.56, "text": " There's a patent for it, and companies are interested in these are using it in in their processes, but it most importantly forms a bedrock for further engineering, because as we see the different pedases have different performance and different", "tokens": [50364, 821, 311, 257, 20495, 337, 309, 11, 293, 3431, 366, 3102, 294, 613, 366, 1228, 309, 294, 294, 641, 7555, 11, 457, 309, 881, 8906, 6422, 257, 2901, 17799, 337, 3052, 7043, 11, 570, 382, 321, 536, 264, 819, 5670, 1957, 362, 819, 3389, 293, 819, 51514], "temperature": 0.0, "avg_logprob": -0.30602880477905275, "compression_ratio": 1.5844155844155845, "no_speech_prob": 0.27804115414619446}, {"id": 358, "seek": 307556, "start": 3076.56, "end": 3092.56, "text": " So we could start to think about cocktails and then synthetic variants of these enzymes to improve performance we could start with. So you want to improve crystalline performance and crystalline substrate, the enzyme one of the enzymes that should really good performance", "tokens": [50414, 407, 321, 727, 722, 281, 519, 466, 49006, 293, 550, 23420, 21669, 295, 613, 29299, 281, 3470, 3389, 321, 727, 722, 365, 13, 407, 291, 528, 281, 3470, 31924, 533, 3389, 293, 31924, 533, 27585, 11, 264, 24521, 472, 295, 264, 29299, 300, 820, 534, 665, 3389, 51214], "temperature": 0.0, "avg_logprob": -0.3562234616747089, "compression_ratio": 1.7597402597402598, "no_speech_prob": 0.5269500613212585}, {"id": 359, "seek": 309256, "start": 3092.56, "end": 3116.56, "text": " 611. We could take 611 and start to do enzyme engineering on it and I know that there are groups that are working on that as well.", "tokens": [50364, 1386, 5348, 13, 492, 727, 747, 1386, 5348, 293, 722, 281, 360, 24521, 7043, 322, 309, 293, 286, 458, 300, 456, 366, 3935, 300, 366, 1364, 322, 300, 382, 731, 13, 51564], "temperature": 0.0, "avg_logprob": -0.3812657220023019, "compression_ratio": 1.2380952380952381, "no_speech_prob": 0.10366342216730118}, {"id": 360, "seek": 311656, "start": 3117.56, "end": 3125.56, "text": " That's a really good point. I, we have not done that. We have not fine tune language models on the pair of these data sets.", "tokens": [50414, 663, 311, 257, 534, 665, 935, 13, 286, 11, 321, 362, 406, 1096, 300, 13, 492, 362, 406, 2489, 10864, 2856, 5245, 322, 264, 6119, 295, 613, 1412, 6352, 13, 50814], "temperature": 0.0, "avg_logprob": -0.3624478227951947, "compression_ratio": 1.23, "no_speech_prob": 0.12573827803134918}, {"id": 361, "seek": 312556, "start": 3126.56, "end": 3141.56, "text": " That is because in the literature, sometimes that actually makes things worse, and you lose, instead of you, making it better you start to lose the global unsupervised features that we learned.", "tokens": [50414, 663, 307, 570, 294, 264, 10394, 11, 2171, 300, 767, 1669, 721, 5324, 11, 293, 291, 3624, 11, 2602, 295, 291, 11, 1455, 309, 1101, 291, 722, 281, 3624, 264, 4338, 2693, 12879, 24420, 4122, 300, 321, 3264, 13, 51164], "temperature": 0.0, "avg_logprob": -0.23852964889171513, "compression_ratio": 1.4402985074626866, "no_speech_prob": 0.39554303884506226}, {"id": 362, "seek": 314156, "start": 3142.56, "end": 3158.56, "text": " And so, some other people suggest fine tuning the embedding so you have frozen embeddings and then you train a model fine tuned on that. But we're treading on, we're treading on very dangerous territory here since we have very small data as well.", "tokens": [50414, 400, 370, 11, 512, 661, 561, 3402, 2489, 15164, 264, 12240, 3584, 370, 291, 362, 12496, 12240, 29432, 293, 550, 291, 3847, 257, 2316, 2489, 10870, 322, 300, 13, 583, 321, 434, 2192, 8166, 322, 11, 321, 434, 2192, 8166, 322, 588, 5795, 11360, 510, 1670, 321, 362, 588, 1359, 1412, 382, 731, 13, 51214], "temperature": 0.0, "avg_logprob": -0.293155078230233, "compression_ratio": 1.5870967741935484, "no_speech_prob": 0.3413439989089966}, {"id": 363, "seek": 315856, "start": 3158.56, "end": 3172.56, "text": " The risk of overfitting is large as well. Another thing is I could take the language embeddings and train the frozen language embeddings and train on an even more expansive model. Why did we use logistic regression, we could train, I have 449 sequences.", "tokens": [50364, 440, 3148, 295, 670, 69, 2414, 307, 2416, 382, 731, 13, 3996, 551, 307, 286, 727, 747, 264, 2856, 12240, 29432, 293, 3847, 264, 12496, 2856, 12240, 29432, 293, 3847, 322, 364, 754, 544, 46949, 2316, 13, 1545, 630, 321, 764, 3565, 3142, 24590, 11, 321, 727, 3847, 11, 286, 362, 1017, 14938, 22978, 13, 51064], "temperature": 0.0, "avg_logprob": -0.3429066609528105, "compression_ratio": 1.5240963855421688, "no_speech_prob": 0.25067445635795593}, {"id": 364, "seek": 317256, "start": 3173.56, "end": 3186.56, "text": " Right, with the pairwise approach you can explode that and we have 18,000 pairs, but these are from 449 sequences. I think that, yeah, it's, it's important to not shoot yourself in the foot with overfitting.", "tokens": [50414, 1779, 11, 365, 264, 6119, 3711, 3109, 291, 393, 21411, 300, 293, 321, 362, 2443, 11, 1360, 15494, 11, 457, 613, 366, 490, 1017, 14938, 22978, 13, 286, 519, 300, 11, 1338, 11, 309, 311, 11, 309, 311, 1021, 281, 406, 3076, 1803, 294, 264, 2671, 365, 670, 69, 2414, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2272144144231623, "compression_ratio": 1.3269230769230769, "no_speech_prob": 0.3171500563621521}, {"id": 365, "seek": 318656, "start": 3187.56, "end": 3205.56, "text": " I did that, I did that, I did that on 200,000 hydrolases. And guess what, it made it worse.", "tokens": [50414, 286, 630, 300, 11, 286, 630, 300, 11, 286, 630, 300, 322, 2331, 11, 1360, 5796, 6623, 1957, 13, 400, 2041, 437, 11, 309, 1027, 309, 5324, 13, 51314], "temperature": 0.0, "avg_logprob": -0.3652406632900238, "compression_ratio": 1.2297297297297298, "no_speech_prob": 0.23324672877788544}, {"id": 366, "seek": 320556, "start": 3206.56, "end": 3222.56, "text": " I did, I did so many things. I did, I worked on this, I played with so many iterations. The conclusion after one year of fine tuning and model architecture training and all of that was you're just overfitting Jaffa, stop shooting yourself in the foot, you're overfitting.", "tokens": [50414, 286, 630, 11, 286, 630, 370, 867, 721, 13, 286, 630, 11, 286, 2732, 322, 341, 11, 286, 3737, 365, 370, 867, 36540, 13, 440, 10063, 934, 472, 1064, 295, 2489, 15164, 293, 2316, 9482, 3097, 293, 439, 295, 300, 390, 291, 434, 445, 670, 69, 2414, 508, 2518, 64, 11, 1590, 5942, 1803, 294, 264, 2671, 11, 291, 434, 670, 69, 2414, 13, 51214], "temperature": 0.0, "avg_logprob": -0.30036112841437845, "compression_ratio": 1.5664739884393064, "no_speech_prob": 0.5421143174171448}, {"id": 367, "seek": 322256, "start": 3222.56, "end": 3237.56, "text": " It works, and especially if you, if you're honest with yourself and you do proper cross validation, you hold out some data set and you optimize, you find out it does well on 0.9 correlation on this data set. When you move to another data set, bang, it fails.", "tokens": [50364, 467, 1985, 11, 293, 2318, 498, 291, 11, 498, 291, 434, 3245, 365, 1803, 293, 291, 360, 2296, 3278, 24071, 11, 291, 1797, 484, 512, 1412, 992, 293, 291, 19719, 11, 291, 915, 484, 309, 775, 731, 322, 1958, 13, 24, 20009, 322, 341, 1412, 992, 13, 1133, 291, 1286, 281, 1071, 1412, 992, 11, 8550, 11, 309, 18199, 13, 51114], "temperature": 0.0, "avg_logprob": -0.3137896656990051, "compression_ratio": 1.5357142857142858, "no_speech_prob": 0.6612440347671509}, {"id": 368, "seek": 323756, "start": 3237.56, "end": 3258.56, "text": " So, looking at performance overall, it's important to not, to use the right set of models with your data. 449 proteins, you should be limiting yourself to maybe one layer, two layers max and not very deep models and things like that, yeah.", "tokens": [50364, 407, 11, 1237, 412, 3389, 4787, 11, 309, 311, 1021, 281, 406, 11, 281, 764, 264, 558, 992, 295, 5245, 365, 428, 1412, 13, 1017, 14938, 15577, 11, 291, 820, 312, 22083, 1803, 281, 1310, 472, 4583, 11, 732, 7914, 11469, 293, 406, 588, 2452, 5245, 293, 721, 411, 300, 11, 1338, 13, 51414], "temperature": 0.0, "avg_logprob": -0.2415394197430527, "compression_ratio": 1.4397590361445782, "no_speech_prob": 0.11116784065961838}, {"id": 369, "seek": 325856, "start": 3258.56, "end": 3261.56, "text": " Okay, thank you very much.", "tokens": [50414, 1033, 11, 1309, 291, 588, 709, 13, 50514], "temperature": 0.0, "avg_logprob": -0.28021650314331054, "compression_ratio": 0.7647058823529411, "no_speech_prob": 0.08485212922096252}, {"id": 370, "seek": 328856, "start": 3288.56, "end": 3289.56, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.6718512376149496, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9990725517272949}, {"id": 371, "seek": 331856, "start": 3318.56, "end": 3319.56, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.5675757726033529, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9995260238647461}, {"id": 372, "seek": 334856, "start": 3348.56, "end": 3349.56, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.47527631123860675, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.999366819858551}, {"id": 373, "seek": 337856, "start": 3378.56, "end": 3379.56, "text": " Thank you.", "tokens": [50364, 1044, 291, 13, 50414], "temperature": 0.0, "avg_logprob": -0.4408878485361735, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9991788268089294}, {"id": 374, "seek": 340856, "start": 3408.56, "end": 3410.56, "text": " Thank you.", "tokens": [50414, 1044, 291, 13, 50464], "temperature": 0.0, "avg_logprob": -0.3760248025258382, "compression_ratio": 0.5555555555555556, "no_speech_prob": 0.9988208413124084}], "language": "en"}